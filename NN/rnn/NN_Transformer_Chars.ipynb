{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06d3802a",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "## Прогнозирование следующей буквы текста\n",
    "\n",
    "\n",
    "Пример прогнозирования очередной буквы текста при помощи механизма внимания.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "746f10e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from   time import perf_counter as tm                  # таймер sec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe559f4",
   "metadata": {},
   "source": [
    "# Загружаем текст\n",
    "\n",
    "Возьмём фиксированным  алфавит `CHARS` (русский язык) в нижнем регистре, включив в него конец строки `\\n` (для стихов):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1424e117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saltan.txt: 24201  chars,  3995 words 6.06 ch/w\n",
      "chars: 37, text length: 24202 chars\n",
      "beg:|  три девицы под окном \n",
      "пряли поздно вечерком. \n",
      "кабы я была царица, \n",
      "говорит одна девица, \n",
      "то на вес|\n",
      "end:|. \n",
      "день прошел царя салтана \n",
      "уложили спать вполпьяна. \n",
      "я там был, мед, пиво пил \n",
      "и усы лишь обмочил.|\n"
     ]
    }
   ],
   "source": [
    "CHARS  = \" .?,абвгдежзийклмнопрстуфхцчшщъыьэюя\\n\"        # фиксированный алфавит \n",
    "charID = { c:i for i,c in enumerate(CHARS) }             # буква в номер\n",
    "\n",
    "def preprocess(txt):\n",
    "    \"\"\" Буквы не из алфавита заменяем пробелами \"\"\"\n",
    "    txt = txt.lower().replace('ё','e').replace('!','.').replace(';',',')\n",
    "    txt = ''.join( [c if c in CHARS else ' ' for c in txt] )    \n",
    "    txt = re.sub(' +', ' ', txt)\n",
    "    txt = txt.replace(' ,', ',').replace(' .', '.').replace(' ?', '?')\n",
    "    return  re.sub('\\n\\s+', '\\n', txt)\n",
    "    \n",
    "def load_Zip(fname = \"data/books.zip\"):\n",
    "    \"\"\" Загрузить в строку содержмое всех файлов из zip-архива, проведя их препроцессинг \"\"\"\n",
    "    txt = \"\"\n",
    "    with zipfile.ZipFile(fname) as myzip:\n",
    "        for fname in myzip.namelist():\n",
    "            print(fname, end=\": \")\n",
    "            with myzip.open(fname) as myfile:\n",
    "                st = preprocess ( myfile.read().decode(\"utf-8\") )     \n",
    "                chars, words = len(st), len(st.split())\n",
    "                print(chars, \" chars, \",  words, \"words\", f\"{chars/words:.2f} ch/w\")  \n",
    "                txt += \" \" + st\n",
    "    return txt\n",
    "    \n",
    "text = load_Zip(\"data/saltan.zip\")        \n",
    "#text = load_Zip(\"data/books.zip\")        \n",
    "        \n",
    "print(f\"chars: {len(CHARS)}, text length: {len(text)} chars\")            \n",
    "print(f\"beg:|{text[:100]}|\")        \n",
    "print(f\"end:|{text[-100:]}|\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eb580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    LEN        = 16    # длина последовательности в символах\n",
    "    STEP       = 16    # букв между началами последовательностей (если LEN == STEP - не перекрываются)    \n",
    "    NUM        =  8    # число последних букв по которым вычисляется ошибка (NUM <= STEP)\n",
    "    \n",
    "    E_DIM      = 16    # размерность эмбединга\n",
    "    H_DIM      = 128   # размерность скрытого слоя\n",
    "    NUM_LAYERS = 3     # число слоёв rnn\n",
    "    \n",
    "\n",
    "    DROP       = 0     # dropout вероятность перед классификационным слоем\n",
    "    BATCH      = 1024  # размер батча\n",
    "    L2         = 1e-5  # L2-регуляризация\n",
    "    LR         = 1e-2  # скорость обучения\n",
    "    \n",
    "    def get(end=\", \"):\n",
    "        return \"\".join([f\"{k}:{v}{end}\" for k,v in CFG.__dict__.items() if not k.startswith(\"__\") and k != \"get\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3af49d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\" B - sample in batch, S - sequence position, E - embedding \"\"\"\n",
    "    def __init__(self):        \n",
    "        super(Transformer, self).__init__()\n",
    "         \n",
    "        self.emb = nn.Embedding(num_embeddings=len(VOCAB),  embedding_dim=CFG.EMB_DIM,   padding_idx=0)                \n",
    "        self.pos = nn.Embedding(num_embeddings=CFG.SEQ_LEN, embedding_dim=CFG.EMB_DIM)                \n",
    "        \n",
    "        self.encoder = nn.TransformerEncoder( \n",
    "            nn.TransformerEncoderLayer(d_model=CFG.EMB_DIM, dim_feedforward = CFG.FF*CFG.EMB_DIM, \n",
    "                                       nhead=CFG.HEADS, dropout=CFG.DROPOUT1, batch_first=True), \n",
    "            num_layers=CFG.LAYERS)       \n",
    "        \n",
    "        self.drop = nn.Dropout(CFG.DROPOUT2)\n",
    "        self.fc = nn.Linear(CFG.SEQ_LEN*CFG.EMB_DIM, 1)         \n",
    "        \n",
    "    def forward(self, x):                   # (B,S)\n",
    "        x = self.emb(x)                     # (B,S,E)         \n",
    "        p = self.pos.weight                 # (S,E)   - position encoder        \n",
    "        \n",
    "        x = F.normalize(x, dim = -1) + F.normalize(p, dim = -1)\n",
    "        x = F.normalize(x, dim = -1)\n",
    "        #x = x + p\n",
    "        \n",
    "        x = self.encoder(x)                 # (B,S,E)\n",
    "        \n",
    "        x = F.normalize(x, dim = -1)\n",
    "        #x = torch.bmm(x, x.transpose(1,2))  # (B,S,E) @ (B,E,S) = (B,S,S)   \n",
    "        \n",
    "        #x = torch.relu(x)\n",
    "        x = nn.Flatten()(x)                 # (B,S*E)        \n",
    "        \n",
    "        x = self.drop(x)\n",
    "        x = self.fc(x)                      # (B,)             \n",
    "        return x                            \n",
    "    \n",
    "model = Transformer()    \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "X = torch.zeros((8, CFG.SEQ_LEN), dtype=torch.long, device=device)\n",
    "X[0,0] = 1\n",
    "Y = model(X)\n",
    "print( Y.shape  )\n",
    "print( Y[0]  )\n",
    "summary(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
