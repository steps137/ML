{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "667343cc",
   "metadata": {
    "id": "05f70e04"
   },
   "source": [
    "# CLIP\n",
    "\n",
    "The `CLIP model` from OpenAI allows to get a feature vector of an image. This vector characterizes the \"textual\" semantics of image content. Features vector has  **512** dimension. Using the `Caltech256` dataset, we train various models to compress this vector to a dimension of **64**.\n",
    "\n",
    "- [CLIP model](https://github.com/openai/CLIP)\n",
    "- [Caltech256](https://pytorch.org/vision/stable/generated/torchvision.datasets.Caltech256.html#torchvision.datasets.Caltech256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "386585cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FlKJ1bndxcbT",
    "outputId": "f2aeae9c-eae0-4345-ab52-3ba8bd09646b"
   },
   "outputs": [],
   "source": [
    "#!pip install ftfy regex tqdm                          # need for CLIP\n",
    "#!pip install git+https://github.com/openai/CLIP.git   # CLIP (local install from Far by Admin)\n",
    "#!pip install torchinfo                                # summary of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1489a1a4",
   "metadata": {
    "id": "eop4Kr8Jo44z"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from   torchvision  import models, datasets, transforms\n",
    "\n",
    "from   torchinfo    import summary  \n",
    "\n",
    "import clip\n",
    "from   PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f89c633",
   "metadata": {
    "id": "lwM8jrUxyfX5"
   },
   "source": [
    "## Caltech256 dataset\n",
    "\n",
    "The dataset contains 30607 images (most of them are RGB). Each image is assigned to one of 256 classes.\n",
    "\n",
    "The dataset is loaded and some random images are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45113eae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 713
    },
    "id": "GpODKBo0s239",
    "outputId": "c1865a9e-70a9-4599-c48f-9716484c74df"
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCaltech256\u001b[49m\u001b[43m(\u001b[49m\u001b[43m                                  \u001b[49m\u001b[38;5;66;43;03m# load dataset to folder data\u001b[39;49;00m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtotal images:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dataset))\n\u001b[0;32m      8\u001b[0m rows, cols, i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\datasets\\caltech.py:172\u001b[0m, in \u001b[0;36mCaltech256.__init__\u001b[1;34m(self, root, transform, target_transform, download)\u001b[0m\n\u001b[0;32m    169\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[1;32m--> 172\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\datasets\\caltech.py:230\u001b[0m, in \u001b[0;36mCaltech256.download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiles already downloaded and verified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 230\u001b[0m \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp://www.vision.caltech.edu/Image_Datasets/Caltech256/256_ObjectCategories.tar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m256_ObjectCategories.tar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmd5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m67b4f42ca05d46448c6bb8ecd2220f6d\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\datasets\\utils.py:430\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[1;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[0;32m    428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[1;32m--> 430\u001b[0m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\datasets\\utils.py:131\u001b[0m, in \u001b[0;36mdownload_url\u001b[1;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[0;32m    128\u001b[0m     _download_file_from_remote_location(fpath, url)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;66;03m# expand redirect chain if needed\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[43m_get_redirect_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_hops\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_redirect_hops\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# check if file is located on Google Drive\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     file_id \u001b[38;5;241m=\u001b[39m _get_google_drive_file_id(url)\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\datasets\\utils.py:79\u001b[0m, in \u001b[0;36m_get_redirect_url\u001b[1;34m(url, max_hops)\u001b[0m\n\u001b[0;32m     76\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: USER_AGENT}\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_hops \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39murl \u001b[38;5;241m==\u001b[39m url \u001b[38;5;129;01mor\u001b[39;00m response\u001b[38;5;241m.\u001b[39murl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m url\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\urllib\\request.py:222\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\urllib\\request.py:531\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    530\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 531\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\urllib\\request.py:640\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 640\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\urllib\\request.py:569\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    568\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\urllib\\request.py:502\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    501\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 502\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\urllib\\request.py:649\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 649\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "dataset = datasets.Caltech256(                                  # load dataset to folder data\n",
    "    root = 'data',\n",
    "    transform = transforms.ToTensor(), \n",
    "    download = True,            \n",
    ")\n",
    "print(dataset,\"\\ntotal images:\", len(dataset))\n",
    "\n",
    "rows, cols, i = 5, 10, 0\n",
    "idx = torch.randint(high = len(dataset), size = (len(dataset),) )  \n",
    "images = []\n",
    "for iD in idx:\n",
    "    img = dataset[iD][0].permute(1, 2, 0).numpy()        \n",
    "    if img.shape[-1] == 3:                                      # remove grayscale\n",
    "        images.append(img)\n",
    "    if len(images) == rows*cols:\n",
    "        break\n",
    "\n",
    "plt.figure(figsize=(2*cols, 2*rows), facecolor ='w')            # draw rows*cols random images      \n",
    "for row in range(rows):        \n",
    "    for col in range(cols):\n",
    "        ax=plt.subplot(rows, cols, i+1)                    \n",
    "        plt.title(f\"{dataset[idx[i]][1]}\")    \n",
    "        plt.imshow(images[i])  \n",
    "        plt.axis(\"off\")\n",
    "        i += 1\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f4e083",
   "metadata": {
    "id": "EpTYsdzS7doI"
   },
   "source": [
    "## CLIP Model\n",
    "\n",
    "We load the CLIP model and display its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "761f6e80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ES5XzR07kRg",
    "outputId": "450ad113-a555-4ff3-f881-f37e1ec53bd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                                                 Param #\n",
       "===============================================================================================\n",
       "CLIP                                                                   301,569\n",
       "├─VisionTransformer: 1-1                                               87,849,216\n",
       "├─Transformer: 1-2                                                     37,828,608\n",
       "├─Embedding: 1-3                                                       25,296,896\n",
       "├─LayerNorm: 1-4                                                       1,024\n",
       "===============================================================================================\n",
       "Total params: 151,277,313\n",
       "Trainable params: 151,277,313\n",
       "Non-trainable params: 0\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "summary(model, depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f8b2d6",
   "metadata": {
    "id": "bP28det87uhi"
   },
   "source": [
    "## Test CLIP for some images from Caltech256\n",
    "\n",
    "The selection the set of texts is important, since `softmax` is an insidious thing. Try removing text:\n",
    "- \"dolphin in the sea\"\n",
    "- \"palm tree on the beach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded183f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5e53d54b",
    "outputId": "7c48bd1c-0255-49a7-ad6d-a087fcde5eb0"
   },
   "outputs": [],
   "source": [
    "image_files = [\n",
    "    \"/content/data/caltech256/256_ObjectCategories/159.people/159_0013.jpg\",\n",
    "    \"/content/data/caltech256/256_ObjectCategories/154.palm-tree/154_0006.jpg\",\n",
    "    \"/content/data/caltech256/256_ObjectCategories/056.dog/056_0017.jpg\",\n",
    "    \"/content/data/caltech256/256_ObjectCategories/057.dolphin-101/057_0020.jpg\"\n",
    "]\n",
    "texts = [ \n",
    "    \"a nice girl\", \n",
    "    \"a dog\", \n",
    "    \"a cat\", \n",
    "    \"fat cat eats sour cream\", \n",
    "    \"palm tree on the beach\", \n",
    "    \"sunset over the sea\", \n",
    "    \"dolphin in the sea\", \n",
    "        ]\n",
    "\n",
    "text = clip.tokenize(texts).to(device)\n",
    "\n",
    "for file in image_files:\n",
    "    image_orig = Image.open(file)\n",
    "    image_prep = preprocess(image_orig).unsqueeze(0).to(device)   \n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_prep)  \n",
    "        text_features  = model.encode_text(text)\n",
    "    \n",
    "        logits_per_image, logits_per_text = model(image_prep, text)\n",
    "        probs = logits_per_image.softmax(dim=-1).float().cpu()\n",
    "\n",
    "    print(image_features.shape, text_features.shape)  \n",
    "\n",
    "    bests = torch.topk(probs[0], 3)                # get 3 best probabilities\n",
    "    for i, Id in enumerate(bests.indices):    \n",
    "        print(f\"{ '+' if i==0 else ' '} {bests.values[i]:.3f}: {texts[Id]}\") \n",
    "\n",
    "    plt.figure(figsize=(8, 4), facecolor ='w')     # plot original and preprocess images\n",
    "    ax=plt.subplot(1,2,1); plt.imshow(image_orig  )\n",
    "    im = image_prep[0].permute(1, 2, 0).cpu().numpy()\n",
    "    ax=plt.subplot(1,2,2); plt.imshow( (im-im.min())/(im.max()-im.min()+1e-6)  )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c7f7c",
   "metadata": {
    "id": "VP05Q6S8X6Cg"
   },
   "source": [
    "## Create a dataset with vectors and image classes\n",
    "\n",
    "- We get all RGB pictures. \n",
    "- We get feature vectors for each of them (do not forget to normalize them and convert from half to float).\n",
    "\n",
    "This stage takes a very long time!!!\n",
    "\n",
    "**TODO**: create batches of images for `preprocess` and `encode_image`. This will speed up the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea74623b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "b81bb143",
    "outputId": "fa9a27be-2b38-4ba6-d76f-35063a72a95f"
   },
   "outputs": [],
   "source": [
    "NUM_IMAGES = len(dataset)                                              # try to take all images except gray\n",
    "\n",
    "idx, i, cnt = torch.randint(high = len(dataset), size = (len(dataset),) ), 0, 0\n",
    "vectors = None\n",
    "classes = []\n",
    "while cnt < NUM_IMAGES and i < len(dataset):    \n",
    "    im = dataset[idx[i]][0].permute(1, 2, 0).numpy()      \n",
    "    im = (255 * (im-im.min())/(im.max()-im.min()+1e-6)).astype('uint8')\n",
    "    if i and i % 100 == 0:\n",
    "        print(f\"\\r{i:10} {cnt: 10} {im.shape}\", end=\"\")\n",
    "    if im.shape[-1] == 3:    \n",
    "        image_orig = Image.fromarray(im, mode='RGB')         \n",
    "        image_prep = preprocess(image_orig).unsqueeze(0).to(device)   \n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image_prep).float()      \n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)   # normalize vector             \n",
    "            vectors = image_features if vectors is None else torch.cat([vectors, image_features], axis=0)\n",
    "            classes.append( dataset[idx[i]][1] )\n",
    "\n",
    "        if cnt < 3:\n",
    "            plt.figure(figsize=(4, 2), facecolor ='w')                    # plot original and preprocess images\n",
    "            ax=plt.subplot(1,2,1); plt.imshow(image_orig  )\n",
    "            im = image_prep[0].permute(1, 2, 0).cpu().numpy()\n",
    "            ax=plt.subplot(1,2,2); plt.imshow( (im-im.min())/(im.max()-im.min()+1e-6)  )\n",
    "            plt.imshow( im )\n",
    "            plt.show()\n",
    "\n",
    "        cnt += 1\n",
    "    i += 1\n",
    "\n",
    "vectors = vectors.cpu().numpy()\n",
    "np.save(\"vectors\", vectors)                                             # save to file vectors.npy\n",
    "np.save(\"classes\", np.array(classes)) \n",
    "print(\"\\n\",vectors.shape, \"\\n last vector: \", vectors[-1][:5], \"\\n classes: \", classes[:10])\n",
    "NUM_IMAGES = len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f4554",
   "metadata": {
    "id": "90P172-dJ8gl"
   },
   "outputs": [],
   "source": [
    "#vectors = np.load(\"vectors.npy\")   \n",
    "#classes = np.load(\"classes.npy\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c742ba",
   "metadata": {
    "id": "v5n2V3iyahdQ"
   },
   "source": [
    "## Comapare\n",
    "\n",
    "We check the quality of the features vector of the image. To do this, we calculate pairwise Euclidean distances for all images (as Google wants). For each image, we select 5 nearest neighbors. For these neighbors, we calculate the average coincidence of their classes with the class of the given image. All these values are averaged over all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c19e9e",
   "metadata": {
    "id": "zzEBxNwFalJ2"
   },
   "outputs": [],
   "source": [
    "from   scipy.spatial.distance import cdist\n",
    "\n",
    "def metric(vecs, classes, neighbors=5):\n",
    "    all = []\n",
    "    dists = cdist(vecs, vecs)                     # pairwise Euclidean distances between vectors\n",
    "    dists = -torch.tensor(dists)                  # we need minimum distances\n",
    "    for i in range(len(dists)):        \n",
    "        res = torch.topk(dists[i], neighbors+1)   # with himself !\n",
    "        oks = sum( [classes[i] == classes[r] for r in res.indices ] ) - 1\n",
    "        all.append(oks / neighbors)\n",
    "    return np.mean(all)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf3934",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J8pzTwT4eI0Q",
    "outputId": "1052e7c4-b8b0-418d-ec23-76cc790ab328"
   },
   "outputs": [],
   "source": [
    "print(f\"orig:{metric(vectors, classes):.3f}\")     # metric of original, uncompressed vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f841812e",
   "metadata": {
    "id": "3QxCxi1_OI4q"
   },
   "source": [
    "```\n",
    "n = 30201\n",
    "orig_512: 0.858\n",
    "aen_64  : 0.853\n",
    "pca_64  : 0.821\n",
    "poo_64  : 0.791\n",
    "\n",
    "pca_32  : 0.749\n",
    "poo_32  : 0.708\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28698195",
   "metadata": {
    "id": "ppE5vWMaZrow"
   },
   "source": [
    "## AverPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114effaa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DmAgzYO7ZvJf",
    "outputId": "98c37fd7-0576-49ef-8938-539591808798"
   },
   "outputs": [],
   "source": [
    "DIM = 64\n",
    "\n",
    "pool = nn.AdaptiveAvgPool1d((DIM,))\n",
    "\n",
    "vectors_pool = pool(torch.tensor(vectors))\n",
    "vectors_pool /= vectors_pool.norm(dim=-1, keepdim=True)   # normalize vector\n",
    "vectors_pool = vectors_pool.numpy()\n",
    "print(vectors_pool.shape)\n",
    "\n",
    "print(f\"pool:{metric(vectors_pool, classes):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec5054d",
   "metadata": {
    "id": "poO7jvz4WgD8"
   },
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2243511",
   "metadata": {
    "id": "WKyl4FaoVtGn"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=vectors.shape[1])\n",
    "pca.fit(vectors)\n",
    "\n",
    "plt.plot(pca.explained_variance_ratio_);\n",
    "\n",
    "pca = PCA(n_components=DIM)\n",
    "pca.fit(vectors)\n",
    "vectors_pca = pca.transform(vectors)\n",
    "\n",
    "\n",
    "vectors_pca = torch.tensor(vectors_pca)\n",
    "vectors_pca /= vectors_pca.norm(dim=-1, keepdim=True)   # normalize vector\n",
    "vectors_pca = vectors_pca.numpy()\n",
    "print(vectors_pca.shape)\n",
    "\n",
    "print(f\"pca: {metric(vectors_pca, classes):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c6600",
   "metadata": {
    "id": "c26W82MoxNDC"
   },
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370ef9eb",
   "metadata": {
    "id": "wOVDMuX8w996"
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 inputs = 512,                                      # count of input features\n",
    "                 dim    = 64,                                       # count of compress features\n",
    "                 hiddens_encoder=[],                                # hiddens layers of encoder\n",
    "                 hiddens_decoder=[],                                # hiddens layers of decoder\n",
    "                 batchnorm = True,\n",
    "                 dropout = 0):                                                        \n",
    "        super(Autoencoder, self).__init__()             \n",
    "\n",
    "        self.encoder = self.net([inputs] + hiddens_encoder + [dim],    dropout, batchnorm, True)\n",
    "        self.decoder = self.net([dim]    + hiddens_decoder + [inputs], dropout, batchnorm, False)\n",
    "        \n",
    "        #self.model =  nn.Sequential(self.encoder, self.decoder)\n",
    " \n",
    "    def net(self, hiddens, dropout, batchnorm, lastTanh):\n",
    "        layers = []\n",
    "        for i in range( len(hiddens)-1):            \n",
    "            layers += [ nn.Linear(hiddens[i], hiddens[i+1], bias=True)]\n",
    "            if batchnorm:\n",
    "                layers +=  [ nn.BatchNorm1d(num_features = hiddens[i+1]) ]       \n",
    "            if dropout:\n",
    "                layers += [ nn.Dropout(dropout) ]                            \n",
    "            if lastTanh or i < len(hiddens)- 2:\n",
    "                layers += [ nn.Tanh() ]\n",
    "        return nn.Sequential(*layers)       \n",
    "\n",
    "    def forward(self, x):               \n",
    "        x = self.encoder(x)\n",
    "        x = x / x.norm(dim=-1, keepdim=True)        # !!!\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a35332a",
   "metadata": {
    "id": "Ygm2rvDXzeKi"
   },
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'encoder':   [512],\n",
    "    'decoder':   [512],\n",
    "    'dropout':   0.0,\n",
    "    'batchnorm': True,\n",
    "    'batch':     256,\n",
    "    'lr':        1e-3,\n",
    "    'L2':        0.0,\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "aen = Autoencoder(hiddens_encoder=CFG['encoder'], hiddens_decoder=CFG[ 'decoder'], batchnorm = CFG['batchnorm'], dropout = CFG['dropout'])\n",
    "aen.to(device)\n",
    "\n",
    "#print(aen)\n",
    "summary(aen, (1,512),  col_names=[\"output_size\", \"num_params\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd2eb9",
   "metadata": {
    "id": "wb5DmMXG03p0"
   },
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde05b4",
   "metadata": {
    "id": "aA8DszKw08XH"
   },
   "outputs": [],
   "source": [
    "def fit(model, X,Y, batch_size=64, train=True):         \n",
    "    \"\"\" One epoch training \"\"\"\n",
    "    batch_size = min(batch_size, len(X))\n",
    "    losses = []\n",
    "    \n",
    "    if train:\n",
    "        idx = torch.randperm( len(X) )                  # permuted index list    \n",
    "        X, Y = X[idx], Y[idx]             \n",
    "    \n",
    "    model.train(train)                                  # important for Dropout, BatchNorm layers\n",
    "    for i in range(0, len(Y), batch_size):             \n",
    "        nb = min(batch_size,  len(Y)-i)\n",
    "        xb = X[i: i+nb].to(device)                      # current batch                \n",
    "        yb = Y[i: i+nb].to(device) \n",
    "        \n",
    "        y = model(xb)                                   # forward propagation\n",
    "        L = loss(y, yb)                                 # calculate the loss        \n",
    "  \n",
    "        if train:                                       # we are in learning mode\n",
    "            optimizer.zero_grad()                       # reset the gradients        \n",
    "            L.backward()                                # calculate gradients           \n",
    "            optimizer.step()                            # adjusting parameters\n",
    "                                     \n",
    "        losses.append(L.cpu().item())                   # total loss (item from graph)\n",
    "                 \n",
    "    return np.mean(losses)**0.5                          # mean loss \n",
    "\n",
    "def plot_train(history):\n",
    "    h = np.array(history)                               # learning output\n",
    "    plt.figure(figsize=(8,5), facecolor ='w')              \n",
    "    ax1 = plt.subplot(1,1,1);  \n",
    "    ax1.set( ylim=(0, 0.03), xlim=(0, h[-1,0]) )\n",
    "    ax1.grid(color='gray', linestyle='--', alpha=0.6)\n",
    "    ax2=ax1.twinx()  \n",
    "    ax1.plot(h[:,0], h[:,1], \"-b\")   \n",
    "    ax1.plot(h[:,0], h[:,2], \"-g\")   \n",
    "\n",
    "    ax2.plot(h[:,0], h[:,3], \":r\")   \n",
    "\n",
    "    pars = [ f\"{k:10s}: {v}\\n\" for k,v in CFG.items() ]\n",
    "    ax1.text(h[0,0]+(h[-1,0]-h[0,0])*0.5, 0., \"\".join(pars), {'fontsize':12, 'fontname':'monospace'})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1feb5d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WgbT6Nsu1l-Y",
    "outputId": "8af1a5dc-3a98-4ca0-ce2b-eb24d2caad55"
   },
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(aen.parameters(), lr=CFG['lr'], weight_decay=CFG['L2'])   \n",
    "\n",
    "epochs, tst_batch, TRAIN = 300, 512, 0.8\n",
    "\n",
    "X = torch.tensor(vectors)\n",
    "X = X[ torch.randint(high = len(X), size = (len(X),) )  ]\n",
    "X_trn, X_tst = X[:int(len(X)*TRAIN)],  X[int(len(X)*TRAIN):]\n",
    "Y_trn = torch.clone(X_trn) # ???\n",
    "print(\"trn:\", X_trn.shape, \"tst: \", X_tst.shape)\n",
    "\n",
    "aen.to(device) # just in case\n",
    "L_tst = fit(aen, X_trn, Y_trn, batch_size=tst_batch, train=False)\n",
    "print( f\"before:      loss: {L_tst:8.6f}\"  )\n",
    "\n",
    "history,  beg, start = [],  time.process_time(), time.process_time()\n",
    "for epoch in range(1, epochs+1):   \n",
    "    L = fit(aen, X_trn, Y_trn, batch_size=CFG['batch'])               # train one epoch\n",
    "\n",
    "    L_tst = fit(aen, X_tst, X_tst, batch_size=tst_batch, train=False) # test\n",
    "    L2 =  sum(p.data.pow(2).sum() for p in aen.parameters()).cpu().item()\n",
    "\n",
    "    if epoch % 1 == 0:                                                 # learning output   \n",
    "        print(f\"\\repoch: {epoch:5d} loss: {L:8.6f} ({L_tst:8.6f}) L2: {L2:.6f}  {time.process_time()-beg:.1f}s\", end=\"\")           \n",
    "        beg = time.process_time()                \n",
    "\n",
    "    history.append([epoch, L, L_tst, L2])\n",
    "\n",
    "    if epoch % 100  == 0 or epoch == epochs:\n",
    "        CFG['time'] = f\"{(time.process_time()-start)/epoch:.1f} s/ep [{(time.process_time()-start)/60:.1f} m]\"\n",
    "        plot_train(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd01d0f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5xoe2y4m8XIf",
    "outputId": "2250b8f1-5d7b-41f3-d1d7-c596f3f36244"
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "aen.train(False) \n",
    "X = torch.tensor(vectors)                 # don't mix!\n",
    "vectors_aen = None\n",
    "for i in range(0, len(X), batch_size):             \n",
    "    nb = min(batch_size,  len(X)-i)\n",
    "    xb = X[i: i+nb].to(device)\n",
    "    with torch.no_grad():\n",
    "        y = aen.encoder(xb)             \n",
    "    vectors_aen = y if vectors_aen is None else torch.cat([vectors_aen, y], axis=0)\n",
    "\n",
    "vectors_aen /= vectors_aen.norm(dim=-1, keepdim=True) \n",
    "vectors_aen = vectors_aen.cpu().numpy()\n",
    "print(vectors_aen.shape)\n",
    "\n",
    "print(f\"aen :{metric(vectors_aen, classes):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c0701a",
   "metadata": {
    "id": "RqsIr5rQzciR"
   },
   "source": [
    "## Autoencoder Hyperparameters\n",
    "\n",
    "```\n",
    "0.855  CFG = {'encoder':[512],     'decoder':[512],     'dropout':0.0, 'batchnorm': True,  'batch':256, 'lr':1e-3, 'L2':0.0}  epochs = 200\n",
    "0.854  CFG = {'encoder':[256],     'decoder':[256],     'dropout':0.0, 'batchnorm': True,  'batch':256, 'lr':1e-3, 'L2':0.0}  epochs = 200\n",
    "0.850  CFG = {'encoder':[256,128], 'decoder':[128,256], 'dropout':0.0, 'batchnorm': True,  'batch':256, 'lr':1e-4, 'L2':0.0}  epochs = 1000\n",
    "0.840  CFG = {'encoder':[256],     'decoder':[256],     'dropout':0.1, 'batchnorm': True,  'batch':256, 'lr':1e-3, 'L2':0.0}  epochs = 200\n",
    "0.830  CFG = {'encoder':[256],     'decoder':[256],     'dropout':0.2, 'batchnorm': True,  'batch':256, 'lr':1e-3, 'L2':0.0}  epochs = 200\n",
    "```\n",
    "- Другой датасет, больше классов\n",
    "- Аугументация, cnn-aen\n",
    "- \"aen\" в вероятности"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51f021e",
   "metadata": {},
   "source": [
    "## Model for submition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96f554c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_CLIP(nn.Module):\n",
    "    def __init__(self, weights=\"ViT-B/32\", device=\"cuda\"):        \n",
    "        super(Model_CLIP, self).__init__()      \n",
    "        \n",
    "        self.model, _ = clip.load(weights, device=device)\n",
    "                      \n",
    "    def forward(self, x):     \n",
    "        \"\"\"\n",
    "        image_orig = Image.fromarray(x, mode='RGB')         \n",
    "        image_prep = self.preprocess(image_orig).unsqueeze(0).to(device)   \n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.encode_image(image_prep).float()      \n",
    "            image_features = image_features / torch.linalg.norm(image_features, dim=-1, keepdim=True)   # normalize vector             \n",
    "        \"\"\"\n",
    "        return x # image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c63a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_CLIP(nn.Module):\n",
    "    def __init__(self, weights=\"ViT-B/32\", device=\"cuda\"):        \n",
    "        super(Model_CLIP, self).__init__()              \n",
    "                      \n",
    "    def forward(self, x):                       \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "070bc97d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't redefine method: forward on class: __torch__.clip.model.LayerNorm (of Python compilation unit at: 000002A28FFC8110)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Model_CLIP(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m saved_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m saved_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\jit\\_script.py:1265\u001b[0m, in \u001b[0;36mscript\u001b[1;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[0;32m   1263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m   1264\u001b[0m     obj \u001b[38;5;241m=\u001b[39m call_prepare_scriptable_func(obj)\n\u001b[1;32m-> 1265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_script_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_methods_to_compile\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m create_script_dict(obj)\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\jit\\_recursive.py:454\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[1;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing:\n\u001b[0;32m    453\u001b[0m     AttributeTypeIsSupportedChecker()\u001b[38;5;241m.\u001b[39mcheck(nn_module)\n\u001b[1;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\jit\\_recursive.py:516\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[1;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[0;32m    513\u001b[0m     script_module\u001b[38;5;241m.\u001b[39m_concrete_type \u001b[38;5;241m=\u001b[39m concrete_type\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m script_module \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecursiveScriptModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcpp_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\jit\\_script.py:594\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[1;34m(cpp_module, init_fn)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;124;03mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;124;03mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;124;03m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    593\u001b[0m script_module \u001b[38;5;241m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[1;32m--> 594\u001b[0m \u001b[43minit_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscript_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;66;03m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;66;03m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m RecursiveScriptModule\u001b[38;5;241m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\jit\\_recursive.py:494\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[1;34m(script_module)\u001b[0m\n\u001b[0;32m    491\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m orig_value\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;66;03m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[1;32m--> 494\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_concrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    496\u001b[0m cpp_module\u001b[38;5;241m.\u001b[39msetattr(name, scripted)\n\u001b[0;32m    497\u001b[0m script_module\u001b[38;5;241m.\u001b[39m_modules[name] \u001b[38;5;241m=\u001b[39m scripted\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\jit\\_recursive.py:516\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[1;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[0;32m    513\u001b[0m     script_module\u001b[38;5;241m.\u001b[39m_concrete_type \u001b[38;5;241m=\u001b[39m concrete_type\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m script_module \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecursiveScriptModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcpp_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\jit\\_script.py:594\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[1;34m(cpp_module, init_fn)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;124;03mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;124;03mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;124;03m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    593\u001b[0m script_module \u001b[38;5;241m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[1;32m--> 594\u001b[0m \u001b[43minit_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscript_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;66;03m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;66;03m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m RecursiveScriptModule\u001b[38;5;241m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\jit\\_recursive.py:494\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[1;34m(script_module)\u001b[0m\n\u001b[0;32m    491\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m orig_value\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;66;03m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[1;32m--> 494\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_concrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    496\u001b[0m cpp_module\u001b[38;5;241m.\u001b[39msetattr(name, scripted)\n\u001b[0;32m    497\u001b[0m script_module\u001b[38;5;241m.\u001b[39m_modules[name] \u001b[38;5;241m=\u001b[39m scripted\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\jit\\_recursive.py:516\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[1;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[0;32m    513\u001b[0m     script_module\u001b[38;5;241m.\u001b[39m_concrete_type \u001b[38;5;241m=\u001b[39m concrete_type\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m script_module \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecursiveScriptModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcpp_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\jit\\_script.py:594\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[1;34m(cpp_module, init_fn)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;124;03mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;124;03mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;124;03m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    593\u001b[0m script_module \u001b[38;5;241m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[1;32m--> 594\u001b[0m \u001b[43minit_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscript_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;66;03m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;66;03m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m RecursiveScriptModule\u001b[38;5;241m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\jit\\_recursive.py:494\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[1;34m(script_module)\u001b[0m\n\u001b[0;32m    491\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m orig_value\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;66;03m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[1;32m--> 494\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_concrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    496\u001b[0m cpp_module\u001b[38;5;241m.\u001b[39msetattr(name, scripted)\n\u001b[0;32m    497\u001b[0m script_module\u001b[38;5;241m.\u001b[39m_modules[name] \u001b[38;5;241m=\u001b[39m scripted\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\jit\\_recursive.py:520\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[1;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n\u001b[1;32m--> 520\u001b[0m     \u001b[43mcreate_methods_and_properties_from_stubs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_stubs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_stubs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;66;03m# Create hooks after methods to ensure no name collisions between hooks and methods.\u001b[39;00m\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;66;03m# If done before, hooks can overshadow methods that aren't exported.\u001b[39;00m\n\u001b[0;32m    523\u001b[0m     create_hooks_from_stubs(concrete_type, hook_stubs, pre_hook_stubs)\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\jit\\_recursive.py:371\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[1;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[0;32m    368\u001b[0m property_defs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mdef_ \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[0;32m    369\u001b[0m property_rcbs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mresolution_callback \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[1;32m--> 371\u001b[0m \u001b[43mconcrete_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_methods_and_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproperty_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defaults\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't redefine method: forward on class: __torch__.clip.model.LayerNorm (of Python compilation unit at: 000002A28FFC8110)"
     ]
    }
   ],
   "source": [
    "model = Model_CLIP(device=\"cpu\")\n",
    "\n",
    "saved_model = torch.jit.script(model)\n",
    "saved_model.save('saved_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614846d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model_CLIP(device=\"cpu\")\n",
    "m.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80e5e03f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 10 required positional arguments: 'embed_dim', 'image_resolution', 'vision_layers', 'vision_width', 'vision_patch_size', 'context_length', 'vocab_size', 'transformer_width', 'transformer_heads', and 'transformer_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mclip\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model\n\u001b[1;32m----> 2\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLIP\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 10 required positional arguments: 'embed_dim', 'image_resolution', 'vision_layers', 'vision_width', 'vision_patch_size', 'context_length', 'vocab_size', 'transformer_width', 'transformer_heads', and 'transformer_layers'"
     ]
    }
   ],
   "source": [
    "from clip import model\n",
    "m = model.CLIP()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "APGk8cNimSkS",
    "3c2d977e",
    "04qTotmkX9Jv",
    "Dnrb3vsXlr11",
    "qVuBMyx6lw96",
    "V8StaKt6Qmm8",
    "BXfEImBW5B9N",
    "0d0bda3e",
    "6b214410",
    "C30B5MYt-yIC",
    "GiqNX1LTb20n"
   ],
   "machine_shape": "hm",
   "name": "CNN_CIFAR10.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
