{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CLIP\n\nThe `CLIP model` from OpenAI allows to get a feature vector of an image. This vector characterizes the \"textual\" semantics of image content. Features vector has  **512** dimension. Using the `Caltech256` dataset, we train various models to compress this vector to a dimension of **64**.\n\n- [CLIP model](https://github.com/openai/CLIP)\n- [Caltech256](https://pytorch.org/vision/stable/generated/torchvision.datasets.Caltech256.html#torchvision.datasets.Caltech256)\n","metadata":{"id":"05f70e04"}},{"cell_type":"code","source":"!pip install ftfy regex tqdm                          # need for CLIP\n!pip install git+https://github.com/openai/CLIP.git   # CLIP (local install from Far by Admin)\n!pip install torchinfo                                # summary of a model","metadata":{"id":"FlKJ1bndxcbT","outputId":"f2aeae9c-eae0-4345-ab52-3ba8bd09646b","execution":{"iopub.status.busy":"2022-08-11T17:36:17.107252Z","iopub.execute_input":"2022-08-11T17:36:17.108463Z","iopub.status.idle":"2022-08-11T17:36:53.495532Z","shell.execute_reply.started":"2022-08-11T17:36:17.108338Z","shell.execute_reply":"2022-08-11T17:36:53.494320Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import time\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom   torchvision  import models, datasets, transforms\nfrom   torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, Lambda, InterpolationMode\n\nfrom   torchinfo    import summary  \n\nimport clip\nfrom   PIL import Image","metadata":{"id":"eop4Kr8Jo44z","execution":{"iopub.status.busy":"2022-08-11T17:36:55.816719Z","iopub.execute_input":"2022-08-11T17:36:55.817090Z","iopub.status.idle":"2022-08-11T17:36:55.824444Z","shell.execute_reply.started":"2022-08-11T17:36:55.817053Z","shell.execute_reply":"2022-08-11T17:36:55.823323Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Caltech256 dataset\n\nThe dataset contains 30607 images (most of them are RGB). Each image is assigned to one of 256 classes.\n\nThe dataset is loaded and some random images are displayed.","metadata":{"id":"lwM8jrUxyfX5"}},{"cell_type":"code","source":"dataset = datasets.Caltech256(                                  # load dataset to folder data\n    root = '/kaggle/input/',                                    # transform like google prepared (but not convert(\"RGB\") !)\n    transform = transforms.PILToTensor(),                       # tensor, (H,W,C) [0...255] -> (C,H,W) [0...255], uint8\n    download = True,            \n)\nprint(dataset,\"\\ntotal images:\", len(dataset))                  # dataset[i][0] - img, dataset[i][1] - class\nprint(dataset[0][0].dtype, dataset[0][0].shape, dataset[0][0].max())\n\nrows, cols, i = 5, 10, 0\nidx = torch.randint(high = len(dataset), size = (len(dataset),) )  \nimages = []\nfor iD in idx:\n    img = dataset[iD][0]   \n    print(f\"\\r{iD}: {img.shape}\", end=\" \")\n    if img.shape[0] == 3:                                      # remove grayscale\n        images.append(img.permute(1,2,0).numpy())              # for plotting (H,W,C)\n        if len(images) == rows*cols:\n            break\nprint(\"Start plotting: \",len(images))\n\nplt.figure(figsize=(2*cols, 2*rows), facecolor ='w')            # draw rows*cols random images      \nfor row in range(rows):        \n    for col in range(cols):\n        ax=plt.subplot(rows, cols, i+1)                            \n        plt.imshow(images[i])  \n        plt.axis(\"off\")\n        i += 1\nplt.show()    ","metadata":{"id":"GpODKBo0s239","outputId":"c1865a9e-70a9-4599-c48f-9716484c74df","execution":{"iopub.status.busy":"2022-08-11T17:37:01.237649Z","iopub.execute_input":"2022-08-11T17:37:01.238017Z","iopub.status.idle":"2022-08-11T17:37:12.837945Z","shell.execute_reply.started":"2022-08-11T17:37:01.237987Z","shell.execute_reply":"2022-08-11T17:37:12.836692Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## CLIP Model\n\nWe load the CLIP model and display its structure.","metadata":{"id":"EpTYsdzS7doI"}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n#model, preprocess = clip.load(\"ViT-B/32\", device=device)\nmodel, preprocess = clip.load(\"ViT-L/14@336px\", device=device)\n\nsummary(model, depth=1)","metadata":{"id":"8ES5XzR07kRg","outputId":"450ad113-a555-4ff3-f881-f37e1ec53bd6","execution":{"iopub.status.busy":"2022-08-11T17:37:12.839330Z","iopub.execute_input":"2022-08-11T17:37:12.839650Z","iopub.status.idle":"2022-08-11T17:37:46.202290Z","shell.execute_reply.started":"2022-08-11T17:37:12.839621Z","shell.execute_reply":"2022-08-11T17:37:46.201122Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Change default preprocess","metadata":{}},{"cell_type":"code","source":"n_px = 336                                                    # size of input images for  ViT-L/14@336px\npreprocess = Compose([                                        # input tensor (C,H,W)        \n        #Resize( (n_px,n_px), interpolation=InterpolationMode.BICUBIC ),\n    \n        Resize(n_px, interpolation=InterpolationMode.BICUBIC),\n        CenterCrop(n_px),        \n    \n        Lambda(lambda x: x / 255.0),                          # [0, 255] unit8  -> [0,1] float32\n        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),        \n    ])    \n    \n    \nimg = dataset[0][0]\nprint(img.dtype, img.shape, img.min().item(), img.max().item())\nimg = preprocess(img)\nprint(img.dtype, img.shape, img.min(), img.max())","metadata":{"execution":{"iopub.status.busy":"2022-08-11T20:19:02.258596Z","iopub.execute_input":"2022-08-11T20:19:02.259002Z","iopub.status.idle":"2022-08-11T20:19:02.298162Z","shell.execute_reply.started":"2022-08-11T20:19:02.258972Z","shell.execute_reply":"2022-08-11T20:19:02.297080Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## Test CLIP for some images from Caltech256\n\nThe selection the set of texts is important, since `softmax` is an insidious thing. Try removing text:\n- \"dolphin in the sea\"\n- \"palm tree on the beach\"","metadata":{"id":"bP28det87uhi"}},{"cell_type":"code","source":"image_files = [\n    \"/kaggle/input/caltech256/256_ObjectCategories/159.people/159_0013.jpg\",\n    \"/kaggle/input/caltech256/256_ObjectCategories/154.palm-tree/154_0006.jpg\",\n    \"/kaggle/input/caltech256/256_ObjectCategories/056.dog/056_0017.jpg\",\n    \"/kaggle/input/caltech256/256_ObjectCategories/057.dolphin-101/057_0020.jpg\"\n]\ntexts = [ \n    \"a nice girl\", \n    \"a dog\", \n    \"a cat\", \n    \"fat cat eats sour cream\", \n    \"palm tree on the beach\", \n    \"sunset over the sea\", \n    \"dolphin in the sea\", \n        ]\n\ntext = clip.tokenize(texts).to(device)\n\nfor file in image_files:\n    image_orig = Image.open(file).convert(\"RGB\")\n    image_orig = transforms.Compose([transforms.PILToTensor()])(image_orig)    \n    image_prep = preprocess(image_orig).unsqueeze(0).to(device)  \n    print(image_prep.shape)\n    \n    with torch.no_grad():\n        image_features = model.encode_image(image_prep)  \n        text_features  = model.encode_text(text)\n    \n        logits_per_image, logits_per_text = model(image_prep, text)\n        probs = logits_per_image.softmax(dim=-1).float().cpu()\n\n    print(image_features.shape, text_features.shape)  \n\n    bests = torch.topk(probs[0], 3)                # get 3 best probabilities\n    for i, Id in enumerate(bests.indices):    \n        print(f\"{ '+' if i==0 else ' '} {bests.values[i]:.3f}: {texts[Id]}\") \n\n    plt.figure(figsize=(8, 4), facecolor ='w')     # plot original and preprocess images\n    ax=plt.subplot(1,3,1); \n    plt.imshow(image_orig.permute(1, 2, 0).numpy()  )\n    im = image_prep[0].permute(1, 2, 0).cpu().numpy()\n    print(im.shape)\n    ax=plt.subplot(1,3,2); \n    plt.imshow( im )    \n    ax=plt.subplot(1,3,3); \n    im = (im-im.min())/(im.max()-im.min()+1e-6) \n    plt.imshow( im )    \n    plt.show()","metadata":{"id":"5e53d54b","outputId":"7c48bd1c-0255-49a7-ad6d-a087fcde5eb0","execution":{"iopub.status.busy":"2022-08-11T18:57:56.919466Z","iopub.execute_input":"2022-08-11T18:57:56.920088Z","iopub.status.idle":"2022-08-11T18:57:59.341111Z","shell.execute_reply.started":"2022-08-11T18:57:56.920052Z","shell.execute_reply":"2022-08-11T18:57:59.340177Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Create a dataset with vectors and image classes\n\n- We get all RGB pictures. \n- We get feature vectors for each of them (do not forget to normalize them and convert from half to float).\n\nThis stage takes a very long time!!!\n\n**TODO**: create batches of images for `preprocess` and `encode_image`. This will speed up the calculations.","metadata":{"id":"VP05Q6S8X6Cg"}},{"cell_type":"code","source":"NUM_IMAGES = len(dataset)                                              # try to take all images except gray\n\nidx, i, cnt = torch.randint(high = len(dataset), size = (len(dataset),) ), 0, 0\nvectors = None\nclasses = []\nwhile cnt < NUM_IMAGES and i < len(dataset):    \n    img = dataset[idx[i]][0]        \n    if i and i % 100 == 0:\n        print(f\"\\r{i:10} {cnt: 10} {img.shape}\", end=\"\")\n        \n    if img.shape[0] == 3:            \n        image_prep = preprocess(img).unsqueeze(0).to(device)           \n        with torch.no_grad():\n            features = model.encode_image(image_prep).float()      \n            features = nn.functional.normalize(features)  # normalize vector             \n            vectors = features if vectors is None else torch.cat([vectors, features], axis=0)\n            classes.append( dataset[idx[i]][1] )\n\n        if cnt < 3:\n            print(img.shape, image_prep.shape)\n            plt.figure(figsize=(4, 2), facecolor ='w')                    # plot original and preprocess images\n            plt.imshow(img.permute(1, 2, 0).numpy() )\n            plt.show()\n\n        cnt += 1\n    i += 1\n\nvectors = vectors.cpu().numpy()\nnp.save(\"vectors\", vectors)                                             # save to file vectors.npy\nnp.save(\"classes\", np.array(classes)) \nprint(\"\\n\",vectors.shape, \"\\n last vector: \", vectors[-1][:5], \"\\n classes: \", classes[:10])\nNUM_IMAGES = len(vectors)","metadata":{"id":"b81bb143","outputId":"fa9a27be-2b38-4ba6-d76f-35063a72a95f","execution":{"iopub.status.busy":"2022-08-11T20:19:16.125997Z","iopub.execute_input":"2022-08-11T20:19:16.126392Z","iopub.status.idle":"2022-08-11T20:55:04.951072Z","shell.execute_reply.started":"2022-08-11T20:19:16.126358Z","shell.execute_reply":"2022-08-11T20:55:04.950112Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"#vectors = np.load(\"vectors.npy\")   \n#classes = np.load(\"classes.npy\")   \n#print(vectors.shape,  vectors.dtype,\"\\n\", vectors[0,:5], \"\\n\", classes[:10])","metadata":{"id":"90P172-dJ8gl","execution":{"iopub.status.busy":"2022-08-11T19:37:49.241511Z","iopub.execute_input":"2022-08-11T19:37:49.242151Z","iopub.status.idle":"2022-08-11T19:37:49.254376Z","shell.execute_reply.started":"2022-08-11T19:37:49.242102Z","shell.execute_reply":"2022-08-11T19:37:49.253412Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Comapare\n\nWe check the quality of the features vector of the image. To do this, we calculate pairwise Euclidean distances for all images (as Google wants). For each image, we select 5 nearest neighbors. For these neighbors, we calculate the average coincidence of their classes with the class of the given image. All these values are averaged over all images.","metadata":{"id":"v5n2V3iyahdQ"}},{"cell_type":"code","source":"def metric(vecs, classes, neighbors=5):\n    all = []\n    for i in range(len(vecs)):\n        vec   = vecs[i]        \n        dists =  - torch.tensor( ((vec - vecs)**2.).sum(axis=1) )\n        res = torch.topk(dists, neighbors+1)               # with himself !\n        oks = sum( [classes[i] == classes[r] for r in res.indices ] ) - 1\n        all.append(oks / neighbors)\n        if i % 100 == 0: print(f\"\\r{i:5d}  {np.mean(all):.3f}\", end=\" \")\n        \n    return np.mean(all)    ","metadata":{"id":"zzEBxNwFalJ2","execution":{"iopub.status.busy":"2022-08-11T18:21:54.350310Z","iopub.execute_input":"2022-08-11T18:21:54.350788Z","iopub.status.idle":"2022-08-11T18:21:54.361986Z","shell.execute_reply.started":"2022-08-11T18:21:54.350752Z","shell.execute_reply":"2022-08-11T18:21:54.360670Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(f\"orig:{metric(vectors, classes):.3f}\")     # metric of original, uncompressed vectors","metadata":{"id":"J8pzTwT4eI0Q","outputId":"1052e7c4-b8b0-418d-ec23-76cc790ab328","execution":{"iopub.status.busy":"2022-08-11T19:38:02.664184Z","iopub.execute_input":"2022-08-11T19:38:02.664832Z","iopub.status.idle":"2022-08-11T20:03:37.404398Z","shell.execute_reply.started":"2022-08-11T19:38:02.664796Z","shell.execute_reply":"2022-08-11T20:03:37.403308Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"```\nn = 30201  \nprep1: Resize( (n_px,n_px) )   # default: BILINEAR\nprep2: Resize( (n_px,n_px), interpolation=InterpolationMode.BICUBIC )\nprep3: Resize(n_px, interpolation=InterpolationMode.BICUBIC);  CenterCrop(n_px)\n---------------------------------\n768 -> 64\n           prep1   prep2    prep3\norig_768:  0.927   0.928    0.938 (*)\n\naen_64  :  0.932   0.935    0.934\npca_64  :  0.890   0.890    0.893\npoo_64  :  0.856   0.856    0.863\n---------------------------------\n512 -> 64\norig_512:                   0.858\naen_64  :                   0.853\npca_64  :                   0.821\npoo_64  :                   0.791\n512 -> 32\npca_32  :                   0.749\npoo_32  :                   0.708\n```","metadata":{"id":"3QxCxi1_OI4q"}},{"cell_type":"markdown","source":"## AverPool","metadata":{"id":"ppE5vWMaZrow"}},{"cell_type":"code","source":"DIM = 64\n\npool = nn.AdaptiveAvgPool1d((DIM,))\n\nvectors_pool = pool(torch.tensor(vectors))\nvectors_pool /= vectors_pool.norm(dim=-1, keepdim=True)   # normalize vector\nvectors_pool = vectors_pool.numpy()\nprint(vectors_pool.shape)\n\nprint(f\"pool:{metric(vectors_pool, classes):.3f}\")","metadata":{"id":"DmAgzYO7ZvJf","outputId":"98c37fd7-0576-49ef-8938-539591808798","execution":{"iopub.status.busy":"2022-08-11T20:57:38.023728Z","iopub.execute_input":"2022-08-11T20:57:38.024119Z","iopub.status.idle":"2022-08-11T20:59:04.184617Z","shell.execute_reply.started":"2022-08-11T20:57:38.024086Z","shell.execute_reply":"2022-08-11T20:59:04.183536Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"## PCA","metadata":{"id":"poO7jvz4WgD8"}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=vectors.shape[1])\npca.fit(vectors)\n\nplt.plot(pca.explained_variance_ratio_);\n\npca = PCA(n_components=64)\npca.fit(vectors)\nvectors_pca = pca.transform(vectors)\n\nvectors_pca = torch.tensor(vectors_pca)\nvectors_pca /= vectors_pca.norm(dim=-1, keepdim=True)   # normalize vector\nvectors_pca = vectors_pca.numpy()\nprint(vectors_pca.shape)\n\nprint(f\"pca: {metric(vectors_pca, classes):.3f}\")","metadata":{"id":"WKyl4FaoVtGn","execution":{"iopub.status.busy":"2022-08-11T20:59:26.794343Z","iopub.execute_input":"2022-08-11T20:59:26.794728Z","iopub.status.idle":"2022-08-11T21:01:04.131014Z","shell.execute_reply.started":"2022-08-11T20:59:26.794695Z","shell.execute_reply":"2022-08-11T21:01:04.129972Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"## Autoencoder","metadata":{"id":"c26W82MoxNDC"}},{"cell_type":"code","source":"class Autoencoder(nn.Module):\n    def __init__(self,\n                 inputs = 768,                                      # count of input features\n                 dim    = 64,                                       # count of compress features\n                 hiddens_encoder=[],                                # hiddens layers of encoder\n                 hiddens_decoder=[],                                # hiddens layers of decoder\n                 batchnorm = True,\n                 dropout = 0):                                                        \n        super(Autoencoder, self).__init__()             \n\n        self.encoder = self.net([inputs] + hiddens_encoder + [dim],    dropout, batchnorm, True)\n        self.decoder = self.net([dim]    + hiddens_decoder + [inputs], dropout, batchnorm, False)\n        \n        #self.model =  nn.Sequential(self.encoder, self.decoder)\n \n    def net(self, hiddens, dropout, batchnorm, lastTanh):\n        layers = []\n        for i in range( len(hiddens)-1):            \n            layers += [ nn.Linear(hiddens[i], hiddens[i+1], bias=True)]\n            if batchnorm:\n                layers +=  [ nn.BatchNorm1d(num_features = hiddens[i+1]) ]       \n            if dropout:\n                layers += [ nn.Dropout(dropout) ]                            \n            if lastTanh or i < len(hiddens)- 2:\n                layers += [ nn.Tanh() ]\n        return nn.Sequential(*layers)       \n\n    def forward(self, x):               \n        x = self.encoder(x)\n        x = x / x.norm(dim=-1, keepdim=True)        # !!!\n        return self.decoder(x)","metadata":{"id":"wOVDMuX8w996","execution":{"iopub.status.busy":"2022-08-11T21:01:29.258514Z","iopub.execute_input":"2022-08-11T21:01:29.258888Z","iopub.status.idle":"2022-08-11T21:01:29.270479Z","shell.execute_reply.started":"2022-08-11T21:01:29.258857Z","shell.execute_reply":"2022-08-11T21:01:29.269419Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"CFG = {\n    'encoder':   [1024],\n    'decoder':   [1024],\n    'dropout':   0.01,\n    'batchnorm': True,\n    'batch':     256,\n    'lr':        1e-4,\n    'L2':        0,\n}\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntorch.cuda.empty_cache()\n\naen = Autoencoder(hiddens_encoder=CFG['encoder'], hiddens_decoder=CFG[ 'decoder'], batchnorm = CFG['batchnorm'], dropout = CFG['dropout'])\naen.to(device)\n\nprint(aen)\n#summary(aen, (1,512),  col_names=[\"output_size\", \"num_params\"])","metadata":{"id":"Ygm2rvDXzeKi","execution":{"iopub.status.busy":"2022-08-11T21:08:43.576177Z","iopub.execute_input":"2022-08-11T21:08:43.577146Z","iopub.status.idle":"2022-08-11T21:08:43.605776Z","shell.execute_reply.started":"2022-08-11T21:08:43.577097Z","shell.execute_reply":"2022-08-11T21:08:43.604732Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"### Train\n","metadata":{"id":"wb5DmMXG03p0"}},{"cell_type":"code","source":"def fit(model, X,Y, batch_size=64, train=True):         \n    \"\"\" One epoch training \"\"\"\n    batch_size = min(batch_size, len(X))\n    totLoss, numBatches = 0, 0\n    \n    if train:\n        idx = torch.randperm( len(X) )                  # permuted index list    \n        X, Y = X[idx], Y[idx]             \n    \n    model.train(train)                                  # important for Dropout, BatchNorm layers\n    for i in range(0, len(Y), batch_size):             \n        nb = min(batch_size,  len(Y)-i)\n        xb = X[i: i+nb].to(device)                      # current batch                \n        yb = Y[i: i+nb].to(device) \n        \n        y = model(xb)                                   # forward propagation\n        L = loss(y, yb)                                 # calculate the loss        \n  \n        if train:                                       # we are in learning mode\n            optimizer.zero_grad()                       # reset the gradients        \n            L.backward()                                # calculate gradients           \n            optimizer.step()                            # adjusting parameters\n                                     \n        totLoss += L.detach()                           # total loss (item from graph)\n        numBatches += 1\n                 \n    return ((totLoss/numBatches)**0.5).cpu()            # mean loss \n\ndef plot_train(history):\n    h = np.array(history)                               # learning output\n    plt.figure(figsize=(8,5), facecolor ='w')              \n    ax1 = plt.subplot(1,1,1);  \n    ax1.set( ylim=(0, 0.03), xlim=(0, h[-1,0]) )\n    ax1.grid(color='gray', linestyle='--', alpha=0.6)\n    ax2=ax1.twinx()  \n    ax1.plot(h[:,0], h[:,1], \"-b\")   \n    ax1.plot(h[:,0], h[:,2], \"-g\")   \n\n    ax2.plot(h[:,0], h[:,3], \":r\")   \n\n    pars = [ f\"{k:10s}: {v}\\n\" for k,v in CFG.items() ]\n    ax1.text(h[0,0]+(h[-1,0]-h[0,0])*0.5, 0., \"\".join(pars), {'fontsize':12, 'fontname':'monospace'})\n    plt.show()","metadata":{"id":"aA8DszKw08XH","execution":{"iopub.status.busy":"2022-08-11T21:08:47.601826Z","iopub.execute_input":"2022-08-11T21:08:47.602195Z","iopub.status.idle":"2022-08-11T21:08:47.615952Z","shell.execute_reply.started":"2022-08-11T21:08:47.602164Z","shell.execute_reply":"2022-08-11T21:08:47.614972Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"loss = nn.MSELoss()\n\noptimizer = torch.optim.Adam(aen.parameters(), lr=CFG['lr'], weight_decay=CFG['L2'])   \n\nepochs, tst_batch, TRAIN = 300, 512, 0.8\n\nX = torch.tensor(vectors)\nX = X[ torch.randint(high = len(X), size = (len(X),) )  ]\nX_trn, X_tst = X[:int(len(X)*TRAIN)],  X[int(len(X)*TRAIN):]\nY_trn = torch.clone(X_trn) # ???\nprint(\"trn:\", X_trn.shape, \"tst: \", X_tst.shape)\n\naen.to(device) # just in case\nL_tst = fit(aen, X_trn, Y_trn, batch_size=tst_batch, train=False)\nprint( f\"before:      loss: {L_tst:8.6f}\"  )\n\nhistory,  beg, start = [],  time.process_time(), time.process_time()\nfor epoch in range(1, epochs+1):   \n    L = fit(aen, X_trn, Y_trn, batch_size=CFG['batch'])               # train one epoch\n\n    L_tst = fit(aen, X_tst, X_tst, batch_size=tst_batch, train=False) # test\n    L2 =  sum(p.data.pow(2).sum() for p in aen.parameters()).cpu().item()\n\n    if epoch % 1 == 0:                                                 # learning output   \n        print(f\"\\repoch: {epoch:5d} loss: {L:8.6f} ({L_tst:8.6f}) L2: {L2:.6f}  {time.process_time()-beg:.1f}s\", end=\"\")           \n        beg = time.process_time()                \n\n    history.append([epoch, L, L_tst, L2])\n\n    if epoch % 100  == 0 or epoch == epochs:\n        CFG['time'] = f\"{(time.process_time()-start)/epoch:.1f} s/ep [{(time.process_time()-start)/60:.1f} m]\"\n        plot_train(history)","metadata":{"id":"WgbT6Nsu1l-Y","outputId":"8af1a5dc-3a98-4ca0-ce2b-eb24d2caad55","execution":{"iopub.status.busy":"2022-08-11T21:08:49.943094Z","iopub.execute_input":"2022-08-11T21:08:49.943727Z","iopub.status.idle":"2022-08-11T21:11:16.227434Z","shell.execute_reply.started":"2022-08-11T21:08:49.943690Z","shell.execute_reply":"2022-08-11T21:11:16.226285Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"batch_size = 512\naen.train(False) \nX = torch.tensor(vectors)                 # don't mix!\nvectors_aen = None\nfor i in range(0, len(X), batch_size):             \n    nb = min(batch_size,  len(X)-i)\n    xb = X[i: i+nb].to(device)\n    with torch.no_grad():\n        y = aen.encoder(xb)             \n    vectors_aen = y if vectors_aen is None else torch.cat([vectors_aen, y], axis=0)\n\nvectors_aen /= vectors_aen.norm(dim=-1, keepdim=True) \nvectors_aen = vectors_aen.cpu().numpy()\nprint(vectors_aen.shape)\n\nprint(f\"aen :{metric(vectors_aen, classes):.3f}\")","metadata":{"id":"5xoe2y4m8XIf","outputId":"2250b8f1-5d7b-41f3-d1d7-c596f3f36244","execution":{"iopub.status.busy":"2022-08-11T21:11:26.112970Z","iopub.execute_input":"2022-08-11T21:11:26.113360Z","iopub.status.idle":"2022-08-11T21:12:50.275146Z","shell.execute_reply.started":"2022-08-11T21:11:26.113327Z","shell.execute_reply":"2022-08-11T21:12:50.273998Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"torch.save(aen.state_dict(), 'model_aen_768-1024-64.pt')","metadata":{"execution":{"iopub.status.busy":"2022-08-11T21:14:29.563212Z","iopub.execute_input":"2022-08-11T21:14:29.564303Z","iopub.status.idle":"2022-08-11T21:14:29.592878Z","shell.execute_reply.started":"2022-08-11T21:14:29.564240Z","shell.execute_reply":"2022-08-11T21:14:29.591721Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"## Model for submit","metadata":{}},{"cell_type":"code","source":"class Model_PCA(nn.Module):\n    def __init__(self, dim1=3, dim2=2):                                                        \n        super(Model_PCA, self).__init__()      \n            \n        self.mean = nn.Parameter( torch.zeros((dim1, ),     dtype=torch.float32),  requires_grad = False)\n        self.comp = nn.Parameter( torch.zeros((dim2, dim1), dtype=torch.float32),  requires_grad = False)\n \n    def set(self, mean, comp):  # numpy\n        self.mean.copy_( torch.tensor(mean, dtype=torch.float32) )\n        self.comp.copy_( torch.tensor(comp, dtype=torch.float32) )\n\n    def forward(self, x):               \n        x =  x - self.mean\n        print(x.shape, self.comp.T.shape)\n        x =  torch.mm(x, self.comp.T)        \n        return x\n\nmodel_pca = Model_PCA(768, 64)\nmodel_pca.set(pca.mean_, pca.components_)\ntorch.save(model_pca.state_dict(), 'model_pca.pt')","metadata":{"execution":{"iopub.status.busy":"2022-08-10T18:21:02.591950Z","iopub.execute_input":"2022-08-10T18:21:02.592341Z","iopub.status.idle":"2022-08-10T18:21:02.604296Z","shell.execute_reply.started":"2022-08-10T18:21:02.592307Z","shell.execute_reply":"2022-08-10T18:21:02.603271Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"clip.available_models()","metadata":{"execution":{"iopub.status.busy":"2022-08-10T15:33:36.521212Z","iopub.execute_input":"2022-08-10T15:33:36.521718Z","iopub.status.idle":"2022-08-10T15:33:36.532149Z","shell.execute_reply.started":"2022-08-10T15:33:36.521686Z","shell.execute_reply":"2022-08-10T15:33:36.530702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model_CLIP(nn.Module):\n    def __init__(self, model=\"ViT-L/14@336px\", n_px = 336, device=\"cuda\"):        \n        super(Model_CLIP, self).__init__()     \n        self.n_px = n_px\n        \n        self.model, _ = clip.load(model, jit=True)            \n        #self.pool = nn.AdaptiveAvgPool1d((64,))\n        self.pca = Model_PCA(768, 64)        \n        self.pca.load_state_dict(torch.load('model_pca.pt'))\n\n    def preprocess(self, x):\n        x = transforms.functional.resize(x, size = (self.n_px,), interpolation=InterpolationMode.BILINEAR)\n        x = transforms.functional.center_crop(x, output_size = (self.n_px,) )        \n        x = transforms.functional.normalize(x, mean = [0.48145466, 0.4578275, 0.40821073], \n                                               std = [0.26862954, 0.26130258, 0.27577711])                \n        return x / x.norm()                \n                      \n    def forward(self, x):     \n        x = self.preprocess(x)\n        with torch.no_grad():\n            x = self.model.encode_image(x).float()      \n            x = x / torch.linalg.norm(x, dim=-1, keepdim=True) \n        #x = self.pool(x)\n        x = self.pca(x)\n        x = x / torch.linalg.norm(x, dim=-1, keepdim=True)\n        return x \n\nmodel = Model_CLIP()    \nsaved_model = torch.jit.script( model )\nsaved_model.save('saved_model.pt')    ","metadata":{"execution":{"iopub.status.busy":"2022-08-10T18:25:25.278610Z","iopub.execute_input":"2022-08-10T18:25:25.279004Z","iopub.status.idle":"2022-08-10T18:25:34.188680Z","shell.execute_reply.started":"2022-08-10T18:25:25.278963Z","shell.execute_reply":"2022-08-10T18:25:34.187306Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from zipfile import ZipFile\n\nsaved_model = torch.jit.script(model)\nsaved_model.save('saved_model.pt')\nwith ZipFile('submission.zip','w') as zip:           \n    zip.write('./saved_model.pt', arcname='saved_model.pt') ","metadata":{"execution":{"iopub.status.busy":"2022-08-10T18:25:40.437164Z","iopub.execute_input":"2022-08-10T18:25:40.437853Z","iopub.status.idle":"2022-08-10T18:25:45.762455Z","shell.execute_reply.started":"2022-08-10T18:25:40.437818Z","shell.execute_reply":"2022-08-10T18:25:45.761307Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Autoencoder Hyperparameters\n\n```\n0.855  CFG = {'encoder':[512],     'decoder':[512],     'dropout':0.0, 'batchnorm': True,  'batch':256, 'lr':1e-3, 'L2':0.0}  epochs = 200\n0.854  CFG = {'encoder':[256],     'decoder':[256],     'dropout':0.0, 'batchnorm': True,  'batch':256, 'lr':1e-3, 'L2':0.0}  epochs = 200\n0.850  CFG = {'encoder':[256,128], 'decoder':[128,256], 'dropout':0.0, 'batchnorm': True,  'batch':256, 'lr':1e-4, 'L2':0.0}  epochs = 1000\n0.840  CFG = {'encoder':[256],     'decoder':[256],     'dropout':0.1, 'batchnorm': True,  'batch':256, 'lr':1e-3, 'L2':0.0}  epochs = 200\n0.830  CFG = {'encoder':[256],     'decoder':[256],     'dropout':0.2, 'batchnorm': True,  'batch':256, 'lr':1e-3, 'L2':0.0}  epochs = 200\n```\n- Другой датасет, больше классов\n- Аугументация, cnn-aen\n- \"aen\" в вероятности","metadata":{"id":"RqsIr5rQzciR"}}]}