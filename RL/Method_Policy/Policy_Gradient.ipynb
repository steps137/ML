{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "013294ab",
   "metadata": {},
   "source": [
    "# Policy Gradient method\n",
    "```\n",
    "!pip3 install box2d-py\n",
    "!pip3 install gym[Box_2D]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c22ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#========================================================================================\n",
    "\n",
    "class AgentModel(nn.Module):\n",
    "    \"\"\" Neural network for pi(s,a) \"\"\"\n",
    "    def __init__(self, sizes, hidden=nn.ReLU, output=nn.Identity):\n",
    "        super(AgentModel, self).__init__()        \n",
    "        layers = []        \n",
    "        for i in range(len(sizes)-1):            \n",
    "            activation = hidden if i < len(sizes)-2 else output\n",
    "            layers += [ nn.Linear(sizes[i], sizes[i+1]), activation() ]        \n",
    "        self.model = nn.Sequential(*layers)\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.model(x)        \n",
    "    \n",
    "\n",
    "class GradientPolicy:\n",
    "    def __init__(self, env):\n",
    "        self.env  = env                         # environment we work with\n",
    "        self.low  = env.observation_space.low   # minimum observation values\n",
    "        self.high = env.observation_space.high  # maximum observation values\n",
    "        self.nA   =  self.env.action_space.n    # number of discrete actions\n",
    "        self.nS   =  self.env.observation_space.shape[0] # number of state components\n",
    "        self.action_space = np.arange(env.action_space.n)    \n",
    "\n",
    "        self.config = {             # default parameters\n",
    "            'method'   : \"GradPolicy\",# kind of the method (GradPolicy)     \n",
    "            'gamma'    : 0.99,      # discount factor\n",
    "            'eps1'     : 1.0,       # initial value epsilon\n",
    "            'eps2'     : 0.001,     # final value   epsilon\n",
    "            'decays'   : 1000,      # number of episodes to decay eps1 - > eps2\n",
    "            'update'   : 10,        # target model update rate (in frames = time steps)         \n",
    "            'batch'    : 100,       # batch size for training\n",
    "            'capacity' : 100000,    # memory size\n",
    "            'rewrite'  : 1.0,       # rewrite memory (if < 1 - sorted)\n",
    "            'hiddens'  : [256,128], # hidden layers\n",
    "            'scale'    : True,      # scale or not observe to [-1...1]\n",
    "            'loss'     : 'huber',     # loss function (mse, huber)\n",
    "            'optimizer': 'sgd',     # optimizer (sgd, adam)\n",
    "            'lr'       : 0.001,     # learning rate           \n",
    "        }\n",
    "        self.last_loss = 0.         # last loss\n",
    "\n",
    "        print(\"low :   \", self.low)\n",
    "        print(\"high:   \", self.high)        \n",
    "                \n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def init(self):\n",
    "        \"\"\" Create a neural network and optimizer \"\"\"\n",
    "\n",
    "        #self.device = \"cpu\"\n",
    "        self.device =torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")        \n",
    "        print(\"device:\", self.device)\n",
    "\n",
    "        self.model  = AgentModel([self.nS]+self.config['hiddens']+[self.nA]).to(self.device)      # current Q        \n",
    "\n",
    "        self.best_model  = AgentModel([self.nS]+self.config['hiddens']+[self.nA]).to(self.device) # best net\n",
    "        self.best_reward = -100000                                                           # best reward\n",
    "\n",
    "        if   self.config['loss'] == 'mse':\n",
    "             self.loss  = nn.MSELoss()\n",
    "        elif self.config['loss'] == 'huber':\n",
    "             self.loss = nn.HuberLoss()\n",
    "        else:\n",
    "            print(\"ERROR: Unknown loss function!!!\")\n",
    "        \n",
    "        if   self.config['optimizer'] == 'sgd':\n",
    "             self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.config['lr'], momentum=0.8)\n",
    "        elif self.config['optimizer'] == 'adam':\n",
    "             self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config['lr'])\n",
    "        else:\n",
    "            print(\"ERROR: Unknown optimizer!!!\")        \n",
    "        \n",
    "        self.epsilon     = self.config['eps1']        # start value in epsilon greedy strategy\n",
    "        self.decay_rate  = math.exp(math.log(self.config['eps2']/self.config['eps1'])/self.config['decays'])\n",
    "\n",
    "        print(f\"decay_rate: {self.decay_rate:.4f}\")\n",
    "        print(self.model)                    \n",
    "        \n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def scale(self, obs):\n",
    "        \"\"\" to [-1...1] \"\"\"\n",
    "        if self.config['scale']:\n",
    "            return -1. + 2.*(obs - self.low)/(self.high-self.low)\n",
    "        else:\n",
    "            return obs\n",
    "        \n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def policy(self, state):\n",
    "        \"\"\" Return action according to epsilon greedy strategy \"\"\"\n",
    "        if np.random.random() < self.epsilon:                        \n",
    "            return np.random.randint(self.nA)    # random action            \n",
    "\n",
    "        x = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            y = self.model(x).detach().to('cpu')\n",
    "        probs = torch.softmax(y, 0).numpy()      \n",
    "        return np.random.choice(self.action_space, p=probs)         # action by probability\n",
    "    \n",
    "    #------------------------------------------------------------------------------------\n",
    "    \n",
    "    def create_memory(self, capacity):                \n",
    "        \"\"\" Сreate a memory for states, actions and rewards \"\"\"\n",
    "        self.capacity = capacity \n",
    "        self.count    = 0                \n",
    "        self.memo_S   = [None]*capacity \n",
    "        self.memo_A   = [None]*capacity\n",
    "        self.memo_R   = [None]*capacity\n",
    "    \n",
    "    #------------------------------------------------------------------------------------\n",
    "    \n",
    "    def append_memory(self, S, A, R):\n",
    "        \"\"\" Add to memory data lists \"\"\"\n",
    "        for i in range(len(S)):\n",
    "            index = self.count % self.capacity\n",
    "            self.memo_S[index] = S[i]\n",
    "            self.memo_A[index] = A[i]\n",
    "            self.memo_R[index] = R[i]\n",
    "            self.count += 1\n",
    "\n",
    "    #------------------------------------------------------------------------------------\n",
    "    \n",
    "    def discount_rewards(self, rewards):\n",
    "        \"\"\" \"\"\"        \n",
    "        gamma = self.config['gamma']                \n",
    "        if self.config.get('discount') == 1:            \n",
    "            res = np.zeros( (len(rewards),) )\n",
    "            cum = 0\n",
    "            for t in range(0, len(rewards), -1):\n",
    "                cum = rewards[t] + cum*gamma\n",
    "                res[t] = cum                         \n",
    "            return res #- np.mean(res)\n",
    "        elif self.config.get('discount') == -1:\n",
    "            r = np.array([gamma**i * rewards[i] for i in range(len(rewards))])\n",
    "            r = r[::-1].cumsum()[::-1]\n",
    "            return r - r.mean()  \n",
    "        else:\n",
    "            return [np.sum(rewards)]*len(rewards)    \n",
    "                \n",
    "    #------------------------------------------------------------------------------------\n",
    "    \n",
    "    def run_episode(self, ticks):\n",
    "        \"\"\" Complete one episode \"\"\"\n",
    "        states, actions, rewards = [], [], []\n",
    "        state, tot_rew = self.env.reset(), 0\n",
    "        for t in range(1, ticks+1):                \n",
    "            action = self.policy(state)\n",
    "            \n",
    "            actions.append( action )\n",
    "            states. append( state.tolist() )                \n",
    "            \n",
    "            state, rew, done, _ = self.env.step(action)               \n",
    "            tot_rew += rew\n",
    "            rewards.append(rew)                \n",
    "                \n",
    "            if done:        \n",
    "                break                                                             \n",
    "        \n",
    "        self.append_memory( states, actions, self.discount_rewards(rewards) )\n",
    "        \n",
    "        return tot_rew, t \n",
    "            \n",
    "    #------------------------------------------------------------------------------------\n",
    "    \n",
    "    def learn(self, episodes=1000, stat1 = 10, stat2 = 100, plots = 1000, rews_range = [-1000, 1000]):                                \n",
    "        self.create_memory(self.config['capacity'])\n",
    "        self.history  = [] \n",
    "        self.beg_time = time.process_time()\n",
    "        rews, lens, beg  = [], [], time.process_time()\n",
    "        for episode in range(1, episodes+1):                       \n",
    "            rew, t = self.run_episode( self.config['ticks'] )\n",
    "            rews.append(rew)\n",
    "            lens.append(t)\n",
    "\n",
    "            self.epsilon *= self.decay_rate                # epsilon-decay\n",
    "            if self.epsilon < self.config['eps2']:\n",
    "                self.epsilon = 0.                \n",
    "            \n",
    "            if self.count >= self.capacity:                # start learning\n",
    "                self.learn_model()\n",
    "                if self.config['clear']:                   \n",
    "                    self.count = 0                         # wait new data until full capacity\n",
    "\n",
    "            if episode % stat1 == 0:\n",
    "                self.history.append([episode, np.mean(rews[-stat1:]), np.mean(rews[-stat2:])])      \n",
    "                                \n",
    "            if  episode % stat2 == 0:                               \n",
    "                mean, std    = np.mean(rews[-stat2:]), np.std(rews[-stat2:])    \n",
    "                lensM, lensS = np.mean(lens[-stat2:]), np.std(lens[-stat2:])                    \n",
    "                if mean > self.best_reward:\n",
    "                    self.best_reward = mean\n",
    "                    self.best_model.load_state_dict( self.model.state_dict() )                     \n",
    "                \n",
    "                print(f\"{episode:6d} rew:{mean:7.1f} ± {std/stat2**0.5:3.1f}, best:{self.best_reward:7.2f}, ticks:{lensM:3.0f}, eps:{self.epsilon:.3f},  loss:{self.last_loss:7.3f}, {(time.process_time() - beg):3.0f}s\")\n",
    "                beg = time.process_time()\n",
    "                \n",
    "            if  episode % plots == 0:                   \n",
    "                self.plot(f\"{self.config['env']}  Episode: {episode}  best: {self.best_reward:7.1f}  time: {time.process_time()-self.beg_time:.0f}s\", rews_range)     \n",
    "                          \n",
    "        print(f\"time = {time.process_time()-self.beg_time: .0f}s\")\n",
    "    \n",
    "    #------------------------------------------------------------------------------------\n",
    "    \n",
    "    def learn_model(self):\n",
    "        \"\"\" Model Training \"\"\"\n",
    "        num = min(self.capacity, self.count)\n",
    "        if num == 0:\n",
    "            return        \n",
    "       \n",
    "        S = torch.FloatTensor(self.memo_S[:num]).to(self.device)\n",
    "        W = torch.FloatTensor(self.memo_R[:num]).to(self.device)                    \n",
    "        A = torch.LongTensor (self.memo_A[:num]).to(self.device) # Actions are used as indices, must be LongTensor        \n",
    "        \n",
    "        batch = min(num, self.config['batch'])\n",
    "        for epoch in range(1, self.config['epochs']+1):\n",
    "            idx = torch.randperm( len(S) ).to(self.device)                       \n",
    "            S, W, A = S[idx], W[idx], A[idx]\n",
    "        \n",
    "            tot_L,  num_B = 0, int( len(S)/batch ) \n",
    "            for i in range(0, num_B*batch, batch):          \n",
    "                sb, wb, ab = S[i: i+batch], W[i: i+batch], A[i: i+batch]                         \n",
    "            \n",
    "                probs = torch.softmax( self.model(sb), 1 )          # !!!\n",
    "                logprob = (probs+1.e-8).log()                     \n",
    "                logprob = torch.gather(logprob, 1, ab.view(-1,1)).squeeze()     \n",
    "                #wb = F.softmax(wb, 0)                       \n",
    "                loss = -(wb * logprob).mean()                       # Calculate loss                    \n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()                                     # Calculate gradients                    \n",
    "                self.optimizer.step()                               # Apply gradients\n",
    "\n",
    "                tot_L += loss.item()                \n",
    "                \n",
    "        self.last_loss = tot_L / num_B\n",
    "\n",
    "    #------------------------------------------------------------------------------------\n",
    "    \n",
    "    def plot(self, text, rews_range):\n",
    "        \"\"\" Plot histogram for states and actions \"\"\"        \n",
    "        num = min(self.count, self.capacity)\n",
    "        if num == 0:\n",
    "            return\n",
    "                \n",
    "        hist_S, bins_S = np.histogram(self.memo_S, bins=np.linspace(0, math.sqrt(self.nS), 101), density=True)        \n",
    "        hist_A, bins_A = np.histogram(self.memo_A, bins=np.linspace(-0.5, self.nA-0.5, self.nA+1), density=True)    \n",
    "\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(16,6), gridspec_kw={'width_ratios': [2, 1, 5]})        \n",
    "        plt.suptitle(text, fontsize=18)\n",
    "                                \n",
    "        ax[0].set_xlim(min(bins_S), max(bins_S))    # histogram for S1\n",
    "        ax[0].grid(axis='x', alpha=0.75); ax[0].grid(axis='y', alpha=0.75)\n",
    "        ax[0].set_xlabel('|s1|', fontsize=16)\n",
    "        bins = [ (bins_S[i]+bins_S[i+1])/2 for i in range(len(bins_S)-1)]\n",
    "        ax[0].bar(bins, hist_S, width=0.5, color='blue')\n",
    "                        \n",
    "        ax[1].set_xlim(min(bins_A), max(bins_A))    # histogram for A\n",
    "        ax[1].grid(axis='x', alpha=0.75); ax[1].grid(axis='y', alpha=0.75)\n",
    "        ax[1].set_xlabel('actions', fontsize=16)\n",
    "        ax[1].set_xticks(np.arange(self.nA));\n",
    "        bins = [ (bins_A[i]+bins_A[i+1])/2 for i in range(len(bins_A)-1)]        \n",
    "        ax[1].bar(bins, hist_A, width=0.5, color='blue')\n",
    "\n",
    "        history = np.array(self.history)            # loss history\n",
    "        ax[2].plot(history[:,0], history[:,1], linewidth=1)\n",
    "        ax[2].plot(history[:,0], history[:,2], linewidth=2)\n",
    "        ax[2].set_ylim(rews_range[0], rews_range[1]);\n",
    "        ax[2].set_xlabel('episode', fontsize=16)        \n",
    "        ax[2].grid(axis='x', alpha=0.75); ax[2].grid(axis='y', alpha=0.75)\n",
    "        params = [ f\"{k:9s}: {v}\\n\" for k,v in self.config.items()]\n",
    "        ax[2].text(history[0,0], rews_range[0], \"\".join(params), {'fontsize':12, 'fontname':'monospace'})\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdc948d",
   "metadata": {},
   "source": [
    "## CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fc8e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v0\"    # (nS=4, nA=2)\n",
    "ml = GradientPolicy( gym.make(env_name) )\n",
    "\n",
    "ml.config = {\n",
    "   'env'      : env_name,\n",
    "    'ticks'    : 200,      \n",
    "    'method'   : \"GradPolicy\",# kind of the method (GradPolicy)     \n",
    "    'eps1'     : 1.0,         # initial value epsilon\n",
    "    'eps2'     : 0.001,       # final value   epsilon\n",
    "    'decays'   : 500,         # number of episodes to decay eps1 - > eps2   \n",
    "    'discount' : 0,           # use descount future reward or total reward\n",
    "    'gamma'    : 0.99,        # discount factor\n",
    "    'clear'    : False,       # clear memory after learn\n",
    "    'epochs'   : 2,           # number of epochs for batch learning\n",
    "    'batch'    : 1000,        # batch size for training\n",
    "    'capacity' : 2000,        # memory size    \n",
    "    'hiddens'  : [32],        # hidden layers\n",
    "    'scale'    : False,       # scale or not observe to [-1...1]\n",
    "    'loss'     : 'mse',       # loss function (mse, huber)\n",
    "    'optimizer': 'adam',      # optimizer (sgd, adam)\n",
    "    'lr'       : 0.001,        # learning rate          \n",
    "}\n",
    "\n",
    "ml.init()\n",
    "ml.learn(episodes=3000,  rews_range = [0, 210])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1860d341",
   "metadata": {},
   "source": [
    "## LunarLander-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c536c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"LunarLander-v2\"    # (nS=4, nA=2)\n",
    "ml = GradientPolicy( gym.make(env_name) )\n",
    "\n",
    "ml.config = {\n",
    "    'env'      : env_name,\n",
    "    'ticks'    : 500,      \n",
    "    'method'   : \"GradPolicy\",# kind of the method (GradPolicy)     \n",
    "    'discount' : 0,           # use descount future reward or total reward\n",
    "    'gamma'    : 1,           # discount factor\n",
    "    'eps1'     : 1.0,         # initial value epsilon\n",
    "    'eps2'     : 0.001,       # final value   epsilon\n",
    "    'decays'   : 500,         # number of episodes to decay eps1 - > eps2   \n",
    "    'epochs'   : 1,\n",
    "    'batch'    : 1000,         # batch size for training\n",
    "    'capacity' : 1000,        # memory size    \n",
    "    'hiddens'  : [32,32],     # hidden layers\n",
    "    'scale'    : False,       # scale or not observe to [-1...1]\n",
    "    'loss'     : 'mse',       # loss function (mse, huber)\n",
    "    'optimizer': 'adam',      # optimizer (sgd, adam)\n",
    "    'lr'       : 0.001,        # learning rate           \n",
    "}\n",
    "\n",
    "ml.init()\n",
    "ml.learn(episodes=3000,  rews_range = [0, 510])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc4f3c0",
   "metadata": {},
   "source": [
    "## MountainCar-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc7178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"MountainCar-v0\"    # (nS=4, nA=2)\n",
    "ml = GradientPolicy( gym.make(env_name) )\n",
    "\n",
    "ml.config = {\n",
    "    'env'      : env_name,\n",
    "    'ticks'    : 200,      \n",
    "    'method'   : \"GradPolicy\",# kind of the method (GradPolicy)     \n",
    "    'eps1'     : 1.0,         # initial value epsilon\n",
    "    'eps2'     : 0.9,         # final value   epsilon\n",
    "    'decays'   : 1000,        # number of episodes to decay eps1 - > eps2   \n",
    "    'discount' : 0,           # use descount future reward or total reward\n",
    "    'gamma'    : 1,           # discount factor\n",
    "    'clear'    : False,        # c\n",
    "    'epochs'   : 20,\n",
    "    'batch'    : 100,        # batch size for training\n",
    "    'capacity' : 2000,        # memory size    \n",
    "    'hiddens'  : [256,128],   # hidden layers\n",
    "    'scale'    : True,       # scale or not observe to [-1...1]\n",
    "    'loss'     : 'mse',       # loss function (mse, huber)\n",
    "    'optimizer': 'adam',      # optimizer (sgd, adam)\n",
    "    'lr'       : 0.01,        # learning rate          \n",
    "}\n",
    "\n",
    "ml.init()\n",
    "ml.learn(episodes=3000,  rews_range = [-200, -80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505f8cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6], \n",
    "                  [7, 8, 9]])\n",
    "a = torch.tensor([[0], [2], [1]])\n",
    "t.gather(1, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = (11, 11)                      # число интервалов по каждой оси\n",
    "low  = np.array([-1.2, -0.07])       # минимальные значения наблюдения\n",
    "high = np.array([ 0.6,  0.07])       # минимальные значения наблюдения\n",
    "bin  = (high-low)/bins               # ширины интервалов\n",
    "\n",
    "def index(state):                    # вещественный state в пару индексов\n",
    "    indx = ((state - low) / bin).clip(np.zeros_like(low), np.array(bins)-1)\n",
    "    return tuple( indx.astype(int) )\n",
    "\n",
    "index(np.array([-1,0.05]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
