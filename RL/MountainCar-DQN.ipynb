{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc31610",
   "metadata": {},
   "source": [
    "# MountainCar DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96adfa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt   \n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "951e275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    \"\"\" DQN метод для дискретных действий \"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.env     = env                                  # среда с которой мы работаем\n",
    "        self.obs_min = env.observation_space.low            # минимальные значения наблюдений\n",
    "        self.obs_max = env.observation_space.high           # максимальные значения наблюдений\n",
    "        self.nS      =  self.env.observation_space.shape[0] # число переменных состояния\n",
    "        self.nA      =  self.env.action_space.n             # число дискретных действий\n",
    "\n",
    "        self.gamma     = 0.99      # дисконтирующий множитель\n",
    "        self.eps       = 1.        # эпсилон-жадная стратегия\n",
    "        self.eps_decay = 0.995      # скорость распада эпсилон\n",
    "        self.eps_min   = 1e-3      # после него eps обнуляется\n",
    "\n",
    "        self.epochs    = 1         # число эпох обучения        \n",
    "        self.batch     = 10        # размер батча для обучения\n",
    "        self.capacity  = 1000      # величина памяти      \n",
    "        self.learn_step = 5        # через сколько эпизодов учим\n",
    "\n",
    "        print(\"obs_min\", self.obs_min)  \n",
    "        print(\"obs_max\", self.obs_max)  \n",
    "\n",
    "    #---------------------------------------------------------------------------------------    \n",
    "\n",
    "    def init(self, nH = 32):\n",
    "        \"\"\" Сформировать нейронную сеть c nH нейронами в скрытом слое \"\"\"            \n",
    "        nX = self.nS + self.nA\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(nX, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1) )\n",
    "              \n",
    "        self.loss      = nn.MSELoss()\n",
    "        #self.optimizer = torch.optim.SGD(self.model.parameters(), lr=0.00001, momentum=0.8)     \n",
    "        self.optimizer  = torch.optim.Adam(self.model.parameters(), lr=0.00001)\n",
    "\n",
    "        self.best_reward = -1e5\n",
    "\n",
    "    #---------------------------------------------------------------------------------------\n",
    "\n",
    "    def scale(self, obs):           \n",
    "        return -1. + 2.*(obs - self.obs_min)/(self.obs_max-self.obs_min)\n",
    "\n",
    "    #---------------------------------------------------------------------------------------\n",
    "\n",
    "    def getQ(self, state):           \n",
    "        \"\"\"\n",
    "        Значения Q при данном state и различных состояниях\n",
    "        \"\"\"\n",
    "        actions = torch.eye(self.nA,  dtype=torch.float32)\n",
    "        states  = torch.tensor(state, dtype=torch.float32).repeat(self.nA, 1)\n",
    "        x = torch.cat([states, actions], axis=1)\n",
    "        with torch.no_grad():\n",
    "            y = self.model(x).detach().numpy()\n",
    "        return y                                # (num_actions,)\n",
    "\n",
    "    #---------------------------------------------------------------------------------------\n",
    "\n",
    "    def policy(self, state):           \n",
    "        \"\"\"\n",
    "        Вернуть действие в соответствии с epsilon-жадной стратегией \n",
    "        \"\"\"\n",
    "        if np.random.random() < self.eps:               # случайное действие: \n",
    "            action = np.random.randint(0, self.nA)\n",
    "            #action = 2*int(state[1] > 0)          \n",
    "            y = self.getQ(state)\n",
    "            return action, y[action][0]\n",
    "        \n",
    "        y = self.getQ(state)\n",
    "        action =  np.argmax(y)   \n",
    "        Q      =  np.max(y)         \n",
    "        return action, Q                                 # лучшее действие\n",
    "\n",
    "    #---------------------------------------------------------------------------------------\n",
    "\n",
    "    def maxQ(self, state):           \n",
    "        \"\"\"\n",
    "        Вычислить максимальное значение ценности\n",
    "        \"\"\"\n",
    "        y = self.getQ(state)\n",
    "        Q =  np.max(y)\n",
    "        return Q                    \n",
    "\n",
    "    #---------------------------------------------------------------------------------------\n",
    "    \n",
    "    def test(self, episodes = 1000, ticks = 1000):\n",
    "        \"\"\" \n",
    "        Тестирование с неизменной Q-функцией \n",
    "        \"\"\"\n",
    "        rews = []\n",
    "        for _ in range(episodes):        \n",
    "            obs =  self.env.reset()\n",
    "            tot = 0\n",
    "            for _ in range(ticks):\n",
    "                obs = -1. + 2.*(obs - self.obs_min)/(self.obs_max-self.obs_min)\n",
    "                action, _ = self.policy(obs)                \n",
    "                obs, rew, done, _ = self.env.step(action) \n",
    "\n",
    "                tot += rew\n",
    "                if done:                    \n",
    "                    break\n",
    "            rews.append(tot)\n",
    "\n",
    "        print(f\"Reward[{episodes},{ticks}]: {np.mean(rews):>7.2f} ± {np.std(rews):>.1f}\")\n",
    "\n",
    "    #---------------------------------------------------------------------------------------\n",
    "    \n",
    "    def learn(self, lm = 0.1, episodes = 100000, ticks = 200):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.memoX, self.memoR, self.memoQ = [], [], []\n",
    "        rews, As, beg   = [], [], time.process_time()\n",
    "        for episode in range(episodes):        \n",
    "            s0 = self.env.reset()             \n",
    "            s0 = self.scale(s0)\n",
    "            a0, _ = self.policy(s0)  \n",
    "            tot = 0\n",
    "            for t in range(ticks):                                   \n",
    "                As.append(float(a0))\n",
    "                s1, r1, done, _ = self.env.step(a0)                  \n",
    "                s1 = self.scale(s1)\n",
    "                tot += r1\n",
    "                \n",
    "                actions = [0.]*self.nA;  actions[a0] = 1.               \n",
    "                self.memoX.append(s0.tolist() + actions)\n",
    "                if s1[0] > 0:\n",
    "                    r1 += s1[0]\n",
    "                self.memoR.append( r1 )\n",
    "                \n",
    "                a1, Q1 = self.policy(s1)  \n",
    "\n",
    "                if done and t+1 < ticks:\n",
    "                    Q1 = 0\n",
    "                self.memoQ.append(Q1)                 \n",
    "\n",
    "                if done:                    \n",
    "                    break\n",
    "                    \n",
    "                s0, a0 = s1, a1                \n",
    "\n",
    "            rews.append(tot)\n",
    "\n",
    "            self.memoX  = self.memoX[-self.capacity:]     # обрезаем память\n",
    "            self.memoR  = self.memoR[-self.capacity:]\n",
    "            self.memoQ  = self.memoQ[-self.capacity:]            \n",
    "\n",
    "            if episode == 0:\n",
    "                print(f\"R: {tot:7.2f}  Q: {np.mean(self.memoQ):8.2f} ± {np.std(self.memoQ):.0e}\")\n",
    "\n",
    "            if episode and episode % self.learn_step:\n",
    "                self.learn_model()                         # обучаем модель\n",
    "\n",
    "            self.eps *= self.eps_decay                     # epsilon-распад\n",
    "            if self.eps < self.eps_min:\n",
    "                self.eps = 0.\n",
    "            \n",
    "            if episode and episode % 100 == 0:\n",
    "                mean, std = np.mean(rews[-100:]), np.std(rews[-100:])\n",
    "                print(f\"{episode:5d} R: {mean:7.2f} ± {std:.0e},  epsilon: {self.eps:.0e},  Q: {np.mean(self.memoQ):8.2f} ± {np.std(self.memoQ):.0e}, loss: {self.lastL:.1e}, As:{np.mean(As[-100:]):.2f} ± {np.std(As[-100:]):.0e} best: {self.best_reward:7.2f}, time: {(time.process_time() - beg):3.0f}s\")\n",
    "                beg = time.process_time()\n",
    "\n",
    "                if mean > self.best_reward:\n",
    "                    self.best_reward = mean                \n",
    "\n",
    "    #---------------------------------------------------------------------------------------\n",
    "\n",
    "    def learn_model(self):\n",
    "        \"\"\"\n",
    "        Обучение модели\n",
    "        \"\"\"        \n",
    "        X = torch.tensor(self.memoX, dtype=torch.float32)\n",
    "        R = torch.tensor(self.memoR, dtype=torch.float32).view(-1,1)\n",
    "        Q = torch.tensor(self.memoQ, dtype=torch.float32).view(-1,1)\n",
    "\n",
    "        idx = torch.randperm( len(X) )     # перемешанный список индексов\n",
    "        X, R, Q = X[idx], R[idx], Q[idx]\n",
    "        \n",
    "        for epoch in range(self.epochs):                     # эпоха - проход по всем примерам\n",
    "            numB, sumL = int( len(X)/self.batch), 0.\n",
    "            for i in range(0, numB*self.batch, self.batch):          \n",
    "                xb, rb, qb = X[i: i+self.batch], R[i: i+self.batch], Q[i: i+self.batch]                        \n",
    "                yb = rb + self.gamma*qb\n",
    "            \n",
    "                y = self.model(xb)                           # прямое распространение    # m = torch.mean(y)                                                                                     \n",
    "                L = self.loss(y, yb)                         #  L = torch.mean((y-yb)**2) - 0.1*torch.mean((y-m)**2)          # вычисляем ошибку\n",
    "                L += 0.01*torch.mean(y*y)\n",
    "            \n",
    "                self.optimizer.zero_grad()                   # обнуляем градиенты        \n",
    "                L.backward()                                 # вычисляем градиенты            \n",
    "                self.optimizer.step()                        # подправляем параметры \n",
    "\n",
    "                sumL += L.detach().item()\n",
    "            self.lastL = sumL/numB\n",
    "\n",
    "    #---------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0c422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "model = DQN( env )\n",
    "model.init()\n",
    "model.learn()\n",
    "#model.test(episodes = 10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
