{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db80df20",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.compat.v1.distribute' has no attribute 'Normal'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 74>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m delta_placeholder \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mplaceholder(tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     72\u001b[0m target_placeholder \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mplaceholder(tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 74\u001b[0m action_tf_var, norm_dist \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_placeholder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m V \u001b[38;5;241m=\u001b[39m value_function(state_placeholder)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# define actor (policy) loss function\u001b[39;00m\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36mpolicy_network\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     41\u001b[0m sigma \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mdense(hidden2, n_outputs, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     42\u001b[0m sigma \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftplus(sigma) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-5\u001b[39m\n\u001b[1;32m---> 43\u001b[0m norm_dist \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNormal\u001b[49m(mu, sigma)\n\u001b[0;32m     44\u001b[0m action_tf_var \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msqueeze(norm_dist\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     45\u001b[0m action_tf_var \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mclip_by_value(\n\u001b[0;32m     46\u001b[0m     action_tf_var, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow[\u001b[38;5;241m0\u001b[39m], \n\u001b[0;32m     47\u001b[0m     env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.compat.v1.distribute' has no attribute 'Normal'"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "import numpy as np\n",
    "import gym  #requires OpenAI gym installed\n",
    "env = gym.envs.make(\"MountainCarContinuous-v0\") \n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "input_dims = 2\n",
    "state_placeholder = tf.placeholder(tf.float32, [None, input_dims]) \n",
    "\n",
    "def value_function(state):\n",
    "    n_hidden1 = 400  \n",
    "    n_hidden2 = 400\n",
    "    n_outputs = 1\n",
    "    \n",
    "    with tf.variable_scope(\"value_network\"):\n",
    "        #init_xavier = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        hidden1 = tf.layers.dense(state, n_hidden1, tf.nn.elu)\n",
    "        hidden2 = tf.layers.dense(hidden1, n_hidden2, tf.nn.elu) \n",
    "        V = tf.layers.dense(hidden2, n_outputs, None)\n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_network(state):\n",
    "    n_hidden1 = 40\n",
    "    n_hidden2 = 40\n",
    "    n_outputs = 1\n",
    "    \n",
    "    with tf.variable_scope(\"policy_network\"):\n",
    "        #init_xavier = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        hidden1 = tf.layers.dense(state, n_hidden1, tf.nn.elu)\n",
    "        hidden2 = tf.layers.dense(hidden1, n_hidden2, tf.nn.elu)\n",
    "        mu = tf.layers.dense(hidden2, n_outputs, None)\n",
    "        sigma = tf.layers.dense(hidden2, n_outputs, None)\n",
    "        sigma = tf.nn.softplus(sigma) + 1e-5\n",
    "        norm_dist = tf.distribute.distributions.Normal(mu, sigma)\n",
    "        action_tf_var = tf.squeeze(norm_dist.sample(1), axis=0)\n",
    "        action_tf_var = tf.clip_by_value(\n",
    "            action_tf_var, env.action_space.low[0], \n",
    "            env.action_space.high[0])\n",
    "    return action_tf_var, norm_dist\n",
    "\n",
    "################################################################\n",
    "#sample from state space for state normalization\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "                                    \n",
    "state_space_samples = np.array(\n",
    "    [env.observation_space.sample() for x in range(10000)])\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(state_space_samples)\n",
    "\n",
    "#function to normalize states\n",
    "def scale_state(state):                 #requires input shape=(2,)\n",
    "    scaled = scaler.transform([state])\n",
    "    return scaled                       #returns shape =(1,2)   \n",
    "###################################################################\n",
    "\n",
    "lr_actor = 0.00002  #set learning rates\n",
    "lr_critic = 0.001\n",
    "\n",
    "# define required placeholders\n",
    "action_placeholder = tf.placeholder(tf.float32)\n",
    "delta_placeholder = tf.placeholder(tf.float32)\n",
    "target_placeholder = tf.placeholder(tf.float32)\n",
    "\n",
    "action_tf_var, norm_dist = policy_network(state_placeholder)\n",
    "V = value_function(state_placeholder)\n",
    "\n",
    "# define actor (policy) loss function\n",
    "loss_actor = -tf.log(norm_dist.prob(action_placeholder) + 1e-5) * delta_placeholder\n",
    "training_op_actor = tf.train.AdamOptimizer(\n",
    "    lr_actor, name='actor_optimizer').minimize(loss_actor)\n",
    "\n",
    "# define critic (state-value) loss function\n",
    "loss_critic = tf.reduce_mean(tf.squared_difference(\n",
    "                             tf.squeeze(V), target_placeholder))\n",
    "training_op_critic = tf.train.AdamOptimizer(\n",
    "        lr_critic, name='critic_optimizer').minimize(loss_critic)\n",
    "################################################################\n",
    "#Training loop\n",
    "gamma = 0.99        #discount factor\n",
    "num_episodes = 300\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    episode_history = []\n",
    "    for episode in range(num_episodes):\n",
    "        #receive initial state from E\n",
    "        state = env.reset()   # state.shape -> (2,)\n",
    "        reward_total = 0 \n",
    "        steps = 0\n",
    "        done = False\n",
    "        while (not done):\n",
    "                \n",
    "            #Sample action according to current policy\n",
    "            #action.shape = (1,1)\n",
    "            action  = sess.run(action_tf_var, feed_dict={\n",
    "                          state_placeholder: scale_state(state)})\n",
    "            #Execute action and observe reward & next state from E\n",
    "            # next_state shape=(2,)    \n",
    "            #env.step() requires input shape = (1,)\n",
    "            next_state, reward, done, _ = env.step(\n",
    "                                    np.squeeze(action, axis=0)) \n",
    "            steps +=1\n",
    "            reward_total += reward\n",
    "            #V_of_next_state.shape=(1,1)\n",
    "            V_of_next_state = sess.run(V, feed_dict = \n",
    "                    {state_placeholder: scale_state(next_state)})  \n",
    "            #Set TD Target\n",
    "            #target = r + gamma * V(next_state)     \n",
    "            target = reward + gamma * np.squeeze(V_of_next_state) \n",
    "            \n",
    "            # td_error = target - V(s)\n",
    "            #needed to feed delta_placeholder in actor training\n",
    "            td_error = target - np.squeeze(sess.run(V, feed_dict = \n",
    "                        {state_placeholder: scale_state(state)})) \n",
    "            \n",
    "            #Update actor by minimizing loss (Actor training)\n",
    "            _, loss_actor_val  = sess.run(\n",
    "                [training_op_actor, loss_actor], \n",
    "                feed_dict={action_placeholder: np.squeeze(action), \n",
    "                state_placeholder: scale_state(state), \n",
    "                delta_placeholder: td_error})\n",
    "            #Update critic by minimizinf loss  (Critic training)\n",
    "            _, loss_critic_val  = sess.run(\n",
    "                [training_op_critic, loss_critic], \n",
    "                feed_dict={state_placeholder: scale_state(state), \n",
    "                target_placeholder: target})\n",
    "            \n",
    "            state = next_state\n",
    "            #end while\n",
    "        episode_history.append(reward_total)\n",
    "        print(\"Episode: {}, Number of Steps : {}, Cumulative reward: {:0.2f}\".format(\n",
    "            episode, steps, reward_total))\n",
    "        \n",
    "        if np.mean(episode_history[-100:]) > 90 and len(episode_history) >= 101:\n",
    "            print(\"****************Solved***************\")\n",
    "            print(\"Mean cumulative reward over 100 episodes:{:0.2f}\" .format(\n",
    "                np.mean(episode_history[-100:])))\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
