{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc31610",
   "metadata": {},
   "source": [
    "# MountainCar DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa0c422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MemoryBuffer:\n",
    "    \"\"\" \n",
    "    Overwrite memory for storing the environment model.\n",
    "    This implementation is not the most efficient, but it is needed for experimenting with sorting.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity, n_states, n_actions):\n",
    "        \"\"\"\n",
    "        capacity  - memory capacity (maximum number of stored items)\n",
    "        n_states  - number of state variables\n",
    "        n_actions - number of actions\n",
    "        \"\"\"\n",
    "        self.capacity = capacity    # maximum number of stored elements\n",
    "        self.count = 0              # how many items have already been saved\n",
    "        self.index = 0              # index of element to be inserted\n",
    "        self.nS = n_states\n",
    "        self.nA = n_actions\n",
    "\n",
    "        self.memo = np.zeros( (capacity, self.nS*2 + 4), dtype = np.float32)\n",
    "\n",
    "    #------------------------------------------------------------------------------------        \n",
    "    \n",
    "    def add(self, s0, a0, s1, r1, done):\n",
    "        \"\"\" Add example to memory \"\"\"\n",
    "        self.index = self.index % self.capacity\n",
    "        \n",
    "        norm = np.dot(s1,s1)**0.5  \n",
    "        self.memo[self.index] = np.concatenate(( s0, s1, [a0], [r1], [1.-done], [norm]) )\n",
    "\n",
    "        self.index += 1\n",
    "        self.count += 1\n",
    "\n",
    "        #if self.count == self.capacity or (self.count > self.capacity and self.index >= self.capacity // 2):\n",
    "        #    self.memo = self.memo[self.memo[:, -1].argsort() ]  # [::-1]\n",
    "        #    self.index = 0\n",
    "        \n",
    "    #------------------------------------------------------------------------------------\n",
    "        \n",
    "    def samples(self, count):\n",
    "        \"\"\" Return count of random examples from memory \"\"\"\n",
    "        mem_max = min(self.count, self.capacity)\n",
    "        indxs = np.random.choice(mem_max, count, replace=False)\n",
    "        sample = self.memo[indxs]\n",
    "        s0 = sample[:, 0:           self.nS]\n",
    "        s1 = sample[:, self.nS:     2*self.nS]\n",
    "        a0 = sample[:, 2*self.nS:   2*self.nS+1]\n",
    "        r1 = sample[:, 2*self.nS+1: 2*self.nS+2]\n",
    "        dn = sample[:, 2*self.nS+2: 2*self.nS+3]\n",
    "        return torch.tensor(s0), torch.tensor(a0, dtype = torch.int64), torch.tensor(s1), torch.tensor(r1), torch.tensor(dn)\n",
    "\n",
    "    #------------------------------------------------------------------------------------\n",
    "    \n",
    "    def plot(self, text, history, params):\n",
    "        \"\"\" Plot histogram for states and actions \"\"\"\n",
    "        num = min(self.count, self.capacity)\n",
    "        if num == 0:\n",
    "            return\n",
    "\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(16,6), gridspec_kw={'width_ratios': [2, 1, 5]})        \n",
    "        plt.suptitle(text, fontsize=18)\n",
    "        \n",
    "        bins = np.linspace(0, math.sqrt(self.nS), 101)\n",
    "        s1 = self.memo[:num, self.nS: 2*self.nS]\n",
    "        hist, bins = np.histogram(s1, bins=bins, density=True)\n",
    "        ax[0].set_xlim(min(bins), max(bins))\n",
    "        ax[0].grid(axis='x', alpha=0.75); ax[0].grid(axis='y', alpha=0.75)\n",
    "        ax[0].set_xlabel('|s1|', fontsize=16)\n",
    "        bins = [ (bins[i]+bins[i+1])/2 for i in range(len(bins)-1)]        \n",
    "        ax[0].bar(bins, hist, width=0.5, color='blue')\n",
    "                \n",
    "        bins = np.linspace(-0.5, self.nA-0.5, self.nA+1)        \n",
    "        a = self.memo[:num, 2*self.nS: 2*self.nS+1],\n",
    "        hist, bins = np.histogram(a, bins=bins, density=True)\n",
    "        ax[1].set_xlim(min(bins), max(bins))\n",
    "        ax[1].grid(axis='x', alpha=0.75); ax[1].grid(axis='y', alpha=0.75)\n",
    "        ax[1].set_xlabel('actions', fontsize=16)\n",
    "        ax[1].set_xticks(np.arange(self.nA));\n",
    "        bins = [ (bins[i]+bins[i+1])/2 for i in range(len(bins)-1)]        \n",
    "        ax[1].bar(bins, hist, width=0.5, color='blue')\n",
    "\n",
    "        ax[2].plot(history[:,0], history[:,1], linewidth=1)\n",
    "        ax[2].set_ylim(-200, -80);\n",
    "        ax[2].set_xlabel('episode', fontsize=16)        \n",
    "        ax[2].grid(axis='x', alpha=0.75); ax[2].grid(axis='y', alpha=0.75)\n",
    "        params = [ f\"{k:9s}: {v}\\n\" for k,v in params.items()]\n",
    "        ax[2].text(100, -200, \"\".join(params), {'fontsize':12, 'fontname':'monospace'})\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "#========================================================================================\n",
    "\n",
    "class DQN:\n",
    "    \"\"\" DQN метод для дискретных действий \"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.env     = env                         # environment we work with\n",
    "        self.obs_min = env.observation_space.low   # minimum observation values\n",
    "        self.obs_max = env.observation_space.high  # maximum observation values\n",
    "        self.nA      =  self.env.action_space.n    # number of discrete actions\n",
    "        self.nS      =  self.env.observation_space.shape[0] # number of state components\n",
    "\n",
    "        self.params = {           # default parameters\n",
    "            'method'   : \"DQN\",     # kind of the method (DQN, DDQN)     \n",
    "            'gamma'    : 0.99,      # discount factor\n",
    "            'eps1'     : 1.0,       # initial value epsilon\n",
    "            'eps2'     : 0.001,     # final value   epsilon\n",
    "            'decays'   : 1000,      # number of episodes to decay eps1 - > eps2\n",
    "            'update'   : 10,        # target model update rate (in frames = time steps)         \n",
    "            'batch'    : 100,       # batch size for training\n",
    "            'capacity' : 100000,    # memory size\n",
    "            'hiddens'  : [256,128], # hidden layers\n",
    "            'scale'    : True,      # scale or not observe to [-1...1]\n",
    "            'optimizer': 'sgd',     # optimizer (sgd, adam)\n",
    "            'lm'       : 0.001,     # learning rate           \n",
    "        }\n",
    "        self.last_loss = 0.       # last loss\n",
    "        self.history = []\n",
    "\n",
    "        print(\"obs_min:   \", self.obs_min)\n",
    "        print(\"obs_max:   \", self.obs_max)\n",
    "        print(\"obs_shape: \", self.obs_max)\n",
    "        \n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def get_model(self, hiddens):\n",
    "        \"\"\" Create a neural network \"\"\"\n",
    "\n",
    "        neurons = [self.nS] + hiddens + [self.nA]\n",
    "        layers  = []\n",
    "        for i in range(len(neurons)-1):\n",
    "            layers.append(nn.Linear(neurons[i], neurons[i+1]) )\n",
    "            if i < len(neurons)-2:\n",
    "                layers.append( nn.ReLU() )\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def init(self):\n",
    "        \"\"\" Create a neural network and optimizer \"\"\"\n",
    "\n",
    "        self.gpu =torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"device:\", self.gpu)\n",
    "\n",
    "        self.model  = self.get_model(self.params['hiddens']).to(self.gpu)      # current Q\n",
    "        self.target = self.get_model(self.params['hiddens']).to(self.gpu)      # target  Q\n",
    "\n",
    "        self.best_model  = self.get_model(self.params['hiddens']).to(self.gpu) # best net\n",
    "        self.best_reward = -100000                                             # best reward\n",
    "\n",
    "        self.loss      = nn.MSELoss()\n",
    "        if   self.params['optimizer'] == 'sgd':\n",
    "            self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.params['lm'], momentum=0.8)\n",
    "        elif self.params['optimizer'] == 'adam':\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.params['lm'])\n",
    "        else:\n",
    "            print(\"ERROR: Unknown optimizer!!!\")\n",
    "\n",
    "        self.memo = MemoryBuffer(self.params['capacity'], self.nS, self.nA)\n",
    "        self.maxQ = torch.zeros( (self.params['batch'], ),  dtype=torch.float32).to(self.gpu)\n",
    "        \n",
    "        self.epsilon     = self.params['eps1']        # start value in epsilon greedy strategy\n",
    "        self.decay_rate  = math.exp(math.log(self.params['eps2']/self.params['eps1'])/self.params['decays'])\n",
    "\n",
    "        print(f\"decay_rate: {self.decay_rate:.4f}\")\n",
    "        print(self.model)        \n",
    "\n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def copy_model(self, model):\n",
    "        model.load_state_dict( self.model.state_dict() )\n",
    "\n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def scale(self, obs):\n",
    "        \"\"\" to [-1...1] \"\"\"\n",
    "        if self.params['scale']:\n",
    "            return -1. + 2.*(obs - self.obs_min)/(self.obs_max-self.obs_min)\n",
    "        else:\n",
    "            return obs\n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def policy(self, state):\n",
    "        \"\"\" Return action according to epsilon greedy strategy \"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            #return 2*int(state[1] > 0)\n",
    "            return np.random.randint(self.nA)    # random action\n",
    "\n",
    "        x = torch.tensor(state, dtype=torch.float32).to(self.gpu)\n",
    "        with torch.no_grad():\n",
    "            y = self.model(x).detach().to('cpu').numpy() \n",
    "        return np.argmax(y)                      # best action\n",
    "\n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def run_episode(self, ticks = 200):\n",
    "        \"\"\" Run one episode, keeping the environment model in memory \"\"\"\n",
    "        rew = 0                                  # total reward\n",
    "        s0 = self.env.reset()                    # initial state\n",
    "        s0 = self.scale (s0)                     # scale it\n",
    "        a0 = self.policy(s0)                     # get action\n",
    "        for t in range(ticks):\n",
    "            s1, r1, done, _ = self.env.step(a0)\n",
    "            s1 = self.scale (s1)\n",
    "            a1 = self.policy(s1)\n",
    "\n",
    "            self.memo.add(s0, a0, s1, r1, float(done and t < ticks) )            \n",
    "\n",
    "            if self.frame % self.params['update'] == 0:\n",
    "                self.copy_model(self.target)\n",
    "\n",
    "            if self.memo.count >= self.params['batch']:    \n",
    "                self.learn_model()                         \n",
    "\n",
    "            rew += r1\n",
    "            self.frame += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            s0, a0 = s1, a1\n",
    "        return rew\n",
    "\n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def learn(self, episodes = 100000, ticks = 200, stat = 100, plots = 1000, env_name = \"\"):\n",
    "        \"\"\" Repeat episodes episodes times \"\"\"\n",
    "        self.frame = 1\n",
    "        rews, mean, beg   = [], 0, time.process_time()\n",
    "        for episode in range(1, episodes+1):\n",
    "            rew = self.run_episode(ticks)\n",
    "            rews.append( rew )\n",
    "\n",
    "            self.epsilon *= self.decay_rate                # epsilon-decay\n",
    "            if self.epsilon < self.params['eps2']:\n",
    "                self.epsilon = 0.\n",
    "\n",
    "            if  episode % stat == 0:                               \n",
    "                mean, std = np.mean(rews[-stat:]), np.std(rews[-stat:])                \n",
    "                self.history.append([episode, mean])      \n",
    "                if mean > self.best_reward:\n",
    "                    self.best_reward = mean\n",
    "                    self.copy_model(self.best_model)\n",
    "                maxQ = self.maxQ.to('cpu')\n",
    "                print(f\"{episode:6d} rew: {mean:7.2f} ± {std/stat**0.5:4.2f},  best: {self.best_reward:7.2f},  epsilon: {self.epsilon:.3f},  Q:{maxQ.mean():8.2f} ± {maxQ.std():7.3f}, loss:{self.last_loss:7.3f}, time:{(time.process_time() - beg):3.0f}s\")\n",
    "                beg = time.process_time()\n",
    "\n",
    "            if  episode % plots == 0:                   \n",
    "                self.memo.plot(f\"{env_name}  Episode: {episode}  best: {self.best_reward:7.1f}\", np.array(self.history), self.params)\n",
    "            \n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def learn_model(self):\n",
    "        \"\"\" Model Training \"\"\"\n",
    "        batch = self.params['batch']\n",
    "        \n",
    "        S0, A0, S1, R1, Dn = self.memo.samples(batch)\n",
    "        S0 = S0.to(self.gpu); A0 = A0.to(self.gpu)\n",
    "        S1 = S1.to(self.gpu); R1 = R1.to(self.gpu);  Dn = Dn.to(self.gpu)\n",
    "        \n",
    "        if self.params['method'] == 'DQN':\n",
    "            with torch.no_grad():\n",
    "                y = self.target(S1).detach()\n",
    "            self.maxQ, _ = torch.max(y, 1)      # maximum Q values for S1\n",
    "        elif self.params['method'] == 'DDQN':\n",
    "            y = self.model(S1)            \n",
    "            a = torch.argmax(y,1).view(-1,1)            \n",
    "            q = self.target(S1)            \n",
    "            self.maxQ = q.gather(1, a)            \n",
    "        else:\n",
    "            \n",
    "            print(\"Unknown method\")\n",
    "            \n",
    "        sum_loss = 0        \n",
    "        s0, a0 = S0, A0.view(-1,1)\n",
    "        r1, dn = R1.view(-1,1), Dn.view(-1,1)\n",
    "        q1     = self.maxQ.view(-1,1)\n",
    "\n",
    "        yb = r1 + self.params['gamma']*q1*dn\n",
    "\n",
    "        y = self.model(s0)             # forward\n",
    "        y = y.gather(1, a0)\n",
    "        L = self.loss(y, yb)\n",
    "\n",
    "        self.optimizer.zero_grad()     # reset the gradients\n",
    "        L.backward()                   # calculate gradients\n",
    "        self.optimizer.step()          # adjusting parameters\n",
    "\n",
    "        sum_loss += L.detach().item()\n",
    "\n",
    "        self.last_loss = sum_loss\n",
    "\n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def test(self, episodes = 1000, ticks = 1000):\n",
    "        \"\"\" Q-Function Testing \"\"\"\n",
    "        rews = []\n",
    "        for episode in range(1, episodes+1):\n",
    "            tot = 0\n",
    "            obs =  self.env.reset()\n",
    "            for _ in range(ticks):\n",
    "                action = self.policy( self.scale(obs) )\n",
    "                obs, rew, done, _ = self.env.step(action)\n",
    "                tot += rew\n",
    "                if done:\n",
    "                    break\n",
    "            rews.append(tot)\n",
    "            if episode % 100:\n",
    "                print(f\"\\r {episode:4d}: Reward: {np.mean(rews):7.3f} ± {np.std(rews)/len(rews)**0.5:.3f}\", end=\"\")\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "998aa843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs_min:    [-1.2  -0.07]\n",
      "obs_max:    [0.6  0.07]\n",
      "obs_shape:  [0.6  0.07]\n",
      "device: cuda:0\n",
      "decay_rate: 0.9863\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#env_name = \"CartPole-v0\"\n",
    "env_name = \"MountainCar-v0\"\n",
    "env = gym.make(env_name)\n",
    "\n",
    "dqn = DQN( env )\n",
    "\n",
    "dqn.params = {\n",
    "    'method'   : \"DDQN\",     # kind of the method (DQN, DDQN)     \n",
    "    'gamma'    : 0.99,      # discount factor\n",
    "    'eps1'     : 1.0,       # initial value epsilon\n",
    "    'eps2'     : 0.001,     # final value   epsilon\n",
    "    'decays'   : 500,       # number of episodes to decay eps1 - > eps2\n",
    "    'update'   : 10,        # target model update rate (in frames = time steps)             \n",
    "    'batch'    : 100,       # batch size for training\n",
    "    'capacity' : 100000,    # memory size\n",
    "    'hiddens'  : [256,128], # hidden layers\n",
    "    'scale'    : True,      # scale or not observe to [-1...1]\n",
    "    'optimizer': 'sgd',     # optimizer (sgd, adam)\n",
    "    'lm'       : 0.001,     # learning rate           \n",
    "}\n",
    "\n",
    "dqn.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0aa280cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'DDQN', 'gamma': 0.99, 'eps1': 1.0, 'eps2': 0.001, 'decays': 500, 'update': 10, 'batch': 100, 'capacity': 100000, 'hiddens': [256, 128], 'scale': True, 'optimizer': 'sgd', 'lm': 0.001}\n",
      "   100 rew: -197.67 ± 1.05,  best: -197.67,  epsilon: 0.251,  Q:  -58.40 ±  11.492, loss:  0.942, time: 70s\n",
      "   200 rew: -169.07 ± 2.37,  best: -169.07,  epsilon: 0.063,  Q:  -49.72 ±  14.894, loss:  8.459, time: 75s\n",
      "   300 rew: -139.72 ± 2.89,  best: -139.72,  epsilon: 0.016,  Q:  -49.60 ±  17.068, loss:  0.267, time: 78s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(dqn\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mticks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplots\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, episodes, ticks, stat, plots, env_name)\u001b[0m\n\u001b[0;32m    236\u001b[0m rews, mean, beg   \u001b[38;5;241m=\u001b[39m [], \u001b[38;5;241m0\u001b[39m, time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, episodes\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 238\u001b[0m     rew \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m     rews\u001b[38;5;241m.\u001b[39mappend( rew )\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecay_rate                \u001b[38;5;66;03m# epsilon-decay\u001b[39;00m\n",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36mDQN.run_episode\u001b[1;34m(self, ticks)\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemo\u001b[38;5;241m.\u001b[39mcount \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m]:    \n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m                         \n\u001b[0;32m    222\u001b[0m rew \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r1\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36mDQN.learn_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m\"\"\" Model Training \"\"\"\u001b[39;00m\n\u001b[0;32m    262\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 264\u001b[0m S0, A0, S1, R1, Dn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m S0 \u001b[38;5;241m=\u001b[39m S0\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu); A0 \u001b[38;5;241m=\u001b[39m A0\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu)\n\u001b[0;32m    266\u001b[0m S1 \u001b[38;5;241m=\u001b[39m S1\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu); R1 \u001b[38;5;241m=\u001b[39m R1\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu);  Dn \u001b[38;5;241m=\u001b[39m Dn\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu)\n",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36mMemoryBuffer.samples\u001b[1;34m(self, count)\u001b[0m\n\u001b[0;32m     55\u001b[0m r1 \u001b[38;5;241m=\u001b[39m sample[:, \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m: \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     56\u001b[0m dn \u001b[38;5;241m=\u001b[39m sample[:, \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m: \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms0\u001b[49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mtensor(a0, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mint64), torch\u001b[38;5;241m.\u001b[39mtensor(s1), torch\u001b[38;5;241m.\u001b[39mtensor(r1), torch\u001b[38;5;241m.\u001b[39mtensor(dn)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(dqn.params)\n",
    "dqn.learn(episodes = 5000, ticks = 200, stat=100, plots = 1000, env_name = env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2be137",
   "metadata": {},
   "source": [
    "## Test best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf649ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.model.load_state_dict( dqn.best_model.state_dict() )\n",
    "dqn.test(episodes = 1000, ticks=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea39509",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc24f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "  \n",
    "state = {'info':      \"MountainCar: Q-function, Reward:  -97.422 ± 0.2, std: 7, [-108, -83]\",     \n",
    "         'date':      datetime.datetime.now(),  \n",
    "         'model':     str(dqn.best_model),\n",
    "         'state' :    dqn.best_model.state_dict(),  \n",
    "        } \n",
    " \n",
    "torch.save(state, 'MountainCar_Q_2_256_128_1.97.4.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebefcdae",
   "metadata": {},
   "source": [
    "## Plot policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2bd9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from   matplotlib import colors\n",
    "\n",
    "bins = (101, 101)   \n",
    "low  = dqn.obs_min   # minimum observation values\n",
    "high = dqn.obs_max   # maximum observation values\n",
    "step = (high-low)/bins\n",
    "\n",
    "def table(model, bins = (101, 101)):    \n",
    "    \"\"\" Get 2D table of policy function \"\"\"\n",
    "    res = np.empty(bins)\n",
    "    step = (high-low)/bins\n",
    "    indx = torch.cartesian_prod(torch.arange(0, bins[0]), torch.arange(0, bins[1]))\n",
    "    gpu  = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    X = torch.tensor(low, dtype = torch.float32) + indx*torch.tensor(step, dtype = torch.float32)        \n",
    "    X = -1. + 2.*(X - low)/(high-low)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        X = X.to(gpu)\n",
    "        Y = model.to(gpu)(X).cpu()     \n",
    "        print(Y.shape)\n",
    "        Y = torch.argmax(Y, 1).float()\n",
    "        print(Y.shape)\n",
    "    \n",
    "    return Y.view(bins[0], bins[1]).numpy()\n",
    "\n",
    "def plot(mat, title, d_ticks=1000, bins = (101,101)):\n",
    "    \"\"\" Plot table \"\"\"\n",
    "    def index(state):        \n",
    "        indx = ((state - low)/step).astype(int)\n",
    "        indx = np.clip(indx, (0,0), (bins[0]-1, bins[1]-1))\n",
    "        return tuple( indx )\n",
    "\n",
    "    g, m, x0   = index([0.5, 0.]), index([-math.pi/6, 0.]), np.array([ index([-0.6, 0.]), index([-0.4, 0.]) ])\n",
    "    cmap = colors.ListedColormap(['blue', 'white', 'red'])    \n",
    "    \n",
    "    plt.imshow(mat.T, interpolation='none', origin='lower', cmap= cmap, alpha=0.5)\n",
    "\n",
    "    plt.title (title, {'fontsize': 16})\n",
    "    plt.xlabel('x', {'fontsize': 16});         plt.ylabel('v', {'fontsize': 16}) \n",
    "    plt.axhline(g[1], c=\"black\", linewidth=1); plt.axvline(g[0], c=\"black\", linewidth=1)\n",
    "    plt.axvline(m[0], c=\"black\", linewidth=1)\n",
    "    plt.axvline(x0[0][0], c=\"black\", linewidth=2, ymin = 0.49, ymax = 0.51)\n",
    "    plt.axvline(x0[1][0], c=\"black\", linewidth=2, ymin = 0.49, ymax = 0.51)\n",
    "    ticks = range(0, bins[0], d_ticks)\n",
    "    plt.xticks( ticks, np.round(100*np.linspace(low[0], high[0], len(ticks)))/100 )\n",
    "    plt.yticks( ticks, np.round(100*np.linspace(low[1], high[1], len(ticks)))/100 )\n",
    "\n",
    "     \n",
    "plt.figure(figsize=(10,10))\n",
    "res = table(dqn.best_model, bins = bins)\n",
    "print(res.shape, res.min(), res.max(), res.mean())\n",
    "plot(res, 'Policy function, Reward: -98.1 ± 0.1 in [-107, -83]', d_ticks=100, bins = bins)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
