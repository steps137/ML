{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95d9c134",
   "metadata": {},
   "source": [
    "# Ray library for RL\n",
    "\n",
    "`pip install ray[rllib]`<br>\n",
    "On 2022-jun there was a problem with the latest version of the OpenAI Gym library. Such a command downgrades the version of this library.<br>\n",
    "https://www.youtube.com/watch?v=krz8SCds7yA  полезная серия видио"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de380fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.8.10', ray_version='1.12.1', ray_commit='4863e33856b54ccf8add5cbe75e41558850a1b75', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': 'tcp://127.0.0.1:54903', 'raylet_socket_name': 'tcp://127.0.0.1:60756', 'webui_url': '', 'session_dir': 'C:\\\\Users\\\\Admin\\\\AppData\\\\Local\\\\Temp\\\\ray\\\\session_2022-06-05_21-09-28_457296_5216', 'metrics_export_port': 54386, 'gcs_address': '127.0.0.1:57632', 'address': '127.0.0.1:57632', 'node_id': '7eb35983a81732198e63fb19ad646152f9d9c7cf3587b78722959cfe'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47debf47",
   "metadata": {},
   "source": [
    "## Choosing an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c207d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENV = \"CartPole-v1\"\n",
    "ENV = \"MountainCar-v0\"\n",
    "#ENV = \"BipedalWalker-v3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11915373",
   "metadata": {},
   "source": [
    "## Set up file locations for checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d32e701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "CHECKPOINT_ROOT = \"tmp/\"+ENV\n",
    "shutil.rmtree(CHECKPOINT_ROOT, ignore_errors=True, onerror=None)\n",
    "\n",
    "RAY_RESULTS = \"tmp/ray_results/\"\n",
    "shutil.rmtree(RAY_RESULTS, ignore_errors=True, onerror=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa9697",
   "metadata": {},
   "source": [
    "## Choose a method\n",
    "\n",
    "- https://docs.ray.io/en/latest/rllib/rllib-algorithms.html - List of methods\n",
    "- https://docs.ray.io/en/latest/rllib/rllib-training.html  - Common parameters\n",
    "\n",
    "### 1. PPO: Proximal Policy Optimization   \n",
    "<b>Actions</b> discrete:`Yes`, continuous:`Yes`<br>\n",
    "Doesn't work for MountainCar-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1d28673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 19:42:41,656\tINFO trainer.py:2295 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2022-06-03 19:42:41,658\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-06-03 19:42:41,660\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24992)\u001b[0m 2022-06-03 19:42:46,201\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22932)\u001b[0m 2022-06-03 19:42:46,275\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "2022-06-03 19:42:48,266\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"log_level\"] = \"WARN\"\n",
    "\n",
    "agent = ppo.PPOTrainer(config, env=ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8051077",
   "metadata": {},
   "source": [
    "### 2. DQN: Deep Q-Network\n",
    "<b>Actions</b> discrete:`Yes`, continuous:`No`<br>\n",
    "Possible for MountainCar-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "627f2858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 21:59:41,538\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 21:59:43,542\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "import ray.rllib.agents.dqn as dqn\n",
    "\n",
    "config = dqn.DEFAULT_CONFIG.copy()\n",
    "config['metrics_num_episodes_for_smoothing'] = 100    # def: 100 (для вычисления mean, min, max)\n",
    "config[\"framework\"]                          = 'tf'   # def: 'tf',  можно  'torch' \n",
    "print(config['model'])\n",
    "\n",
    "agent = dqn.DQNTrainer(config, env=ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016c5753",
   "metadata": {},
   "source": [
    "### 3. DDPG  (TD3):  Deep Deterministic Policy Gradients\n",
    "<b>Actions</b> discrete:`No`, continuous:`Yes`<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0504718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 21:57:17,026\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 21:57:18,842\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "import ray.rllib.agents.ddpg as ddpg\n",
    "\n",
    "config = ddpg.DEFAULT_CONFIG.copy()\n",
    "print(config['model'])\n",
    "\n",
    "agent = ddpg.DDPGTrainer(config, env=ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29525759",
   "metadata": {},
   "source": [
    "### 4. SAC\n",
    "<b>Actions</b> discrete:`Yes`, continuous:`Yes`<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "992ced67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 19:53:19,905\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "2022-06-03 19:53:20,895\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "import ray.rllib.agents.sac as sac\n",
    "\n",
    "config = sac.DEFAULT_CONFIG.copy()\n",
    "\n",
    "agent = sac.SACTrainer(config, env=ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592a3c0f",
   "metadata": {},
   "source": [
    "### 5. PG:  vanila  Policy Gradients\n",
    "<b>Actions</b> discrete:`Yes`, continuous:`Yes`<br>\n",
    "Ray: we include a vanilla policy gradients implementation as an example algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5001800f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 21:16:51,601\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "2022-06-05 21:16:51,623\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "import ray.rllib.agents.pg as pg\n",
    "\n",
    "config = pg.DEFAULT_CONFIG.copy()\n",
    "config[\"framework\"] = 'torch'\n",
    "\n",
    "agent = pg.PGTrainer(config, env=ENV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393ec984",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54d75aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 episodes:   11  mean:-122.86  (-169.54, -110.17) tmp/BipedalWalker-v3\\checkpoint_000002\\checkpoint-2\n",
      "  2 episodes:   13  mean:-123.59  (-169.54, -110.17) tmp/BipedalWalker-v3\\checkpoint_000002\\checkpoint-2\n",
      "  3 episodes:   17  mean:-118.77  (-169.54, -89.92) tmp/BipedalWalker-v3\\checkpoint_000004\\checkpoint-4\n",
      "  4 episodes:   38  mean:-110.95  (-169.54, -89.92) tmp/BipedalWalker-v3\\checkpoint_000005\\checkpoint-5\n",
      "  5 episodes:   59  mean:-110.59  (-169.54, -89.92) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      "  6 episodes:   80  mean:-114.47  (-169.54, -89.92) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      "  7 episodes:  101  mean:-116.01  (-138.34, -89.92) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      "  8 episodes:  121  mean:-118.25  (-126.65, -96.19) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      "  9 episodes:  133  mean:-120.42  (-147.42, -96.19) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 10 episodes:  154  mean:-123.34  (-147.42, -105.28) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 11 episodes:  171  mean:-122.81  (-147.42, -106.13) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 12 episodes:  182  mean:-122.12  (-147.42, -106.13) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 13 episodes:  192  mean:-121.18  (-147.42, -101.98) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 14 episodes:  199  mean:-121.02  (-147.42, -101.98) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 15 episodes:  213  mean:-120.44  (-147.42, -101.98) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 16 episodes:  227  mean:-119.89  (-141.84, -101.98) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 17 episodes:  231  mean:-119.96  (-141.84, -101.98) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 18 episodes:  233  mean:-120.05  (-141.84, -101.98) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 19 episodes:  235  mean:-120.46  (-153.59, -101.98) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 20 episodes:  236  mean:-120.59  (-153.59, -101.98) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 21 episodes:  236  mean:-120.59  (-153.59, -101.98) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 22 episodes:  237  mean:-120.56  (-153.59, -101.98) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 23 episodes:  237  mean:-120.56  (-153.59, -101.98) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 24 episodes:  238  mean:-120.67  (-153.59, -101.98) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 25 episodes:  244  mean:-121.93  (-196.07, -101.98) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 26 episodes:  253  mean:-122.14  (-196.07, -101.98) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 27 episodes:  264  mean:-122.72  (-196.07, -101.98) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 28 episodes:  277  mean:-121.70  (-196.07, -101.98) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 29 episodes:  287  mean:-121.34  (-196.07, -102.25) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 30 episodes:  300  mean:-119.39  (-196.07, -101.97) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 31 episodes:  313  mean:-119.00  (-196.07, -101.97) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 32 episodes:  324  mean:-120.06  (-196.07, -101.97) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 33 episodes:  336  mean:-119.89  (-196.07, -101.97) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 34 episodes:  336  mean:-119.89  (-196.07, -101.97) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 35 episodes:  346  mean:-118.52  (-158.96, -101.97) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 36 episodes:  357  mean:-118.39  (-158.96, -101.97) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 37 episodes:  368  mean:-117.88  (-158.96, -101.97) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 38 episodes:  373  mean:-117.68  (-158.96, -101.97) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 39 episodes:  377  mean:-119.34  (-250.36, -101.97) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 40 episodes:  382  mean:-119.68  (-250.36, -101.97) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 41 episodes:  386  mean:-121.08  (-253.57, -101.97) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 42 episodes:  388  mean:-121.76  (-253.57, -101.97) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 43 episodes:  396  mean:-122.13  (-253.57, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 44 episodes:  402  mean:-122.26  (-253.57, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 45 episodes:  409  mean:-121.76  (-253.57, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 46 episodes:  415  mean:-121.57  (-253.57, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 47 episodes:  422  mean:-121.19  (-253.57, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 48 episodes:  427  mean:-121.39  (-253.57, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 49 episodes:  435  mean:-121.02  (-253.57, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 50 episodes:  436  mean:-120.94  (-253.57, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 51 episodes:  440  mean:-122.13  (-253.57, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 52 episodes:  442  mean:-122.53  (-253.57, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 53 episodes:  443  mean:-122.59  (-253.57, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 54 episodes:  446  mean:-123.80  (-253.57, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 55 episodes:  449  mean:-124.18  (-253.57, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 56 episodes:  450  mean:-124.35  (-253.57, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 57 episodes:  453  mean:-125.10  (-253.57, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 58 episodes:  456  mean:-125.53  (-253.57, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 59 episodes:  457  mean:-126.03  (-253.57, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 60 episodes:  458  mean:-127.57  (-263.92, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 61 episodes:  459  mean:-128.79  (-263.92, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 62 episodes:  459  mean:-128.79  (-263.92, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 63 episodes:  460  mean:-128.72  (-263.92, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 64 episodes:  460  mean:-128.72  (-263.92, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 65 episodes:  461  mean:-128.91  (-263.92, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 66 episodes:  462  mean:-129.31  (-263.92, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 67 episodes:  462  mean:-129.31  (-263.92, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 68 episodes:  463  mean:-129.89  (-263.92, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 69 episodes:  464  mean:-131.11  (-263.92, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 70 episodes:  464  mean:-131.11  (-263.92, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n",
      " 71 episodes:  465  mean:-131.67  (-263.92, -89.88) tmp/BipedalWalker-v3\\checkpoint_000006\\checkpoint-6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m best_mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100000\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_ITER):\n\u001b[1;32m----> 4\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[0;32m      6\u001b[0m     mean \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode_reward_mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mean \u001b[38;5;241m>\u001b[39m best_mean:\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ray\\tune\\trainable.py:349\u001b[0m, in \u001b[0;36mTrainable.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warmup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[0;32m    348\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 349\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py:1074\u001b[0m, in \u001b[0;36mTrainer.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m step_ctx\u001b[38;5;241m.\u001b[39mshould_stop(step_attempt_results):\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1074\u001b[0m         step_attempt_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_attempt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;66;03m# @ray.remote RolloutWorker failure.\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RayError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1077\u001b[0m         \u001b[38;5;66;03m# Try to recover w/o the failed worker.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py:1155\u001b[0m, in \u001b[0;36mTrainer.step_attempt\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1153\u001b[0m \u001b[38;5;66;03m# No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluate_this_iter:\n\u001b[1;32m-> 1155\u001b[0m     step_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exec_plan_or_training_iteration_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;66;03m# We have to evaluate in this training iteration.\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;66;03m# No parallelism.\u001b[39;00m\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_parallel_to_training\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py:2174\u001b[0m, in \u001b[0;36mTrainer._exec_plan_or_training_iteration_fn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2172\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_iteration()\n\u001b[0;32m   2173\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2174\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_exec_impl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ray\\util\\iter.py:779\u001b[0m, in \u001b[0;36mLocalIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_once()\n\u001b[1;32m--> 779\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilt_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ray\\util\\iter.py:807\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_foreach\u001b[39m(it):\n\u001b[1;32m--> 807\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    808\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[0;32m    809\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ray\\util\\iter.py:869\u001b[0m, in \u001b[0;36mLocalIterator.filter.<locals>.apply_filter\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_filter\u001b[39m(it):\n\u001b[1;32m--> 869\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    870\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics_context():\n\u001b[0;32m    871\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady) \u001b[38;5;129;01mor\u001b[39;00m fn(item):\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ray\\util\\iter.py:869\u001b[0m, in \u001b[0;36mLocalIterator.filter.<locals>.apply_filter\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_filter\u001b[39m(it):\n\u001b[1;32m--> 869\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    870\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics_context():\n\u001b[0;32m    871\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady) \u001b[38;5;129;01mor\u001b[39;00m fn(item):\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ray\\util\\iter.py:807\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_foreach\u001b[39m(it):\n\u001b[1;32m--> 807\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    808\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[0;32m    809\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ray\\util\\iter.py:869\u001b[0m, in \u001b[0;36mLocalIterator.filter.<locals>.apply_filter\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_filter\u001b[39m(it):\n\u001b[1;32m--> 869\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    870\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics_context():\n\u001b[0;32m    871\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady) \u001b[38;5;129;01mor\u001b[39;00m fn(item):\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ray\\util\\iter.py:1108\u001b[0m, in \u001b[0;36mLocalIterator.union.<locals>.build_union\u001b[1;34m(timeout)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1107\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_pull):\n\u001b[1;32m-> 1108\u001b[0m         item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[0;32m   1110\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ray\\util\\iter.py:779\u001b[0m, in \u001b[0;36mLocalIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_once()\n\u001b[1;32m--> 779\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilt_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ray\\util\\iter.py:807\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_foreach\u001b[39m(it):\n\u001b[1;32m--> 807\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    808\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[0;32m    809\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ray\\util\\iter.py:807\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_foreach\u001b[39m(it):\n\u001b[1;32m--> 807\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    808\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[0;32m    809\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ray\\util\\iter.py:815\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics_context():\n\u001b[1;32m--> 815\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    816\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m result\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, _NextValueNotReady):\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ray\\rllib\\execution\\train_ops.py:343\u001b[0m, in \u001b[0;36mMultiGPUTrainOneStep.__call__\u001b[1;34m(self, samples)\u001b[0m\n\u001b[0;32m    338\u001b[0m         permutation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(num_batches)\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m batch_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[0;32m    340\u001b[0m             \u001b[38;5;66;03m# Learn on the pre-loaded data in the buffer.\u001b[39;00m\n\u001b[0;32m    341\u001b[0m             \u001b[38;5;66;03m# Note: For minibatch SGD, the data is an offset into\u001b[39;00m\n\u001b[0;32m    342\u001b[0m             \u001b[38;5;66;03m# the pre-loaded entire train batch.\u001b[39;00m\n\u001b[1;32m--> 343\u001b[0m             results \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_on_loaded_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpermutation\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mper_device_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbuffer_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m             learner_info_builder\u001b[38;5;241m.\u001b[39madd_learn_on_batch_results(\n\u001b[0;32m    349\u001b[0m                 results, policy_id\n\u001b[0;32m    350\u001b[0m             )\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# Tower reduce and finalize results.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ray\\rllib\\policy\\dynamic_tf_policy.py:609\u001b[0m, in \u001b[0;36mDynamicTFPolicy.learn_on_loaded_batch\u001b[1;34m(self, offset, buffer_index)\u001b[0m\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    606\u001b[0m         sliced_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loaded_single_cpu_batch\u001b[38;5;241m.\u001b[39mslice(\n\u001b[0;32m    607\u001b[0m             start\u001b[38;5;241m=\u001b[39moffset, end\u001b[38;5;241m=\u001b[39moffset \u001b[38;5;241m+\u001b[39m batch_size\n\u001b[0;32m    608\u001b[0m         )\n\u001b[1;32m--> 609\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43msliced_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_gpu_tower_stacks[buffer_index]\u001b[38;5;241m.\u001b[39moptimize(\n\u001b[0;32m    612\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_session(), offset\n\u001b[0;32m    613\u001b[0m )\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ray\\rllib\\policy\\tf_policy.py:448\u001b[0m, in \u001b[0;36mTFPolicy.learn_on_batch\u001b[1;34m(self, postprocessed_batch)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mon_learn_on_batch(\n\u001b[0;32m    444\u001b[0m     policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, train_batch\u001b[38;5;241m=\u001b[39mpostprocessed_batch, result\u001b[38;5;241m=\u001b[39mlearn_stats\n\u001b[0;32m    445\u001b[0m )\n\u001b[0;32m    447\u001b[0m fetches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_learn_on_batch(builder, postprocessed_batch)\n\u001b[1;32m--> 448\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    449\u001b[0m stats\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    450\u001b[0m     {\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_metrics\u001b[39m\u001b[38;5;124m\"\u001b[39m: learn_stats,\n\u001b[0;32m    452\u001b[0m         NUM_AGENT_STEPS_TRAINED: postprocessed_batch\u001b[38;5;241m.\u001b[39mcount,\n\u001b[0;32m    453\u001b[0m     }\n\u001b[0;32m    454\u001b[0m )\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stats\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ray\\rllib\\utils\\tf_run_builder.py:42\u001b[0m, in \u001b[0;36mTFRunBuilder.get\u001b[1;34m(self, to_fetch)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executed \u001b[38;5;241m=\u001b[39m \u001b[43mrun_timeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdebug_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m            \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTF_TIMELINE_DIR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     50\u001b[0m         logger\u001b[38;5;241m.\u001b[39mexception(\n\u001b[0;32m     51\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError fetching: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, feed_dict=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     52\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetches, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_dict\n\u001b[0;32m     53\u001b[0m             )\n\u001b[0;32m     54\u001b[0m         )\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ray\\rllib\\utils\\tf_run_builder.py:102\u001b[0m, in \u001b[0;36mrun_timeline\u001b[1;34m(sess, ops, debug_name, feed_dict, timeline_dir)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m log_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_timeline\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m     99\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting TF run without tracing. To dump TF timeline traces \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto disk, set the TF_TIMELINE_DIR environment variable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m         )\n\u001b[1;32m--> 102\u001b[0m     fetches \u001b[38;5;241m=\u001b[39m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeed_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fetches\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\client\\session.py:957\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 957\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    959\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m    960\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1180\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[1;32m-> 1180\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1183\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1358\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1355\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[0;32m   1357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1358\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1359\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1361\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1365\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m   1364\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1366\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1367\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1349\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[0;32m   1347\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[0;32m   1348\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[1;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1350\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1441\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1440\u001b[0m                         run_metadata):\n\u001b[1;32m-> 1441\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_ITER, file_name, episodes = 1000, \"\", 0\n",
    "best_mean = -100000\n",
    "for n in range(N_ITER):\n",
    "    res = agent.train()    \n",
    "    \n",
    "    mean = res['episode_reward_mean']\n",
    "    if mean > best_mean:\n",
    "        file_name = agent.save(CHECKPOINT_ROOT)\n",
    "        best_mean = mean\n",
    "\n",
    "    episodes += res['episodes_this_iter']\n",
    "    print(f\"{n+1:3d} episodes:{episodes:5d}  mean:{mean:6.2f}  ({res['episode_reward_min']:6.2f}, {res['episode_reward_max']:6.2f})\", file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b227dfa9",
   "metadata": {},
   "source": [
    "## Model used\n",
    "### For `config[\"framework\"]  = 'tf'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19f96dfa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoopModel_as_DDPGTFModel' object has no attribute 'base_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m policy \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_policy()\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241m.\u001b[39msummary())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoopModel_as_DDPGTFModel' object has no attribute 'base_model'"
     ]
    }
   ],
   "source": [
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "print(model.base_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecd0d24",
   "metadata": {},
   "source": [
    "### For `config[\"framework\"]  = 'torch' `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "363932ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullyConnectedNetwork(\n",
      "  (_logits): SlimFC(\n",
      "    (_model): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (_hidden_layers): Sequential(\n",
      "    (0): SlimFC(\n",
      "      (_model): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (1): SlimFC(\n",
      "      (_model): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (_value_branch): SlimFC(\n",
      "    (_model): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65070b7",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b98643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym \n",
    "env = gym.make(ENV)                              # создать среду\n",
    " \n",
    "def run(episodes = 1000, ticks = 10000):\n",
    "    rews = []                                    # доходы за каждую попытку\n",
    "    for _ in range(episodes):                    # делаем trials попыток (игр)\n",
    "        tot = 0                                  # cуммарное вознаграждение\n",
    "        obs = env.reset()                        # начальное состояние\n",
    "        for _ in range(ticks):                   # ticks временных шагов\n",
    "            action = agent.compute_single_action(obs)                 # выбрать действие\n",
    " \n",
    "            obs, rew, done, _ = env.step(action) # получить информацию\n",
    "            tot += rew                           # cуммарное вознаграждение            \n",
    "            env.render()\n",
    "            if done:                             \n",
    "                break\n",
    "\n",
    "        rews.append(tot)                         # накопить вознаграждение\n",
    "\n",
    "    print(f\"Reward: {np.mean(rews):.2f} ± {np.std(rews)/len(rews)**0.5:.2f}, std: {np.std(rews):.2f}, min: {np.min(rews)}, max: {np.max(rews)}\")    \n",
    "    \n",
    "run(episodes = 10)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd5cb96",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27945e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = dqn.DQNTrainer(config, env=ENV)\n",
    "#agent = ppo.PPOTrainer(config, env=ENV)\n",
    "agent.restore(\"tmp/MountainCar-v0\\checkpoint_000120\\checkpoint-120\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
