{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95d9c134",
   "metadata": {},
   "source": [
    "# Ray library for RL\n",
    "\n",
    "`!pip install box2d-py`<br>\n",
    "`!pip install ray[rllib]`<br>\n",
    "On 2022-jun there was a problem with the latest version of the OpenAI Gym library. \n",
    "\n",
    "- [Algorithms](https://docs.ray.io/en/latest/rllib/rllib-algorithms.html)\n",
    "- [Helpful video series](https://www.youtube.com/watch?v=krz8SCds7yA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de380fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ray\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11915373",
   "metadata": {},
   "source": [
    "## Choosing an environment and Set up file locations for checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "#ENV_NAME = \"CartPole-v1\"\n",
    "#ENV_NAME = \"MountainCar-v0\"\n",
    "#ENV_NAME = \"LunarLander-v2\"\n",
    "ENV_NAME = \"BipedalWalker-v3\"\n",
    "\n",
    "CHECKPOINT_ROOT = \"tmp/\"+ENV_NAME\n",
    "shutil.rmtree(CHECKPOINT_ROOT, ignore_errors=True, onerror=None)\n",
    "\n",
    "RAY_RESULTS = \"tmp/ray_results/\"\n",
    "shutil.rmtree(RAY_RESULTS, ignore_errors=True, onerror=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa9697",
   "metadata": {},
   "source": [
    "## Choose a method\n",
    "\n",
    "- https://docs.ray.io/en/latest/rllib/rllib-algorithms.html - List of methods\n",
    "- https://docs.ray.io/en/latest/rllib/rllib-training.html  - Common parameters\n",
    "\n",
    "### 1. PPO: Proximal Policy Optimization   \n",
    "<b>Actions</b> discrete:`Yes`, continuous:`Yes`<br>\n",
    "Doesn't work for MountainCar-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d28673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.agents.ppo as trainer\n",
    "\n",
    "config = trainer.DEFAULT_CONFIG.copy()\n",
    "\n",
    "config['num_workers'] = 1           # 8 parallel workers\n",
    "config['num_sgd_iter'] = 50 \n",
    "config['sgd_minibatch_size'] = 250\n",
    "config['model']['fcnet_hiddens'] = [512, 512]\n",
    "\n",
    "agent = trainer.PPOTrainer(config, env=ENV_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8051077",
   "metadata": {},
   "source": [
    "### 2. DQN: Deep Q-Network\n",
    "<b>Actions</b> discrete:`Yes`, continuous:`No`<br>\n",
    "Possible for MountainCar-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627f2858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.agents.dqn as trainer\n",
    "\n",
    "config = trainer.DEFAULT_CONFIG.copy()\n",
    "config['metrics_num_episodes_for_smoothing'] = 100    # def: 100 (для вычисления mean, min, max)\n",
    "config[\"framework\"]                          = 'tf'   # def: 'tf',  можно  'torch' \n",
    "print(config['model'])\n",
    "\n",
    "agent = trainer.DQNTrainer(config, env=ENV_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016c5753",
   "metadata": {},
   "source": [
    "### 3. DDPG  (TD3):  Deep Deterministic Policy Gradients\n",
    "<b>Actions</b> discrete:`No`, continuous:`Yes`<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0504718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.agents.ddpg as trainer\n",
    "\n",
    "config = trainer.DEFAULT_CONFIG.copy()\n",
    "print(config['model'])\n",
    "\n",
    "agent = trainer.DDPGTrainer(config, env=ENV_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29525759",
   "metadata": {},
   "source": [
    "### 4. SAC\n",
    "<b>Actions</b> discrete:`Yes`, continuous:`Yes`<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992ced67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.agents.sac as trainer\n",
    "\n",
    "config = trainer.DEFAULT_CONFIG.copy()\n",
    "\n",
    "agent = trainer.SACTrainer(config, env=ENV_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b1dde9",
   "metadata": {},
   "source": [
    "### 5. A3C\n",
    "<b>Actions</b> discrete:`Yes`, continuous:`Yes`<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c201b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.agents.a3c as trainer\n",
    "\n",
    "config = trainer.DEFAULT_CONFIG.copy()\n",
    "config[\"framework\"] = 'torch'\n",
    "      \n",
    "for name in ['gamma', 'train_batch_size', 'batch_mode', 'lr']:\n",
    "    print(f\"{name:20s}: {config[name]}\")\n",
    "for name in ['fcnet_hiddens', 'fcnet_activation']:\n",
    "    print(f\"{name:20s}: {config['model'][name]}\")\n",
    "\n",
    "\n",
    "agent = trainer.A3CTrainer(config, env=ENV_NAME)\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592a3c0f",
   "metadata": {},
   "source": [
    "### 6. PG:  vanila  Policy Gradients\n",
    "<b>Actions</b> discrete:`Yes`, continuous:`Yes`<br>\n",
    "Ray: we include a vanilla policy gradients implementation as an example algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5001800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.agents.pg as trainer\n",
    "\n",
    "config = trainer.DEFAULT_CONFIG.copy()\n",
    "config[\"framework\"] = 'torch'\n",
    "      \n",
    "for name in ['gamma', 'train_batch_size', 'batch_mode', 'lr']:\n",
    "    print(f\"{name:20s}: {config[name]}\")\n",
    "for name in ['fcnet_hiddens', 'fcnet_activation']:\n",
    "    print(f\"{name:20s}: {config['model'][name]}\")\n",
    "\n",
    "\n",
    "agent = trainer.PGTrainer(config, env=ENV_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393ec984",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d75aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITER     = 100000                 # max iterations\n",
    "MAX_EPISODES = 1000                   # max episodes\n",
    "file_name, episode, history = \"\", 0, []\n",
    "#print(config)\n",
    "\n",
    "best_mean = -100000\n",
    "for it in range(1,MAX_ITER+1):\n",
    "    res = agent.train()    \n",
    "    \n",
    "    episode += res['episodes_this_iter']\n",
    "    \n",
    "    mean = res['episode_reward_mean']\n",
    "    history.append([episode, mean])\n",
    "    if mean > best_mean:\n",
    "        file_name = agent.save(CHECKPOINT_ROOT)\n",
    "        best_mean = mean\n",
    "    \n",
    "    if it % 10 == 0:\n",
    "        print(f\"\\r{it:3d} episode:{episode:5d}  reawrd: {mean:6.2f}  ({res['episode_reward_min']:6.2f}, {res['episode_reward_max']:6.2f}), best: {best_mean:.2f}  {file_name:30s}\", end=\"\")        \n",
    "        \n",
    "    if episode > MAX_EPISODES:\n",
    "        print(\"\\nfinish\")\n",
    "        break\n",
    "\n",
    "history = np.array(history)        \n",
    "plt.title(f\"{ENV_NAME} best: {mean:.2f}\", fontsize=18)\n",
    "plt.plot(history[:,0], history[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b227dfa9",
   "metadata": {},
   "source": [
    "## Model used\n",
    "### For `config[\"framework\"]  = 'tf'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f96dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "print(model.base_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecd0d24",
   "metadata": {},
   "source": [
    "### For `config[\"framework\"]  = 'torch' `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363932ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65070b7",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b98643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym \n",
    "env = gym.make(ENV)                              # создать среду\n",
    " \n",
    "def run(episodes = 1000, ticks = 10000):\n",
    "    rews = []                                    # доходы за каждую попытку\n",
    "    for _ in range(episodes):                    # делаем trials попыток (игр)\n",
    "        tot = 0                                  # cуммарное вознаграждение\n",
    "        obs = env.reset()                        # начальное состояние\n",
    "        for _ in range(ticks):                   # ticks временных шагов\n",
    "            action = agent.compute_single_action(obs)                 # выбрать действие\n",
    " \n",
    "            obs, rew, done, _ = env.step(action) # получить информацию\n",
    "            tot += rew                           # cуммарное вознаграждение            \n",
    "            env.render()\n",
    "            if done:                             \n",
    "                break\n",
    "\n",
    "        rews.append(tot)                         # накопить вознаграждение\n",
    "\n",
    "    print(f\"Reward: {np.mean(rews):.2f} ± {np.std(rews)/len(rews)**0.5:.2f}, std: {np.std(rews):.2f}, min: {np.min(rews)}, max: {np.max(rews)}\")    \n",
    "    \n",
    "run(episodes = 10)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd5cb96",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27945e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = dqn.DQNTrainer(config, env=ENV)\n",
    "#agent = ppo.PPOTrainer(config, env=ENV)\n",
    "agent.restore(\"tmp/MountainCar-v0\\checkpoint_000120\\checkpoint-120\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644453d2",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e349fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in config.items():\n",
    "    if type(v) == dict:\n",
    "        print(f\"{k}: \")\n",
    "        for kk, vv in v.items():\n",
    "            print(f\"      {kk:50s}:\", vv)        \n",
    "    else:\n",
    "        print(f\"{k:30s}:\", v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
