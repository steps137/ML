{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "907b4db0",
   "metadata": {},
   "source": [
    "# Markov Decision Process\n",
    "\n",
    "## Bellman equation\n",
    "\n",
    "\n",
    "<img style=\"float: right; width: 200px;\" src=\"im/RL_Markov_Bellman.png\">\n",
    "\n",
    "Consider first a model in which there are no actions.\n",
    "At discrete moments of time, the environment randomly passes from state $s$ to the next state $s'$ with probabilities $P_{ss'}=P(s'|s)$. Upon entering the state with the number $s'$, a fixed reward $r_{s'}$ is charged.\n",
    "\n",
    "The value $V(s)$ of the state $s$  is the average total discounted reward\n",
    "\n",
    "$$\n",
    "R_t = r_t + \\gamma\\,r_{t+1} + \\gamma^2\\,r_{t+2}  + ... ~=~ r_t + \\gamma\\, R_{t+1},\n",
    "$$\n",
    "\n",
    "\n",
    "when starting from state $s$ in all of the following states:\n",
    "\n",
    "$$\n",
    "V(s) ~=~ \\langle R_{t+1}\\,|\\, s_t = s  \\rangle ~=~ \\langle r_{t+1}+\\gamma\\,R_{t+2}\\,|\\, s_t = s  \\rangle.\n",
    "$$\n",
    "\n",
    "When moving to the $j$-th state, the agent will receive a reward $r_j$ (for getting into it) plus a discounted future reward equal to $V_j$ for the entire subsequent history (the value of the $j$-th state).\n",
    "\n",
    "$$\n",
    "V_i = \\sum_j P_{ij}\\,\\cdot\\, \\bigr(r_{j} + \\gamma\\,V_j\\bigr)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e68bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from   numpy.linalg import inv     # inverse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b820c4",
   "metadata": {},
   "source": [
    "## Fake terminal state\n",
    "\n",
    "\n",
    "<img style=\"float: right; width: 300px;\" src=\"im/RL_Markov.png\">\n",
    "\n",
    "Let the environment be in five states, two of which (3 and 4) are terminal (after them, the \"random walk\" ends). It is convenient to introduce one more state (a square in the figure) upon entering which there is no reward and where the environment remains \"forever\" and the rewards cease to be accrued (end of walk). The transition probabilities are indicated next to the arrows, and the rewards are next to the positions.\n",
    "\n",
    "In matrix notation, the Bellman equation has a simple solution:\n",
    "\n",
    "$$\n",
    "\\mathbf{V} = (\\mathbf{1} -\\gamma\\,\\mathbf{P})^{-1}\\cdot \\mathbf{P}\\cdot \\mathbf{r},\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4dbc486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.array([ [0.,   0.6,  0.,   0.4,  0.,   0.,   ],\n",
    "               [0.,   0.4,  0.5,  0.,   0.1,  0.,   ],\n",
    "               [0.7,  0.,   0.,   0.2,  0.1,  0.,   ],\n",
    "               [0.,   0.,   0.,   0.,   0.,   1.0,  ],\n",
    "               [0.,   0.,   0.,   0.,   0.,   1.0,  ],\n",
    "               [0.,   0.,   0.,   0.,   0.,   1.0,  ]  ])\n",
    "r = np.array(  [2.,  -1.,   1.,  -4.,   8.,   0. ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60604514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.19488201,  1.86132961,  0.64722433,  0.        ,  0.        ,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "np.dot( inv(  np.eye(len(r)) - gamma*P  ), np.dot(P,r) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e12d0ea",
   "metadata": {},
   "source": [
    "## No fake terminal state\n",
    "\n",
    "\n",
    "<img src = \"im/RL_Markov2.png\"  style=\"float: right; width: 250px; margin-left:10px;\">\n",
    "\n",
    "It is not necessary to introduce a fake terminal state. However, then the rewards will be the matrix $r_{ij}$.\n",
    "For example, upon transition from a terminal state to it, the reward will be zero $r_{4,4}=0$.\n",
    "At the same time $r_{1,4}=8$. The Bellman equation in this case takes the following form:\n",
    "\n",
    "$$\n",
    "V_i = \\sum_j P_{ij}\\,\\cdot\\, \\bigr(r_{ij} + \\gamma\\,V_j\\bigr).\n",
    "$$\n",
    "\n",
    "More generally, rewards can be probabilistic $P(j,r|i)$ and then:\n",
    "\n",
    "$$\n",
    "V_i = \\sum_{j,\\,r} P_{ijr}\\,\\cdot\\, \\bigr(r + \\gamma\\,V_j\\bigr).\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19031607",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.array([ [0.,   0.6,  0.,   0.4,  0.  ],\n",
    "               [0.,   0.4,  0.5,  0.,   0.1 ],\n",
    "               [0.7,  0.,   0.,   0.2,  0.1 ],\n",
    "               [0.,   0.,   0.,   1.0,  0.  ],\n",
    "               [0.,   0.,   0.,   0.,   1.0 ] ])\n",
    "\n",
    "r = np.array([ [0.,  -1.,   0.,  -4.,   0.  ],\n",
    "               [0.,  -1.,   1.,   0.,   8.  ],\n",
    "               [2.,   0.,   0.,  -4.,   8.  ],\n",
    "               [0.,   0.,   0.,   0.,   0.  ],\n",
    "               [0.,   0.,   0.,   0.,   0.  ] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2591005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.19488201,  1.86132961,  0.64722433,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "np.dot( inv(  np.eye(len(r)) - gamma*P  ), np.sum(P*r, axis=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc60daa8",
   "metadata": {},
   "source": [
    "## Monte Carlo simulation\n",
    "\n",
    "Let's reproduce the same result using Monte Carlo simulation. To do this, we will start walking the environment many times starting from the same state <b class=\"norm\">s0</b>, calculating the reward received for each episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ccd256e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(0) =    -1.19 ± 0.0365  T(0):    3.85 ± 0.0366\n"
     ]
    }
   ],
   "source": [
    "states      = np.arange(len(r))             # [0,1,2,3,4,5]\n",
    "terminals   = [3,4]                         # terminal states\n",
    "gamma       = 0.9                           # discount factor\n",
    "rews, steps = [], []                        # rewards and steps\n",
    "s0          = 0                             # start state\n",
    "\n",
    "for _ in range(10000):                      # run 10,000 experiments\n",
    "    rew, s, discont = 0, s0, 1\n",
    "    for i in range(100):                    # just in case, limit\n",
    "        s = np.random.choice(states,p=P[s]) # state by probability distribution P[s]\n",
    "\n",
    "        rew += r[s] * discont               # accumulating reward\n",
    "        discont *= gamma                    # we discount more\n",
    "\n",
    "        if s in terminals:                  # entered the terminal state\n",
    "            break                           # further reward does not change\n",
    "\n",
    "    rews.append(rew)                        # keep the reward\n",
    "    steps.append(i+1)                       # save the number of steps taken\n",
    "\n",
    "print(\"V(%d) =  %7.2f ± %5.4f  T(%d): %7.2f ± %5.4f\"\n",
    "      % (s0, np.mean(rews),  np.std(rews) /len(rews)**0.5,\n",
    "         s0, np.mean(steps), np.std(steps)/len(steps)**0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea4de39",
   "metadata": {},
   "source": [
    "## Policy function\n",
    "\n",
    "Let an agent with probability $\\pi_{i\\alpha}=\\pi(\\alpha\\,|\\,i)$ in a state with number $i$ perform an action with number $\\alpha$ (below, Latin indices are used for state numbers, and Greek for action).\n",
    "As a result of the agent's action, the environment passes into the state numbered $j$ with the probability $P_{i\\alpha j}=P(j\\,|\\,i,\\alpha)$.\n",
    "\n",
    "The state values still satisfy the Bellman equation:\n",
    "\n",
    "$$\n",
    "V_i = \\sum_j \\tilde{P}_{ij}\\, \\bigr(r_j + \\gamma\\,V_j\\bigr),~~~~~~~~~~~~\\tilde{P}_{ij} = \\sum_\\alpha \\pi_{i\\alpha}\\,P_{i\\alpha j},\n",
    "$$\n",
    "\n",
    "where $\\tilde{P}_{ij}$ the probabilities of going from $i$ to $j$ are equal to the product of the probability of taking the action $\\alpha$ in state $i$ by the probability of going after that to state $j$ with the sum over all possible actions.\n",
    "\n",
    "Let us write down the equations for the Q-utility function of the action $a$ performed in the state $s$:\n",
    "\n",
    "$$\n",
    "Q(s,a) ~=~ \\langle R_{t+1}\\,|\\,s_t=a,\\,a_t=a\\rangle.\n",
    "$$\n",
    "\n",
    "For the discrete case, this will be the $Q_{i\\alpha}$ matrix.\n",
    "Knowing it, we can calculate the utility of the state (when performing any action with probabilities $\\pi_{i\\alpha}$):\n",
    "\n",
    "$$\n",
    "V_i = \\sum_\\alpha \\pi_{i\\alpha} \\, Q_{i\\alpha}.\n",
    "$$\n",
    "\n",
    "\n",
    "You can also record feedback. The value of actions $Q_{i\\alpha}$ in state $i$ is equal to the average reward for the immediate transition, plus the discounted future rewards of the states that the environment enters during such a transition:\n",
    "\n",
    "$$\n",
    "Q_{i\\alpha} = \\sum_{j}P_{i\\alpha j}\\,(r_j + \\gamma\\,V_j\\bigr).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15850d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states, num_actions = 10, 3                            # number of states and actions\n",
    "\n",
    "pi = np.random.random((num_states, num_actions))           # probabilities of action\n",
    "pi = pi/np.sum(pi, axis=1).reshape(-1,1)                   # normalize to unit\n",
    "\n",
    "P = np.random.random((num_states,num_actions,num_states))  # environment model\n",
    "P = P/np.sum(P, axis=2).reshape(num_states,num_actions,1)  # normalize\n",
    "\n",
    "r  = np.linspace(-10, 10, num_states)                      # rewards [-10...10]\n",
    "\n",
    "piP = np.sum( np.expand_dims(pi, axis=2) * P, axis=1)\n",
    "\n",
    "V = np.dot( inv(  np.eye(len(r)) - gamma*piP  ), np.dot( piP, r) )\n",
    "Q = np.dot(P, r + gamma*V)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
