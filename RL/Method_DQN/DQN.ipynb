{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc31610",
   "metadata": {},
   "source": [
    "# DQN and DDQN methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d887a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab:\n",
    "!pip3 install box2d-py\n",
    "!pip3 install gym[Box_2D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0c422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MemoryBuffer:\n",
    "    \"\"\" \n",
    "    Overwrite memory for storing the environment model.\n",
    "    This implementation is not the most efficient, but it is needed for experimenting with sorting.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity, n_states, n_actions):\n",
    "        \"\"\"\n",
    "        capacity  - memory capacity (maximum number of stored items)\n",
    "        n_states  - number of state variables       \n",
    "        n_actions - number of actions\n",
    "        \"\"\"\n",
    "        self.capacity = capacity    # maximum number of stored elements\n",
    "        self.count = 0              # how many items have already been saved\n",
    "        self.index = 0              # index of element to be inserted\n",
    "        self.nS = n_states        \n",
    "        self.nA = n_actions\n",
    "\n",
    "        self.memo = np.zeros( (capacity, self.nS*2 + 4), dtype = np.float32)\n",
    "\n",
    "    #------------------------------------------------------------------------------------        \n",
    "    \n",
    "    def add(self, s0, a0, s1, r1, done, rewrite = 0):\n",
    "        \"\"\" Add example to memory \"\"\"  \n",
    "        self.index = self.index % self.capacity\n",
    "        \n",
    "        norm = np.dot(s1,s1)**0.5  \n",
    "        self.memo[self.index] = np.concatenate(( s0, s1, [a0], [r1], [done], [norm]) )\n",
    "\n",
    "        self.index += 1\n",
    "        self.count += 1\n",
    "        \n",
    "        if  abs(rewrite) < 1.0 and (self.count == self.capacity                 \n",
    "            or (self.count > self.capacity and self.index >= int(abs(rewrite)*self.capacity)) ):\n",
    "            self.memo = self.memo[ self.memo[:, -1].argsort() ]  \n",
    "            if rewrite < 0:                      \n",
    "                self.memo = self.memo[::-1]   # large norm at the beginig\n",
    "            self.index = 0        \n",
    "        \n",
    "    #------------------------------------------------------------------------------------\n",
    "        \n",
    "    def samples(self, count):\n",
    "        \"\"\" Return count of random examples from memory \"\"\"\n",
    "        mem_max = min(self.count, self.capacity)\n",
    "        indxs = np.random.choice(mem_max, count, replace=False)\n",
    "        sample = self.memo[indxs]\n",
    "        s0 = sample[:, 0:           self.nS]\n",
    "        s1 = sample[:, self.nS:     2*self.nS]\n",
    "        a0 = sample[:, 2*self.nS:   2*self.nS+1]\n",
    "        r1 = sample[:, 2*self.nS+1: 2*self.nS+2]\n",
    "        en = sample[:, 2*self.nS+2: 2*self.nS+3]\n",
    "        return torch.tensor(s0), torch.tensor(a0, dtype = torch.int64), torch.tensor(s1), torch.tensor(r1), torch.tensor(en)\n",
    "\n",
    "    #------------------------------------------------------------------------------------\n",
    "        \n",
    "    def stat(self):\n",
    "        \"\"\" Statistic of s1 length and actions \"\"\"\n",
    "        num = min(self.count, self.capacity)\n",
    "        if num == 0:\n",
    "            return [],[],[],[]\n",
    "        \n",
    "        s1 = self.memo[:num, self.nS: 2*self.nS]\n",
    "        hist_S, bins_S = np.histogram(s1, bins=np.linspace(0, math.sqrt(self.nS), 101), density=True)\n",
    "\n",
    "        a = self.memo[:num, 2*self.nS: 2*self.nS+1],\n",
    "        hist_A, bins_A = np.histogram(a, bins=np.linspace(-0.5, self.nA-0.5, self.nA+1), density=True)\n",
    "    \n",
    "        return hist_S, bins_S, hist_A, bins_A\n",
    "\n",
    "#========================================================================================\n",
    "\n",
    "class AgentModel(nn.Module):\n",
    "    \"\"\" Neural network for Q(s,a) \"\"\"\n",
    "    def __init__(self, nS, nA, hiddens):\n",
    "        super(AgentModel, self).__init__()\n",
    "        \n",
    "        neurons, layers = [nS] + hiddens + [nA], []        \n",
    "        for i in range(len(neurons)-1):\n",
    "            layers.append(nn.Linear(neurons[i], neurons[i+1]) )\n",
    "            if i < len(neurons)-2:\n",
    "                layers.append( nn.ReLU() )\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.model(x)        \n",
    "\n",
    "#========================================================================================    \n",
    "    \n",
    "class DQN:\n",
    "    \"\"\" DQN метод для дискретных действий \"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.env  = env                         # environment we work with\n",
    "        self.low  = env.observation_space.low   # minimum observation values\n",
    "        self.high = env.observation_space.high  # maximum observation values\n",
    "        self.nA   =  self.env.action_space.n    # number of discrete actions\n",
    "        self.nS   =  self.env.observation_space.shape[0] # number of state components\n",
    "\n",
    "        self.params = {                   # default parameters\n",
    "            'env'      : \"Environment\",\n",
    "            'ticks'    : 200,                  \n",
    "            'timeout'  : True,            # whether to consider reaching ticks as a terminal state\n",
    "            'method'   : \"DQN\",           # kind of the method (DQN, DDQN)     \n",
    "            'gamma'    : 0.99,            # discount factor\n",
    "            'eps1'     : 1.0,             # initial value epsilon\n",
    "            'eps2'     : 0.001,           # final value   epsilon\n",
    "            'decays'   : 1000,            # number of episodes to decay eps1 - > eps2\n",
    "            'update'   : 10,              # target model update rate (in frames = time steps)         \n",
    "            'batch'    : 100,             # batch size for training\n",
    "            'capacity' : 100000,          # memory size\n",
    "            'rewrite'  : 1.0,             # rewrite memory (if < 1 - sorted)\n",
    "            'hiddens'  : [256,128],       # hidden layers\n",
    "            'scale'    : True,            # scale or not observe to [-1...1]\n",
    "            'loss'     : 'huber',         # loss function (mse, huber)\n",
    "            'optimizer': 'sgd',           # optimizer (sgd, adam)\n",
    "            'lm'       : 0.001,           # learning rate           \n",
    "        }\n",
    "        self.last_loss = 0.               # last loss\n",
    "        self.history   = []\n",
    "\n",
    "        print(\"low :   \", self.low)\n",
    "        print(\"high:   \", self.high)        \n",
    "        \n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def init(self):\n",
    "        \"\"\" Create a neural network and optimizer \"\"\"\n",
    "\n",
    "        self.gpu =torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"device:\", self.gpu)\n",
    "\n",
    "        self.model  = AgentModel(self.nS, self.nA, self.params['hiddens']).to(self.gpu)      # current Q\n",
    "        self.target = AgentModel(self.nS, self.nA, self.params['hiddens']).to(self.gpu)      # target  Q\n",
    "\n",
    "        self.best_model  = AgentModel(self.nS, self.nA, self.params['hiddens']).to(self.gpu) # best net\n",
    "        self.best_reward = -100000                                                           # best reward\n",
    "\n",
    "        if   self.params['loss'] == 'mse':\n",
    "             self.loss  = nn.MSELoss()\n",
    "        elif self.params['loss'] == 'huber':\n",
    "             self.loss = nn.HuberLoss()\n",
    "        else:\n",
    "            print(\"ERROR: Unknown loss function!!!\")\n",
    "        \n",
    "        if   self.params['optimizer'] == 'sgd':\n",
    "             self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.params['lm'], momentum=0.8)\n",
    "        elif self.params['optimizer'] == 'adam':\n",
    "             self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.params['lm'])\n",
    "        else:\n",
    "            print(\"ERROR: Unknown optimizer!!!\")\n",
    "\n",
    "        self.memo = MemoryBuffer(self.params['capacity'], self.nS, self.nA)        \n",
    "        \n",
    "        self.epsilon     = self.params['eps1']        # start value in epsilon greedy strategy\n",
    "        self.decay_rate  = math.exp(math.log(self.params['eps2']/self.params['eps1'])/self.params['decays'])\n",
    "\n",
    "        print(f\"decay_rate: {self.decay_rate:.4f}\")\n",
    "        print(self.model)        \n",
    "       \n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def scale(self, obs):\n",
    "        \"\"\" to [-1...1] \"\"\"\n",
    "        if self.params['scale']:\n",
    "            return -1. + 2.*(obs - self.low)/(self.high-self.low)\n",
    "        else:\n",
    "            return obs\n",
    "        \n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def policy(self, state):\n",
    "        \"\"\" Return action according to epsilon greedy strategy \"\"\"\n",
    "        if np.random.random() < self.epsilon:            \n",
    "            return np.random.randint(self.nA)    # random action\n",
    "\n",
    "        x = torch.tensor(state, dtype=torch.float32).to(self.gpu)\n",
    "        with torch.no_grad():\n",
    "            y = self.model(x).detach().to('cpu').numpy() \n",
    "        return np.argmax(y)                      # best action\n",
    "\n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def run_episode(self, ticks = 200):\n",
    "        \"\"\" Run one episode, keeping the environment model in memory \"\"\"\n",
    "        rew = 0                                  # total reward\n",
    "        s0 = self.env.reset()                    # initial state\n",
    "        s0 = self.scale (s0)                     # scale it\n",
    "        a0 = self.policy(s0)                     # get action\n",
    "        for t in range(1, ticks+1):\n",
    "            s1, r1, done, _ = self.env.step(a0)\n",
    "            s1 = self.scale (s1)\n",
    "            a1 = self.policy(s1)\n",
    "\n",
    "            dn = done and (self.params['timeout'] or t < ticks)            \n",
    "            #self.memo.add(s0, a0, s1, r1, float(dn), self.params['rewrite'] )\n",
    "            self.memo.add(s0, a0, s1, r1, float(dn), 1. )\n",
    "\n",
    "            if self.frame % self.params['update'] == 0:  # copy model to target\n",
    "                self.target.load_state_dict( self.model.state_dict() ) \n",
    "\n",
    "            if self.memo.count >= self.params['batch']:    \n",
    "                self.learn_model()                         \n",
    "\n",
    "            rew += r1\n",
    "            self.frame += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            s0, a0 = s1, a1\n",
    "        return rew, t\n",
    "\n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def learn(self, episodes = 100000, stat1 = 10, stat2 = 100, plots = 1000, rews_range=[-200, -80]):\n",
    "        \"\"\" Repeat episodes episodes times \"\"\"\n",
    "        self.frame = 1        \n",
    "        rews, lens, mean, beg   = [], [], 0, time.process_time()\n",
    "        for episode in range(1, episodes+1):\n",
    "            rew, t = self.run_episode( self.params['ticks'] )\n",
    "            rews.append( rew )\n",
    "            lens.append(t)\n",
    "\n",
    "            self.epsilon *= self.decay_rate                # epsilon-decay\n",
    "            if self.epsilon < self.params['eps2']:\n",
    "                self.epsilon = 0.\n",
    "                \n",
    "            if episode % stat1 == 0:\n",
    "                self.history.append([episode, np.mean(rews[-stat1:]), np.mean(rews[-stat2:])])                                      \n",
    "                \n",
    "            if  episode % stat2 == 0:                               \n",
    "                mean, std    = np.mean(rews[-stat2:]), np.std(rews[-stat2:])    \n",
    "                lensM, lensS = np.mean(lens[-stat2:]), np.std(lens[-stat2:])                    \n",
    "                if mean > self.best_reward:\n",
    "                    self.best_reward = mean\n",
    "                    self.best_model.load_state_dict( self.model.state_dict() )                     \n",
    "                maxQ = self.maxQ.to('cpu')\n",
    "                print(f\"{episode:6d} rew:{mean:7.1f} ± {std/stat2**0.5:3.1f}, best:{self.best_reward:7.2f}, ticks:{lensM:3.0f}, eps:{self.epsilon:.3f}, Q:{maxQ.mean():8.2f} ±{maxQ.std():6.2f}, loss:{self.last_loss:7.3f}, {(time.process_time() - beg):3.0f}s\")\n",
    "                beg = time.process_time()\n",
    "                \n",
    "            if  episode % plots == 0:                   \n",
    "                self.plot(f\"{self.params['env']}  Episode: {episode}  best: {self.best_reward:7.1f}\", rews_range)\n",
    "                #self.test(episodes = 1, ticks = self.params['ticks'], render = True)\n",
    "                #env.close()\n",
    "            \n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def learn_model(self):\n",
    "        \"\"\" Model Training \"\"\"\n",
    "        batch = self.params['batch']\n",
    "        \n",
    "        S0, A0, S1, R1, Done = self.memo.samples(batch)\n",
    "        S0 = S0.to(self.gpu); A0 = A0.to(self.gpu)\n",
    "        S1 = S1.to(self.gpu); R1 = R1.to(self.gpu);  Done = Done.to(self.gpu)\n",
    "        \n",
    "        if self.params['method'] == 'DQN':\n",
    "            with torch.no_grad():\n",
    "                y = self.target(S1).detach()\n",
    "            self.maxQ, _ = torch.max(y, 1)      # maximum Q values for S1\n",
    "        elif self.params['method'] == 'DDQN':\n",
    "            y = self.model(S1)                 \n",
    "            a = torch.argmax(y,1).view(-1,1)   # a = arg max Q(s1,a; theta)                 \n",
    "            with torch.no_grad():\n",
    "                q = self.target(S1)                       \n",
    "            self.maxQ = q.gather(1, a)         # Q(s1, a; theta')   \n",
    "        else:            \n",
    "            print(\"Unknown method\")\n",
    "            \n",
    "        sum_loss = 0        \n",
    "        s0, a0   = S0, A0.view(-1,1)\n",
    "        r1, done = R1.view(-1,1), Done.view(-1,1)\n",
    "        q1       = self.maxQ.view(-1,1)\n",
    "\n",
    "        yb = r1 + self.params['gamma']*q1*(1.0 - done)\n",
    "\n",
    "        y = self.model(s0)             # forward\n",
    "        y = y.gather(1, a0)\n",
    "        L = self.loss(y, yb)\n",
    "\n",
    "        self.optimizer.zero_grad()     # reset the gradients\n",
    "        L.backward()                   # calculate gradients\n",
    "        self.optimizer.step()          # adjusting parameters\n",
    "\n",
    "        sum_loss += L.detach().item()\n",
    "\n",
    "        self.last_loss = sum_loss\n",
    "        \n",
    "    #------------------------------------------------------------------------------------\n",
    "        \n",
    "    def plot(self, text, rews_range):\n",
    "        \"\"\" Plot histogram for states and actions \"\"\"        \n",
    "        hist_S, bins_S, hist_A, bins_A = self.memo.stat()\n",
    "\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(16,6), gridspec_kw={'width_ratios': [2, 1, 5]})        \n",
    "        plt.suptitle(text, fontsize=18)\n",
    "                                \n",
    "        ax[0].set_xlim(min(bins_S), max(bins_S))    # histogram for S1\n",
    "        ax[0].grid(axis='x', alpha=0.75); ax[0].grid(axis='y', alpha=0.75)\n",
    "        ax[0].set_xlabel('|s1|', fontsize=16)\n",
    "        bins = [ (bins_S[i]+bins_S[i+1])/2 for i in range(len(bins_S)-1)]\n",
    "        ax[0].bar(bins, hist_S, width=0.5, color='blue')\n",
    "                        \n",
    "        ax[1].set_xlim(min(bins_A), max(bins_A))    # histogram for A\n",
    "        ax[1].grid(axis='x', alpha=0.75); ax[1].grid(axis='y', alpha=0.75)\n",
    "        ax[1].set_xlabel('actions', fontsize=16)\n",
    "        ax[1].set_xticks(np.arange(self.nA));\n",
    "        bins = [ (bins_A[i]+bins_A[i+1])/2 for i in range(len(bins_A)-1)]        \n",
    "        ax[1].bar(bins, hist_A, width=0.5, color='blue')\n",
    "\n",
    "        history = np.array(self.history)            # loss history\n",
    "        ax[2].plot(history[:,0], history[:,1], linewidth=1)\n",
    "        ax[2].plot(history[:,0], history[:,2], linewidth=2)\n",
    "        ax[2].set_ylim(rews_range[0], rews_range[1]);\n",
    "        ax[2].set_xlabel('episode', fontsize=16)        \n",
    "        ax[2].grid(axis='x', alpha=0.75); ax[2].grid(axis='y', alpha=0.75)\n",
    "        params = [ f\"{k:9s}: {v}\\n\" for k,v in self.params.items()]\n",
    "        ax[2].text(history[0,0], rews_range[0], \"\".join(params), {'fontsize':12, 'fontname':'monospace'})\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def test(self, episodes = 1000, ticks = 1000, render = False):\n",
    "        \"\"\" Q-Function Testing \"\"\"\n",
    "        rews = []\n",
    "        for episode in range(1, episodes+1):\n",
    "            tot = 0\n",
    "            obs =  self.env.reset()\n",
    "            for _ in range(ticks):\n",
    "                action = self.policy( self.scale(obs) )\n",
    "                obs, rew, done, _ = self.env.step(action)\n",
    "                tot += rew\n",
    "                if render:\n",
    "                    env.render()\n",
    "                if done:\n",
    "                    break\n",
    "            rews.append(tot)\n",
    "            if episode % 100:\n",
    "                print(f\"\\r {episode:4d}: Reward: {np.mean(rews):7.3f} ± {np.std(rews)/len(rews)**0.5:.3f}\", end=\"\")\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b35d73d",
   "metadata": {},
   "source": [
    "## MountainCar-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c3f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"MountainCar-v0\"  # (nS=2, nA=3)\n",
    "env = gym.make(env_name)\n",
    "\n",
    "dqn = DQN( env )\n",
    "\n",
    "dqn.params = {\n",
    "    'env'      : env_name,\n",
    "    'ticks'    : 200,        \n",
    "    'timeout'  : True,      # whether to consider reaching ticks as a terminal state\n",
    "    'method'   : \"DQN\",     # kind of the method (DQN, DDQN)     \n",
    "    'gamma'    : 0.99,      # discount factor\n",
    "    'eps1'     : 1.0,       # initial value epsilon\n",
    "    'eps2'     : 0.001,     # final value   epsilon\n",
    "    'decays'   : 500,       # number of episodes to decay eps1 - > eps2\n",
    "    'update'   : 100,       # target model update rate (in frames = time steps)             \n",
    "    'batch'    : 100,       # batch size for training\n",
    "    'capacity' : 100000,    # memory size    \n",
    "    'hiddens'  : [256,128], # hidden layers\n",
    "    'scale'    : True,      # scale or not observe to [-1...1]\n",
    "    'loss'     : 'mse',     # loss function (mse, huber)\n",
    "    'optimizer': 'adam',    # optimizer (sgd, adam)\n",
    "    'lm'       : 0.001,     # learning rate           \n",
    "}\n",
    "\n",
    "dqn.init()\n",
    "print(dqn.params)\n",
    "dqn.learn(episodes = 3000,  rews_range=[-200, -80])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df2fe2",
   "metadata": {},
   "source": [
    "## CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6ca64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v0\"    # (nS=4, nA=2)\n",
    "env = gym.make(env_name)\n",
    "\n",
    "dqn = DQN( env )\n",
    "\n",
    "dqn.params = {\n",
    "    'env'      : env_name,\n",
    "    'ticks'    : 200,    \n",
    "    'timeout'  : False,     # whether to consider reaching ticks as a terminal state\n",
    "    'method'   : \"DQN\",     # kind of the method (DQN, DDQN)     \n",
    "    'gamma'    : 0.99,      # discount factor\n",
    "    'eps1'     : 1.0,       # initial value epsilon\n",
    "    'eps2'     : 0.001,     # final value   epsilon\n",
    "    'decays'   : 500,       # number of episodes to decay eps1 - > eps2\n",
    "    'update'   : 100,       # target model update rate (in frames = time steps)             \n",
    "    'batch'    : 100,       # batch size for training\n",
    "    'capacity' : 1000,      # memory size    \n",
    "    'hiddens'  : [64,32],   # hidden layers\n",
    "    'scale'    : False,     # scale or not observe to [-1...1]\n",
    "    'loss'     : 'mse',     # loss function (mse, huber)\n",
    "    'optimizer': 'adam',    # optimizer (sgd, adam)\n",
    "    'lm'       : 0.0001,    # learning rate           \n",
    "}\n",
    "\n",
    "dqn.init()\n",
    "print(dqn.params)\n",
    "dqn.learn(episodes = 3000,  rews_range=[0, 210] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5961bc28",
   "metadata": {},
   "source": [
    "## LunarLander-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc176de",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"LunarLander-v2\" # (nS=8, nA=4)\n",
    "env = gym.make(env_name)\n",
    "\n",
    "dqn = DQN( env )\n",
    "\n",
    "dqn.params = {\n",
    "    'env'      : env_name,\n",
    "    'ticks'    : 500,\n",
    "    'timeout'  : True,      # whether to consider reaching ticks as a terminal state\n",
    "    'method'   : \"DQN\",     # kind of the method (DQN, DDQN)     \n",
    "    'gamma'    : 0.99,      # discount factor\n",
    "    'eps1'     : 1.0,       # initial value epsilon\n",
    "    'eps2'     : 0.001,     # final value   epsilon\n",
    "    'decays'   : 1000,      # number of episodes to decay eps1 - > eps2\n",
    "    'update'   : 1000,      # target model update rate (in frames = time steps)             \n",
    "    'batch'    : 100,       # batch size for training\n",
    "    'capacity' : 100000,    # memory size\n",
    "    'rewrite'  : 1,         # rewrite memory (if < 1 - sorted)\n",
    "    'hiddens'  : [256,64],  # hidden layers\n",
    "    'scale'    : False,     # scale or not observe to [-1...1]\n",
    "    'loss'     : 'huber',   # loss function (mse, huber)\n",
    "    'optimizer': 'adam',    # optimizer (sgd, adam)    \n",
    "    'lm'       : 0.0001,     # learning rate           \n",
    "}\n",
    "\n",
    "dqn.init()\n",
    "print(dqn.params)\n",
    "dqn.learn(episodes = 3000, rews_range=[-100, 300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a903cc",
   "metadata": {},
   "source": [
    "## Test best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29296c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.model.load_state_dict( dqn.best_model.state_dict() )\n",
    "dqn.test(episodes = 1, ticks=500, render=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dcaf96",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4966a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "  \n",
    "state = {'info':      f\"{env_name}: Q-function, Reward:  286\",     \n",
    "         'date':      datetime.datetime.now(),  \n",
    "         'model':     str(dqn.best_model),\n",
    "         'state' :    dqn.best_model.state_dict(),  \n",
    "        } \n",
    "print(dqn.params['hiddens'])\n",
    "torch.save(state, f\"{env_name}_{'_'.join([str(x) for x in dqn.params['hiddens']])}.286.pt\")\n",
    "print(state['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814690aa",
   "metadata": {},
   "source": [
    "## Plot policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90dbead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from   matplotlib import colors\n",
    "\n",
    "bins = (101, 101)   \n",
    "low  = dqn.low    # minimum observation values\n",
    "high = dqn.high   # maximum observation values\n",
    "step = (high-low)/bins\n",
    "\n",
    "def table(model, bins = (101, 101)):    \n",
    "    \"\"\" Get 2D table of policy function \"\"\"\n",
    "    res = np.empty(bins)\n",
    "    step = (high-low)/bins\n",
    "    indx = torch.cartesian_prod(torch.arange(0, bins[0]), torch.arange(0, bins[1]))\n",
    "    gpu  = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    X = torch.tensor(low, dtype = torch.float32) + indx*torch.tensor(step, dtype = torch.float32)        \n",
    "    X = -1. + 2.*(X - low)/(high-low)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        X = X.to(gpu)\n",
    "        Y = model.to(gpu)(X).cpu()     \n",
    "        print(Y.shape)\n",
    "        Y = torch.argmax(Y, 1).float()\n",
    "        print(Y.shape)\n",
    "    \n",
    "    return Y.view(bins[0], bins[1]).numpy()\n",
    "\n",
    "def plot(mat, title, d_ticks=1000, bins = (101,101)):\n",
    "    \"\"\" Plot table \"\"\"\n",
    "    def index(state):        \n",
    "        indx = ((state - low)/step).astype(int)\n",
    "        indx = np.clip(indx, (0,0), (bins[0]-1, bins[1]-1))\n",
    "        return tuple( indx )\n",
    "\n",
    "    g, m, x0   = index([0.5, 0.]), index([-math.pi/6, 0.]), np.array([ index([-0.6, 0.]), index([-0.4, 0.]) ])\n",
    "    cmap = colors.ListedColormap(['blue', 'white', 'red'])    \n",
    "    \n",
    "    plt.imshow(mat.T, interpolation='none', origin='lower', cmap= cmap, alpha=0.5)\n",
    "\n",
    "    plt.title (title, {'fontsize': 16})\n",
    "    plt.xlabel('x', {'fontsize': 16});         plt.ylabel('v', {'fontsize': 16}) \n",
    "    plt.axhline(g[1], c=\"black\", linewidth=1); plt.axvline(g[0], c=\"black\", linewidth=1)\n",
    "    plt.axvline(m[0], c=\"black\", linewidth=1)\n",
    "    plt.axvline(x0[0][0], c=\"black\", linewidth=2, ymin = 0.49, ymax = 0.51)\n",
    "    plt.axvline(x0[1][0], c=\"black\", linewidth=2, ymin = 0.49, ymax = 0.51)\n",
    "    ticks = range(0, bins[0], d_ticks)\n",
    "    plt.xticks( ticks, np.round(100*np.linspace(low[0], high[0], len(ticks)))/100 )\n",
    "    plt.yticks( ticks, np.round(100*np.linspace(low[1], high[1], len(ticks)))/100 )\n",
    "\n",
    "     \n",
    "plt.figure(figsize=(10,10))\n",
    "res = table(dqn.best_model, bins = bins)\n",
    "print(res.shape, res.min(), res.max(), res.mean())\n",
    "plot(res, 'Policy function, Reward: -98.1 ± 0.1 in [-107, -83]', d_ticks=100, bins = bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cbe150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ar = np.array([[1,1],[2,0], [3,2]])\n",
    "ar[ ar[:, -1].argsort() ] [::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc72ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = [ [ np.array([1,2]), np.array([3,4])], [ np.array([1,2]), np.array([3,4])] ]\n",
    "np.array(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd37992",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
