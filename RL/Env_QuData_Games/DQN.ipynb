{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc31610",
   "metadata": {},
   "source": [
    "# DQN and DDQN methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa0c422c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.8.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import copy\n",
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from modelsummary import summary\n",
    "\n",
    "\n",
    "from qugames import snake\n",
    "\n",
    "class MemoryBuffer:\n",
    "    def __init__(self, capacity, state_shape):\n",
    "        self.capacity = capacity  # memory capacity (number of examples)\n",
    "        self.count    = 0         # number of examples added\n",
    "        self.S0 = torch.empty( (capacity, ) + state_shape, dtype=torch.float32)\n",
    "        self.S1 = torch.empty( (capacity, ) + state_shape, dtype=torch.float32)\n",
    "        self.A0 = torch.empty( (capacity, 1),              dtype=torch.int64)        \n",
    "        self.R1 = torch.empty( (capacity, 1),              dtype=torch.float32)\n",
    "        self.Dn = torch.empty( (capacity, 1),              dtype=torch.float32)\n",
    "\n",
    "    def add(self, s0, a0, s1, r1, done):\n",
    "        \"\"\" Add to memory (s0,a0,s1,r1) \"\"\"\n",
    "        idx = self.count % self.capacity\n",
    "        self.S0[idx] = s0\n",
    "        self.S1[idx] = s1\n",
    "        self.A0[idx] = a0;  self.R1[idx] = r1; self.Dn[idx] = done\n",
    "        self.count += 1\n",
    "\n",
    "    def get(self, count):\n",
    "        \"\"\" Return count of examples for (s0,a0,s1,r1) \"\"\"        \n",
    "        high = min(self.count, self.capacity)\n",
    "        num  = min(count, high)\n",
    "        ids = torch.randint(high = high, size = (num,) )\n",
    "        return self.S0[ids], self.A0[ids], self.S1[ids], self.R1[ids], self.Dn[ids]\n",
    "\n",
    "#========================================================================================\n",
    "\n",
    "class AgentModel(nn.Module):\n",
    "    \"\"\" Neural network for Q(s,a) \"\"\"\n",
    "    def __init__(self, state_shape, nA = 5, \n",
    "                 channels=[12,24,64], kernels = [8,3,3], strides = [8,1,1], paddings = [0,1,1],\n",
    "                 pools=[2,2,2], dropout=0.2,  hidden = 128):\n",
    "        \"\"\"\n",
    "        state_shape = (3 * n_frames, image_width, image_height)\n",
    "        nA - number of state (Snake = 5)\n",
    "        \"\"\"\n",
    "        super(AgentModel, self).__init__()\n",
    "                \n",
    "        channels = [ state_shape[0] ] + channels;  conv_kernels = kernels      \n",
    "        w, h     =  state_shape[1], state_shape[2]\n",
    "        layers = []\n",
    "        for i in range(len(channels)-1):\n",
    "            layers +=  [ \n",
    "                nn.Conv2d(channels[i], channels[i+1], kernel_size=kernels[i], stride=strides[i], padding=paddings[i]),\n",
    "                nn.ReLU()]\n",
    "            if pools[i] > 1:\n",
    "                layers += [\n",
    "                    nn.MaxPool2d(kernel_size=pools[i], stride=pools[i]),\n",
    "                    nn.Dropout(p=dropout) ]                            \n",
    "            w = (((w + 2*paddings[i] - kernels[i]) // strides[i] + 1) - pools[i]) // pools[i] + 1\n",
    "            h = (((h + 2*paddings[i] - kernels[i]) // strides[i] + 1) - pools[i]) // pools[i] + 1        \n",
    "            \n",
    "        self.features = channels[-1] * w * h\n",
    "        layers += [ \n",
    "            nn.Flatten(1),\n",
    "            nn.Linear(self.features, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, nA) ]                \n",
    "                \n",
    "        self.model = nn.Sequential(*layers)       \n",
    " \n",
    "    def forward(self, x):        \n",
    "        return self.model(x)        \n",
    "\n",
    "#========================================================================================    \n",
    "    \n",
    "class DQN:\n",
    "    \"\"\" DQN метод для дискретных действий \"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.env  = env                         # environment we work with\n",
    "        self.nA   =  self.env.action_space.n    # number of discrete actions\n",
    "        shape     = env.observation_space.shape\n",
    "        self.state_shape = (2*shape[2], shape[0], shape[1]) # (2*channels, width, heights)\n",
    "        print(\"state_shape\", self.state_shape)\n",
    "\n",
    "        self.params = {                   # default parameters\n",
    "            'env'      : \"Environment\",\n",
    "            'ticks'    : 1000,                  \n",
    "            'timeout'  : True,            # whether to consider reaching ticks as a terminal state\n",
    "            'method'   : \"DQN\",           # kind of the method (DQN, DDQN)     \n",
    "            'gamma'    : 0.99,            # discount factor\n",
    "            'eps1'     : 1.0,             # initial value epsilon\n",
    "            'eps2'     : 0.001,           # final value   epsilon\n",
    "            'decays'   : 1000,            # number of episodes to decay eps1 - > eps2\n",
    "            'update'   : 10,              # target model update rate (in frames = time steps)         \n",
    "            'batch'    : 100,             # batch size for training\n",
    "            'capacity' : 100000,          # memory size\n",
    "            'channels' : [2,48,92],       # conv channles\n",
    "            'kernels'  : [8,3,3],         # conv kernels\n",
    "            'strides'  : [8,1,1],         # conv strides\n",
    "            'paddings' : [0,1,1],         # conv paddings\n",
    "            'pools'    : [2,2,2],         # = pool_strides\n",
    "            'dropout'  : 0.2,             # after cnn,reLU,pool \n",
    "            'hidden'  :  128,             # hidden layers            \n",
    "            'loss'     : 'mse',           # loss function (mse, huber)\n",
    "            'optimizer': 'adam',          # optimizer (sgd, adam)\n",
    "            'lr'       : 0.0001,          # learning rate           \n",
    "        }\n",
    "        self.last_loss = 0.               # last loss\n",
    "        self.history   = []\n",
    "        self.maxQ      = []\n",
    "   \n",
    "        \n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def init(self):\n",
    "        \"\"\" Create a neural network and optimizer \"\"\"\n",
    "\n",
    "        self.gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"device:\", self.gpu)\n",
    "\n",
    "        self.model     = AgentModel(self.state_shape, self.nA,                       # current Q\n",
    "                             channels=self.params['channels'], kernels = self.params['kernels'], paddings = self.params['paddings'],\n",
    "                             pools = self.params['pools'], dropout=self.params['dropout'], hidden  = self.params['hidden']).to(self.gpu)      \n",
    "        self.target    = AgentModel(self.state_shape, self.nA,                       # target  Q\n",
    "                             channels=self.params['channels'], kernels = self.params['kernels'], paddings = self.params['paddings'],\n",
    "                             pools = self.params['pools'], dropout=self.params['dropout'], hidden  = self.params['hidden']).to(self.gpu)      \n",
    "        self.best_model= AgentModel(self.state_shape, self.nA,                       # best net\n",
    "                             channels=self.params['channels'], kernels = self.params['kernels'], paddings = self.params['paddings'], \n",
    "                             pools = self.params['pools'], dropout=self.params['dropout'], hidden  = self.params['hidden']).to(self.gpu)\n",
    "        self.best_reward = -100000                                                   # best reward\n",
    "\n",
    "        if   self.params['loss'] == 'mse':\n",
    "             self.loss  = nn.MSELoss()\n",
    "        elif self.params['loss'] == 'huber':\n",
    "             self.loss = nn.HuberLoss()\n",
    "        else:\n",
    "            print(\"ERROR: Unknown loss function!!!\")\n",
    "        \n",
    "        if   self.params['optimizer'] == 'sgd':\n",
    "             self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.params['lr'], momentum=0.8)\n",
    "        elif self.params['optimizer'] == 'adam':\n",
    "             self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.params['lr'])\n",
    "        else:\n",
    "            print(\"ERROR: Unknown optimizer!!!\")\n",
    "\n",
    "        self.memo = MemoryBuffer(self.params['capacity'], self.state_shape)        \n",
    "        \n",
    "        self.epsilon     = self.params['eps1']        # start value in epsilon greedy strategy\n",
    "        self.decay_rate  = math.exp(math.log(self.params['eps2']/self.params['eps1'])/self.params['decays'])\n",
    "\n",
    "        print(f\"decay_rate: {self.decay_rate:.4f}\")\n",
    "        print(\"features:\", self.model.features)\n",
    "        print(self.model)                 \n",
    "        print(\"features:\", self.model.features)\n",
    "\n",
    "        #tot = 0\n",
    "        #for k, v in self.model.state_dict().items():\n",
    "        #    pars = np.prod(list(v.shape)); tot += pars\n",
    "        #    print(f'{k:20s} :{pars:7d}  =  {tuple(v.shape)} ')\n",
    "        #print(f\"{'parameters':20s} :{tot:7d}\")\n",
    "        \n",
    "        summary(self.model.to(\"cpu\"), torch.zeros((1, )+self.state_shape), show_input=False)      # \n",
    "        self.model.to(self.gpu)\n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def policy(self, state):\n",
    "        \"\"\" Return action according to epsilon greedy strategy \"\"\"\n",
    "        if np.random.random() < self.epsilon:            \n",
    "            return np.random.randint(self.nA)    # random action\n",
    "\n",
    "        x = state.unsqueeze(dim=0).to(self.gpu)\n",
    "        with torch.no_grad():\n",
    "            y = self.model(x).detach().to('cpu').numpy() \n",
    "        return np.argmax(y)                      # best action\n",
    "\n",
    "    \n",
    "    #------------------------------------------------------------------------------------\n",
    "    \n",
    "    def set_state(self, s1,s2):\n",
    "        s1 = torch.tensor(s1, dtype=torch.float32).permute(2, 0, 1)/255.\n",
    "        s2 = torch.tensor(s2, dtype=torch.float32).permute(2, 0, 1)/255.\n",
    "        return torch.cat([s1,s2],0)\n",
    "    \n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def run_episode(self, ticks = 200):\n",
    "        \"\"\" Run one episode, keeping the environment model in memory \"\"\"\n",
    "        rew = 0                                  # total reward\n",
    "        oi = self.env.reset()                    # initial state        \n",
    "        o0, _, _, _ = self.env.step(0)           # nothing do\n",
    "        s0 = self.set_state(oi,o0)\n",
    "        a0 = self.policy(s0)  # get action        \n",
    "        for t in range(1, ticks+1):\n",
    "            o1, r1, done, _ = self.env.step(a0)            \n",
    "            s1 = self.set_state(o0,o1)\n",
    "            a1 = self.policy(s1)\n",
    "\n",
    "            dn = done and (self.params['timeout'] or t < ticks)                        \n",
    "            self.memo.add(s0, a0, s1, r1, float(dn) )\n",
    "\n",
    "            if self.frame % self.params['update'] == 0:  # copy model to target\n",
    "                self.target.load_state_dict( self.model.state_dict() ) \n",
    "\n",
    "            if self.memo.count >= self.params['batch']:    \n",
    "                self.learn_model()                         \n",
    "\n",
    "            rew += r1\n",
    "            self.frame += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            s0, a0, o0 = s1, a1, o1\n",
    "        return rew, t\n",
    "\n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def learn(self, episodes = 100000, stat1 = 10, stat2 = 100, plots = 1000, rews_range=None):\n",
    "        \"\"\" Repeat episodes episodes times \"\"\"\n",
    "        self.frame = 1      \n",
    "        self.max_lens = 0\n",
    "        rews, lens, mean, beg   = [], [], 0, time.process_time()\n",
    "        for episode in range(1, episodes+1):\n",
    "            rew, t = self.run_episode( self.params['ticks'] )\n",
    "            rews.append( rew )\n",
    "            lens.append(t)\n",
    "\n",
    "            self.epsilon *= self.decay_rate                # epsilon-decay\n",
    "            if self.epsilon < self.params['eps2']:\n",
    "                self.epsilon = 0.\n",
    "                \n",
    "            if episode % stat1 == 0:\n",
    "                self.history.append([episode, np.mean(rews[-stat1:]), np.mean(rews[-stat2:]), np.mean(lens[-stat2:])])                                      \n",
    "                \n",
    "            if  episode % stat2 == 0:                               \n",
    "                mean, std    = np.mean(rews[-stat2:]), np.std(rews[-stat2:])    \n",
    "                lensM, lensS = np.mean(lens[-stat2:]), np.std(lens[-stat2:])                    \n",
    "                if mean > self.best_reward:\n",
    "                    self.best_reward = mean\n",
    "                    self.best_model.load_state_dict( self.model.state_dict() )                     \n",
    "                if self.max_lens < lensM:\n",
    "                    self.max_lens = lensM\n",
    "                    \n",
    "                maxQ = self.maxQ.to('cpu')\n",
    "                print(f\"{episode:6d} rew:{mean:7.1f} ± {std/stat2**0.5:3.1f}  ({self.best_reward:7.2f}), ticks:{lensM:3.0f} ({self.max_lens:3.0f}), eps:{self.epsilon:.3f}, Q:{maxQ.mean():8.2f} ±{maxQ.std():6.2f}, loss:{self.last_loss:7.3f}, {(time.process_time() - beg):3.0f}s\")\n",
    "                beg = time.process_time()\n",
    "                \n",
    "            if  episode % plots == 0:                   \n",
    "                self.plot(f\"{self.params['env']}  Episode: {episode}  best: {self.best_reward:7.1f}\", rews_range)\n",
    "                #self.test(episodes = 1, ticks = self.params['ticks'], render = True)\n",
    "                #env.close()\n",
    "            \n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def learn_model(self):\n",
    "        \"\"\" Model Training \"\"\"\n",
    "        batch = self.params['batch']\n",
    "        \n",
    "        S0, A0, S1, R1, Done = self.memo.get(batch)\n",
    "        S0 = S0.to(self.gpu); A0 = A0.to(self.gpu)\n",
    "        S1 = S1.to(self.gpu); R1 = R1.to(self.gpu);  Done = Done.to(self.gpu)\n",
    "        \n",
    "        if self.params['method'] == 'DQN':\n",
    "            with torch.no_grad():\n",
    "                y = self.target(S1).detach()\n",
    "            self.maxQ, _ = torch.max(y, 1)      # maximum Q values for S1\n",
    "        elif self.params['method'] == 'DDQN':\n",
    "            y = self.model(S1)                 \n",
    "            a = torch.argmax(y,1).view(-1,1)   # a = arg max Q(s1,a; theta)                 \n",
    "            with torch.no_grad():\n",
    "                q = self.target(S1)                       \n",
    "            self.maxQ = q.gather(1, a)         # Q(s1, a; theta')   \n",
    "        else:            \n",
    "            print(\"Unknown method\")\n",
    "            \n",
    "        sum_loss = 0        \n",
    "        s0, a0   = S0, A0.view(-1,1)\n",
    "        r1, done = R1.view(-1,1), Done.view(-1,1)\n",
    "        q1       = self.maxQ.view(-1,1)\n",
    "\n",
    "        yb = r1 + self.params['gamma']*q1*(1.0 - done)\n",
    "\n",
    "        y = self.model(s0)             # forward\n",
    "        y = y.gather(1, a0)\n",
    "        L = self.loss(y, yb)\n",
    "\n",
    "        self.optimizer.zero_grad()     # reset the gradients\n",
    "        L.backward()                   # calculate gradients\n",
    "        self.optimizer.step()          # adjusting parameters\n",
    "\n",
    "        sum_loss += L.detach().item()\n",
    "\n",
    "        self.last_loss = sum_loss\n",
    "        \n",
    "    #------------------------------------------------------------------------------------\n",
    "        \n",
    "    def plot(self, text, rews_range):\n",
    "        \"\"\" Plot histogram for states and actions \"\"\"        \n",
    "        hist_A, bins_A = np.histogram(self.memo.A0, bins=np.linspace(-0.5, self.nA-0.5, self.nA+1), density=True)\n",
    "\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(16,6), gridspec_kw={'width_ratios': [1, 5]})        \n",
    "        plt.suptitle(text, fontsize=18)\n",
    "                                                        \n",
    "        ax[0].set_xlim(min(bins_A), max(bins_A))    # histogram for A\n",
    "        ax[0].grid(axis='x', alpha=0.75); ax[0].grid(axis='y', alpha=0.75)\n",
    "        ax[0].set_xlabel('actions', fontsize=16)\n",
    "        ax[0].set_xticks(np.arange(self.nA));\n",
    "        bins = [ (bins_A[i]+bins_A[i+1])/2 for i in range(len(bins_A)-1)]        \n",
    "        ax[0].bar(bins, hist_A, width=0.5, color='blue')\n",
    "\n",
    "        history = np.array(self.history)            # loss history\n",
    "        ax[1].plot(history[:,0], history[:,1], linewidth=1)\n",
    "        ax[1].plot(history[:,0], history[:,2], linewidth=2)\n",
    "        ax[1].set_ylim(rews_range[0], rews_range[1]);\n",
    "        ax[1].set_xlabel('reward', fontsize=16)        \n",
    "        ax[1].set_xlabel('episode', fontsize=16)        \n",
    "        ax[1].grid(axis='x', alpha=0.75); ax[1].grid(axis='y', alpha=0.75)                \n",
    "        \n",
    "        params = [ f\"{k:9s}: {v}\\n\" for k,v in self.params.items()]\n",
    "        ax[1].text(history[0,0], rews_range[0], \"\".join(params), {'fontsize':12, 'fontname':'monospace'})\n",
    "        \n",
    "        ax2=ax[1].twinx()  \n",
    "        ax2.plot(history[:,0], history[:,3], color=\"blue\")        \n",
    "        ax2.set_ylabel(\"episode length\",color=\"blue\", fontsize=16)        \n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    def test(self, episodes = 1000, ticks = 1000, render = False):\n",
    "        \"\"\" Q-Function Testing \"\"\"\n",
    "        rews = []\n",
    "        for episode in range(1, episodes+1):\n",
    "            tot = 0\n",
    "            obs =  self.env.reset()\n",
    "            for _ in range(ticks):\n",
    "                action = self.policy( self.scale(obs) )\n",
    "                obs, rew, done, _ = self.env.step(action)\n",
    "                tot += rew\n",
    "                if render:\n",
    "                    env.render()\n",
    "                if done:\n",
    "                    break\n",
    "            rews.append(tot)\n",
    "            if episode % 100:\n",
    "                print(f\"\\r {episode:4d}: Reward: {np.mean(rews):7.3f} ± {np.std(rews)/len(rews)**0.5:.3f}\", end=\"\")\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b35d73d",
   "metadata": {},
   "source": [
    "## Snake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e08181a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_shape (6, 128, 128)\n",
      "device: cuda:0\n",
      "decay_rate: 0.0010\n",
      "features: 1472\n",
      "features: 1472\n",
      "-----------------------------------------------------------------------\n",
      "             Layer (type)               Output Shape         Param #\n",
      "=======================================================================\n",
      "                 Conv2d-1            [-1, 2, 16, 16]             770\n",
      "                   ReLU-2            [-1, 2, 16, 16]               0\n",
      "                 Conv2d-3           [-1, 48, 16, 16]             912\n",
      "                   ReLU-4           [-1, 48, 16, 16]               0\n",
      "              MaxPool2d-5             [-1, 48, 8, 8]               0\n",
      "                Dropout-6             [-1, 48, 8, 8]               0\n",
      "                 Conv2d-7             [-1, 92, 8, 8]          39,836\n",
      "                   ReLU-8             [-1, 92, 8, 8]               0\n",
      "              MaxPool2d-9             [-1, 92, 4, 4]               0\n",
      "               Dropout-10             [-1, 92, 4, 4]               0\n",
      "               Flatten-11                 [-1, 1472]               0\n",
      "                Linear-12                  [-1, 128]         188,544\n",
      "                  ReLU-13                  [-1, 128]               0\n",
      "                Linear-14                    [-1, 5]             645\n",
      "=======================================================================\n",
      "Total params: 230,707\n",
      "Trainable params: 230,707\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "env_name = \"Snake\"  # (nS=(), nA=5)\n",
    "env = snake.Snake()\n",
    "\n",
    "dqn = DQN( env )\n",
    "\n",
    "dqn.params = {\n",
    "            'env'      : env_name,\n",
    "            'ticks'    : 200,                  \n",
    "            'timeout'  : True,            # whether to consider reaching ticks as a terminal state\n",
    "            'method'   : \"DQN\",           # kind of the method (DQN, DDQN)     \n",
    "            'gamma'    : 0.99,            # discount factor\n",
    "            'eps1'     : 1.0,             # initial value epsilon\n",
    "            'eps2'     : 0.001,           # final value   epsilon\n",
    "            'decays'   : 1,               # number of episodes to decay eps1 - > eps2\n",
    "            'update'   : 100,             # target model update rate (in frames = time steps)         \n",
    "            'batch'    : 512,             # batch size for training\n",
    "            'capacity' : 10000,           # memory size\n",
    "            'channels' : [2,48,92],       # conv channles\n",
    "            'kernels'  : [8,3,3],         # conv kernels\n",
    "            'strides'  : [8,1,1],         # conv strides\n",
    "            'paddings' : [0,1,1],         # conv paddings\n",
    "            'pools'    : [1,2,2],         # = pool_strides\n",
    "            'dropout'  : 0.2,             # after cnn,reLU,pool \n",
    "            'hidden'   : 128,             # hidden layers            \n",
    "            'loss'     : 'mse',           # loss function (mse, huber)\n",
    "            'optimizer': 'adam',          # optimizer (sgd, adam)\n",
    "            'lr'       : 0.0001,          # learning rate             \n",
    "}\n",
    "\n",
    "dqn.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3c3f197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env': 'Snake', 'ticks': 200, 'timeout': True, 'method': 'DQN', 'gamma': 0.99, 'eps1': 1.0, 'eps2': 0.001, 'decays': 1, 'update': 100, 'batch': 512, 'capacity': 10000, 'channels': [2, 48, 92], 'kernels': [8, 3, 3], 'strides': [8, 1, 1], 'paddings': [0, 1, 1], 'pools': [1, 2, 2], 'dropout': 0.2, 'hidden': 128, 'loss': 'mse', 'optimizer': 'adam', 'lr': 0.0001}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(dqn\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrews_range\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, episodes, stat1, stat2, plots, rews_range)\u001b[0m\n\u001b[0;32m    226\u001b[0m rews, lens, mean, beg   \u001b[38;5;241m=\u001b[39m [], [], \u001b[38;5;241m0\u001b[39m, time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, episodes\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 228\u001b[0m     rew, t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mticks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m     rews\u001b[38;5;241m.\u001b[39mappend( rew )\n\u001b[0;32m    230\u001b[0m     lens\u001b[38;5;241m.\u001b[39mappend(t)\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mDQN.run_episode\u001b[1;34m(self, ticks)\u001b[0m\n\u001b[0;32m    194\u001b[0m o0, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m0\u001b[39m)           \u001b[38;5;66;03m# nothing do\u001b[39;00m\n\u001b[0;32m    195\u001b[0m s0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_state(oi,o0)\n\u001b[1;32m--> 196\u001b[0m a0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms0\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# get action        \u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, ticks\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    198\u001b[0m     o1, r1, done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(a0)            \n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mDQN.policy\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    175\u001b[0m x \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 177\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy() \n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(y)\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mAgentModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):        \n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "print(dqn.params)\n",
    "dqn.learn(episodes = 100000, rews_range = [-100, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a903cc",
   "metadata": {},
   "source": [
    "## Test best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29296c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.model.load_state_dict( dqn.best_model.state_dict() )\n",
    "dqn.test(episodes = 1, ticks=500, render=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dcaf96",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4966a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "  \n",
    "state = {'info':      f\"{env_name}: Q-function, Reward:  286\",     \n",
    "         'date':      datetime.datetime.now(),  \n",
    "         'model':     str(dqn.best_model),\n",
    "         'state' :    dqn.best_model.state_dict(),  \n",
    "        } \n",
    "print(dqn.params['hiddens'])\n",
    "torch.save(state, f\"{env_name}_{'_'.join([str(x) for x in dqn.params['hiddens']])}.286.pt\")\n",
    "print(state['model'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
